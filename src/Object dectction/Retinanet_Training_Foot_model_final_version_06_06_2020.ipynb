{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Retinanet-Training Foot model final version 06_06_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuCOm-NuF88-",
        "colab_type": "text"
      },
      "source": [
        "# Installing retinanet library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeV8um3iGFwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a = []\n",
        "# while(1):\n",
        "#     a.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kkE0rKhUjA8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "83408131-d2f3-4937-9f1b-91bad67087f8"
      },
      "source": [
        "!git clone --recursive https://github.com/fizyr/keras-retinanet.git\n",
        "!ls\n",
        "!cd keras-retinanet && pip install . --user\n",
        "!pip install Cython\n",
        "!pip install --user git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
        "!cd keras-retinanet/snapshots && \\\n",
        "wget https://github.com/fizyr/keras-retinanet/releases/download/0.2/resnet50_coco_best_v2.0.3.h5\n",
        "\n",
        "!pip install --upgrade git+https://github.com/fizyr/keras-retinanet\n",
        "!pip install --upgrade git+https://github.com/broadinstitute/keras-resnet\n",
        "\n",
        "import keras\n",
        "import keras_resnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-retinanet'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5762 (delta 0), reused 1 (delta 0), pack-reused 5757\u001b[K\n",
            "Receiving objects: 100% (5762/5762), 13.38 MiB | 32.39 MiB/s, done.\n",
            "Resolving deltas: 100% (3865/3865), done.\n",
            "Submodule 'tests/test-data' (https://github.com/fizyr/keras-retinanet-test-data.git) registered for path 'tests/test-data'\n",
            "Cloning into '/content/keras-retinanet/tests/test-data'...\n",
            "remote: Enumerating objects: 45, done.        \n",
            "remote: Total 45 (delta 0), reused 0 (delta 0), pack-reused 45        \n",
            "Submodule path 'tests/test-data': checked out '98404379fbf1ff1273d01db835c10cc83a4f8007'\n",
            "drive  keras-retinanet\tsample_data\n",
            "Processing /content/keras-retinanet\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (2.3.1)\n",
            "Collecting keras-resnet==0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/05/46/ad0b2d1a05d9497bd80c98a2c3f4d8be38a4601ace69af72814f5fafd851/keras-resnet-0.1.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (1.4.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (0.29.19)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (7.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (4.1.2.30)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (3.38.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (2.10.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->keras-retinanet==0.5.1) (2.4.0)\n",
            "Building wheels for collected packages: keras-retinanet, keras-resnet\n",
            "  Building wheel for keras-retinanet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-retinanet: filename=keras_retinanet-0.5.1-cp36-cp36m-linux_x86_64.whl size=169702 sha256=cbc7680823ab6d57839cb3c8ab30853278255dabc2fcdc06f5d5fa77ede5f54e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/9f/57/cb0305f6f5a41fc3c11ad67b8cedfbe9127775b563337827ba\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-resnet: filename=keras_resnet-0.1.0-py2.py3-none-any.whl size=13346 sha256=1827af4e60e94c812e32c9e15f6f9f84c8f7a3c763ba481256726a477412c098\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/dd/ac/842235b63dddac12faa4b48ebe58b8944e8c2e57c2e38dddb6\n",
            "Successfully built keras-retinanet keras-resnet\n",
            "Installing collected packages: keras-resnet, keras-retinanet\n",
            "\u001b[33m  WARNING: The scripts retinanet-convert-model, retinanet-debug, retinanet-evaluate and retinanet-train are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed keras-resnet-0.1.0 keras-retinanet-0.5.1\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.19)\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-bblg_ad1\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-bblg_ad1\n",
            "Requirement already satisfied (use --upgrade to upgrade): pycocotools==2.0 from git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (47.1.1)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.19)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.12.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=267013 sha256=4e28bb6af69248408ae3483f6169f8c3dc35337eb13c4b0dc82c5c9bbaae6c0a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c_g0aaz5/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n",
            "--2020-06-14 17:23:14--  https://github.com/fizyr/keras-retinanet/releases/download/0.2/resnet50_coco_best_v2.0.3.h5\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/100249425/190de828-40ad-11e8-8d21-51a2a173a26c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200614%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200614T172314Z&X-Amz-Expires=300&X-Amz-Signature=69b8569d0e5d3c1367f074d42da8d0011a0fc4e710e18aef7035ef99d66bb829&X-Amz-SignedHeaders=host&actor_id=0&repo_id=100249425&response-content-disposition=attachment%3B%20filename%3Dresnet50_coco_best_v2.0.3.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-06-14 17:23:14--  https://github-production-release-asset-2e65be.s3.amazonaws.com/100249425/190de828-40ad-11e8-8d21-51a2a173a26c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200614%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200614T172314Z&X-Amz-Expires=300&X-Amz-Signature=69b8569d0e5d3c1367f074d42da8d0011a0fc4e710e18aef7035ef99d66bb829&X-Amz-SignedHeaders=host&actor_id=0&repo_id=100249425&response-content-disposition=attachment%3B%20filename%3Dresnet50_coco_best_v2.0.3.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.10.52\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.10.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 152662144 (146M) [application/octet-stream]\n",
            "Saving to: ‘resnet50_coco_best_v2.0.3.h5’\n",
            "\n",
            "resnet50_coco_best_ 100%[===================>] 145.59M  89.4MB/s    in 1.6s    \n",
            "\n",
            "2020-06-14 17:23:16 (89.4 MB/s) - ‘resnet50_coco_best_v2.0.3.h5’ saved [152662144/152662144]\n",
            "\n",
            "Collecting git+https://github.com/fizyr/keras-retinanet\n",
            "  Cloning https://github.com/fizyr/keras-retinanet to /tmp/pip-req-build-3wpg9oew\n",
            "  Running command git clone -q https://github.com/fizyr/keras-retinanet /tmp/pip-req-build-3wpg9oew\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied, skipping upgrade: keras in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-resnet==0.1.0 in /root/.local/lib/python3.6/site-packages (from keras-retinanet==0.5.1) (0.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cython in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (0.29.19)\n",
            "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (3.38.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->keras-retinanet==0.5.1) (2.4.0)\n",
            "Building wheels for collected packages: keras-retinanet\n",
            "  Building wheel for keras-retinanet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-retinanet: filename=keras_retinanet-0.5.1-cp36-cp36m-linux_x86_64.whl size=169714 sha256=b341128bfb8d13c78c914e49e6add553b1176263f96fed28c740e113796a8cfa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w79g6rr5/wheels/3e/f1/75/4b42a59887b48ec1022cc76889b1e48da866f1482fd7a0f3df\n",
            "Successfully built keras-retinanet\n",
            "Installing collected packages: keras-retinanet\n",
            "  Found existing installation: keras-retinanet 0.5.1\n",
            "    Uninstalling keras-retinanet-0.5.1:\n",
            "      Successfully uninstalled keras-retinanet-0.5.1\n",
            "Successfully installed keras-retinanet-0.5.1\n",
            "Collecting git+https://github.com/broadinstitute/keras-resnet\n",
            "  Cloning https://github.com/broadinstitute/keras-resnet to /tmp/pip-req-build-wu6l9llb\n",
            "  Running command git clone -q https://github.com/broadinstitute/keras-resnet /tmp/pip-req-build-wu6l9llb\n",
            "Requirement already satisfied, skipping upgrade: keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from keras-resnet==0.2.0) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (1.4.1)\n",
            "Building wheels for collected packages: keras-resnet\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-resnet: filename=keras_resnet-0.2.0-py2.py3-none-any.whl size=22144 sha256=03646051c463980fa0773321ccd342b39aed46103086cca365e1297ab944952a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_woalzau/wheels/10/52/f3/6a1fdbfb022ce9abfdf00a1ca7e90cef71dea99976edbcb53f\n",
            "Successfully built keras-resnet\n",
            "\u001b[31mERROR: keras-retinanet 0.5.1 has requirement keras-resnet==0.1.0, but you'll have keras-resnet 0.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-resnet\n",
            "  Found existing installation: keras-resnet 0.1.0\n",
            "    Uninstalling keras-resnet-0.1.0:\n",
            "      Successfully uninstalled keras-resnet-0.1.0\n",
            "Successfully installed keras-resnet-0.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnBrHba0FOFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from os import listdir, walk\n",
        "from os.path import join\n",
        "from keras_retinanet.bin.train import create_generators,create_models,create_callbacks\n",
        "from keras_retinanet.models import backbone,load_model,convert_model\n",
        "from keras_retinanet.utils.config import read_config_file,parse_anchor_parameters\n",
        "from keras_retinanet.utils.visualization import draw_boxes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imgaug import augmenters as iaa\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASy6w9dhK6Lf",
        "colab_type": "text"
      },
      "source": [
        "# Training Left Foot model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjl4eT8PHmBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c98112c-e05b-430e-c73e-bbe6719ceadb"
      },
      "source": [
        "\n",
        "# tf.set_random_seed(31) # SEEDS MAKE RESULTS MORE REPRODUCABLE\n",
        "np.random.seed(17)\n",
        "classes = np.arange(0, 6, 1).tolist()\n",
        "\n",
        "import pandas as pd\n",
        "tr_annots = pd.read_csv('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/all_foot_train_03_05.csv', header = None)\n",
        "tr_annots['ids'] = tr_annots[0].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
        "train_ids = tr_annots['ids'].unique().tolist()\n",
        "\n",
        "import pandas as pd\n",
        "tr_annots = pd.read_csv('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/all_foot_val_03_06.csv', header = None)\n",
        "tr_annots['ids'] = tr_annots[0].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
        "val_ids = tr_annots['ids'].unique().tolist()\n",
        "\n",
        "print(len(train_ids), len(val_ids))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "355 61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avi3yrZEgfoA",
        "colab_type": "text"
      },
      "source": [
        "# Previous config files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXrpIRtvgjif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # with open('config.ini','w') as f:\n",
        "# #     f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 1 1.5 2 2.5 \\nscales  = 1 2\\n')\n",
        "\n",
        "# #Updating the anchor parameters\n",
        "# with open('config.ini','w') as f:\n",
        "#     f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 1.5 1.8 2 2.2 2.5 2.8 3.0\\nscales  = 1 1.2 1.6 2 3\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAZ0OgRggkkP",
        "colab_type": "text"
      },
      "source": [
        "## New config parameters for anchors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIbpqIMZgjYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Updating the anchor parameters\n",
        "# with open('config.ini','w') as f:\n",
        "#     f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 0.8 1 1.3 1.5 1.8 2 2.2 2.5 2.8 3.0 3.2\\nscales  = 1 1.2 1.6\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sfsBVQSdvFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # New ratios parameters for training as previous didn't help\n",
        "# with open('config.ini','w') as f:\n",
        "#     f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 0.8 1 1.3 1.5 1.8 2 2.2 2.5 2.8 3.0 3.2\\nscales  = 1 1.2 1.6\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p743XGNlw3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Updating the parameters\n",
        "# New ratios parameters for training as previous didn't help\n",
        "with open('config.ini','w') as f:\n",
        "    f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512 1024\\nstrides = 8 16 32 64 128 256\\nratios  = 1 1.5 2 2.5 3\\nscales  = 1 1.2 1.6\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD6Z4rVwH01Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "b = backbone('resnet50')\n",
        "\n",
        "# Increasing batch size from 4 -> 8\n",
        "class args:\n",
        "    batch_size = 8\n",
        "    config = read_config_file('config.ini')\n",
        "    random_transform = True # Image augmentation\n",
        "    annotations = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/all_foot_train_03_05.csv'\n",
        "    val_annotations = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/all_foot_val_03_06.csv'\n",
        "    classes = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/Foot class names.csv'\n",
        "    image_min_side = 1000\n",
        "    image_max_side = 1400\n",
        "    no_resize = False\n",
        "    dataset_type = 'csv'\n",
        "    tensorboard_dir = ''\n",
        "    evaluation = False\n",
        "    snapshots = True\n",
        "    snapshot_path = \"/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued\"\n",
        "    backbone = 'resnet50'\n",
        "    epochs = 200\n",
        "    steps = len(train_ids)//(batch_size)\n",
        "    weighted_average = True\n",
        "    reduce_lr_factor = 0.1\n",
        "    reduce_lr_patience = 4\n",
        "    compute_val_loss = True\n",
        "    iou_threshold = 0.6\n",
        "    nms_threshold = 0.5\n",
        "    score_threshold = 0.15\n",
        "    anchors = True\n",
        "    resize = True\n",
        "    display_name = 'Anchors'\n",
        "    no_gui = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73t1WInbMalg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen, valid_gen = create_generators(args, b.preprocess_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFqwzungdkb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras_retinanet.preprocessing.generator import Generator\n",
        "# from ..utils.image import read_image_bgr\n",
        "from keras_retinanet.utils.transform import random_transform_generator\n",
        "from keras_retinanet.utils.image import random_visual_effect_generator\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from six import raise_from\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "import os.path\n",
        "from collections import OrderedDict\n",
        "\n",
        "import PIL.Image\n",
        "import PIL.ImageOps\n",
        "import numpy as np\n",
        "from os import makedirs \n",
        "\n",
        "\n",
        "def exif_transpose(img):\n",
        "    if not img:\n",
        "        return img\n",
        "\n",
        "    exif_orientation_tag = 274\n",
        "\n",
        "    # Check for EXIF data (only present on some files)\n",
        "    if hasattr(img, \"_getexif\") and isinstance(img._getexif(), dict) and exif_orientation_tag in img._getexif():\n",
        "        exif_data = img._getexif()\n",
        "        orientation = exif_data[exif_orientation_tag]\n",
        "\n",
        "        # Handle EXIF Orientation\n",
        "        if orientation == 1:\n",
        "            # Normal image - nothing to do!\n",
        "            pass\n",
        "        elif orientation == 2:\n",
        "            # Mirrored left to right\n",
        "            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "        elif orientation == 3:\n",
        "            # Rotated 180 degrees\n",
        "            img = img.rotate(180)\n",
        "        elif orientation == 4:\n",
        "            # Mirrored top to bottom\n",
        "            img = img.rotate(180).transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "        elif orientation == 5:\n",
        "            # Mirrored along top-left diagonal\n",
        "            img = img.rotate(-90, expand=True).transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "        elif orientation == 6:\n",
        "            # Rotated 90 degrees\n",
        "            img = img.rotate(-90, expand=True)\n",
        "        elif orientation == 7:\n",
        "            # Mirrored along top-right diagonal\n",
        "            img = img.rotate(90, expand=True).transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "        elif orientation == 8:\n",
        "            # Rotated 270 degrees\n",
        "            img = img.rotate(90, expand=True)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_image_file(file, mode='RGB'):\n",
        "    # Load the image with PIL\n",
        "    img = PIL.Image.open(file)\n",
        "\n",
        "    if hasattr(PIL.ImageOps, 'exif_transpose'):\n",
        "        # Very recent versions of PIL can do exit transpose internally\n",
        "        img = PIL.ImageOps.exif_transpose(img)\n",
        "    else:\n",
        "        # Otherwise, do the exif transpose ourselves\n",
        "        img = exif_transpose(img)\n",
        "\n",
        "    img = img.convert(mode)\n",
        "\n",
        "    return np.array(img)\n",
        "\n",
        "def _parse(value, function, fmt):\n",
        "    \"\"\"\n",
        "    Parse a string into a value, and format a nice ValueError if it fails.\n",
        "\n",
        "    Returns `function(value)`.\n",
        "    Any `ValueError` raised is catched and a new `ValueError` is raised\n",
        "    with message `fmt.format(e)`, where `e` is the caught `ValueError`.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return function(value)\n",
        "    except ValueError as e:\n",
        "        raise_from(ValueError(fmt.format(e)), None)\n",
        "\n",
        "\n",
        "def _read_classes(csv_reader):\n",
        "    \"\"\" Parse the classes file given by csv_reader.\n",
        "    \"\"\"\n",
        "    result = OrderedDict()\n",
        "    for line, row in enumerate(csv_reader):\n",
        "        line += 1\n",
        "\n",
        "        try:\n",
        "            class_name, class_id = row\n",
        "        except ValueError:\n",
        "            raise_from(ValueError('line {}: format should be \\'class_name,class_id\\''.format(line)), None)\n",
        "        class_id = _parse(class_id, int, 'line {}: malformed class ID: {{}}'.format(line))\n",
        "\n",
        "        if class_name in result:\n",
        "            raise ValueError('line {}: duplicate class name: \\'{}\\''.format(line, class_name))\n",
        "        result[class_name] = class_id\n",
        "    return result\n",
        "\n",
        "\n",
        "def _read_annotations(csv_reader, classes):\n",
        "    \"\"\" Read annotations from the csv_reader.\n",
        "    \"\"\"\n",
        "    result = OrderedDict()\n",
        "    for line, row in enumerate(csv_reader):\n",
        "        line += 1\n",
        "\n",
        "        try:\n",
        "            img_file, x1, y1, x2, y2, class_name = row[:6]\n",
        "        except ValueError:\n",
        "            raise_from(ValueError('line {}: format should be \\'img_file,x1,y1,x2,y2,class_name\\' or \\'img_file,,,,,\\''.format(line)), None)\n",
        "\n",
        "        if img_file not in result:\n",
        "            result[img_file] = []\n",
        "\n",
        "        # If a row contains only an image path, it's an image without annotations.\n",
        "        if (x1, y1, x2, y2, class_name) == ('', '', '', '', ''):\n",
        "            continue\n",
        "\n",
        "        x1 = _parse(x1, int, 'line {}: malformed x1: {{}}'.format(line))\n",
        "        y1 = _parse(y1, int, 'line {}: malformed y1: {{}}'.format(line))\n",
        "        x2 = _parse(x2, int, 'line {}: malformed x2: {{}}'.format(line))\n",
        "        y2 = _parse(y2, int, 'line {}: malformed y2: {{}}'.format(line))\n",
        "\n",
        "        # Check that the bounding box is valid.\n",
        "        if x2 <= x1:\n",
        "            raise ValueError('line {}: x2 ({}) must be higher than x1 ({})'.format(line, x2, x1))\n",
        "        if y2 <= y1:\n",
        "            raise ValueError('line {}: y2 ({}) must be higher than y1 ({})'.format(line, y2, y1))\n",
        "\n",
        "        # check if the current class name is correctly present\n",
        "        if class_name not in classes:\n",
        "            raise ValueError('line {}: unknown class name: \\'{}\\' (classes: {})'.format(line, class_name, classes))\n",
        "\n",
        "        result[img_file].append({'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': class_name})\n",
        "    return result\n",
        "\n",
        "\n",
        "def _open_for_csv(path):\n",
        "    \"\"\" Open a file with flags suitable for csv.reader.\n",
        "\n",
        "    This is different for python2 it means with mode 'rb',\n",
        "    for python3 this means 'r' with \"universal newlines\".\n",
        "    \"\"\"\n",
        "    if sys.version_info[0] < 3:\n",
        "        return open(path, 'rb')\n",
        "    else:\n",
        "        return open(path, 'r', newline='')\n",
        "\n",
        "\n",
        "class CSVGenerator(Generator):\n",
        "    \"\"\" Generate data for a custom CSV dataset.\n",
        "\n",
        "    See https://github.com/fizyr/keras-retinanet#csv-datasets for more information.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_data_file,\n",
        "        csv_class_file,\n",
        "        base_dir=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\" Initialize a CSV data generator.\n",
        "\n",
        "        Args\n",
        "            csv_data_file: Path to the CSV annotations file.\n",
        "            csv_class_file: Path to the CSV classes file.\n",
        "            base_dir: Directory w.r.t. where the files are to be searched (defaults to the directory containing the csv_data_file).\n",
        "        \"\"\"\n",
        "        self.image_names = []\n",
        "        self.image_data  = {}\n",
        "        self.base_dir    = base_dir\n",
        "\n",
        "        # Take base_dir from annotations file if not explicitly specified.\n",
        "        if self.base_dir is None:\n",
        "            self.base_dir = os.path.dirname(csv_data_file)\n",
        "\n",
        "        # parse the provided class file\n",
        "        try:\n",
        "            with _open_for_csv(csv_class_file) as file:\n",
        "                self.classes = _read_classes(csv.reader(file, delimiter=','))\n",
        "        except ValueError as e:\n",
        "            raise_from(ValueError('invalid CSV class file: {}: {}'.format(csv_class_file, e)), None)\n",
        "\n",
        "        self.labels = {}\n",
        "        for key, value in self.classes.items():\n",
        "            self.labels[value] = key\n",
        "\n",
        "        # csv with img_path, x1, y1, x2, y2, class_name\n",
        "        try:\n",
        "            with _open_for_csv(csv_data_file) as file:\n",
        "                self.image_data = _read_annotations(csv.reader(file, delimiter=','), self.classes)\n",
        "        except ValueError as e:\n",
        "            raise_from(ValueError('invalid CSV annotations file: {}: {}'.format(csv_data_file, e)), None)\n",
        "        self.image_names = list(self.image_data.keys())\n",
        "\n",
        "        super(CSVGenerator, self).__init__(**kwargs)\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\" Size of the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def num_classes(self):\n",
        "        \"\"\" Number of classes in the dataset.\n",
        "        \"\"\"\n",
        "        return max(self.classes.values()) + 1\n",
        "\n",
        "    def has_label(self, label):\n",
        "        \"\"\" Return True if label is a known label.\n",
        "        \"\"\"\n",
        "        return label in self.labels\n",
        "\n",
        "    def has_name(self, name):\n",
        "        \"\"\" Returns True if name is a known class.\n",
        "        \"\"\"\n",
        "        return name in self.classes\n",
        "\n",
        "    def name_to_label(self, name):\n",
        "        \"\"\" Map name to label.\n",
        "        \"\"\"\n",
        "        return self.classes[name]\n",
        "\n",
        "    def label_to_name(self, label):\n",
        "        \"\"\" Map label to name.\n",
        "        \"\"\"\n",
        "        return self.labels[label]\n",
        "\n",
        "    def image_path(self, image_index):\n",
        "        \"\"\" Returns the image path for image_index.\n",
        "        \"\"\"\n",
        "        return os.path.join(self.base_dir, self.image_names[image_index])\n",
        "\n",
        "    def image_aspect_ratio(self, image_index):\n",
        "        \"\"\" Compute the aspect ratio for an image with image_index.\n",
        "        \"\"\"\n",
        "        # PIL is fast for metadata\n",
        "        image = Image.open(self.image_path(image_index))\n",
        "        return float(image.width) / float(image.height)\n",
        "\n",
        "    def load_image(self, image_index):\n",
        "        \"\"\" Load an image at the image_index.\n",
        "        \"\"\"\n",
        "        return load_image_file(self.image_path(image_index))\n",
        "\n",
        "    def load_annotations(self, image_index):\n",
        "        \"\"\" Load annotations for an image_index.\n",
        "        \"\"\"\n",
        "        path        = self.image_names[image_index]\n",
        "        annotations = {'labels': np.empty((0,)), 'bboxes': np.empty((0, 4))}\n",
        "\n",
        "        for idx, annot in enumerate(self.image_data[path]):\n",
        "            annotations['labels'] = np.concatenate((annotations['labels'], [self.name_to_label(annot['class'])]))\n",
        "            annotations['bboxes'] = np.concatenate((annotations['bboxes'], [[\n",
        "                float(annot['x1']),\n",
        "                float(annot['y1']),\n",
        "                float(annot['x2']),\n",
        "                float(annot['y2']),\n",
        "            ]]))\n",
        "\n",
        "        return annotations\n",
        "\n",
        "def mAP(y_true, y_pred):\n",
        "    num_classes = y_true.shape[1]\n",
        "    average_precisions = []\n",
        "    relevant = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    tp_whole = K.round(K.clip(y_true * y_pred, 0, 1))\n",
        "    for index in range(num_classes):\n",
        "      temp = K.sum(tp_whole[:,:index+1],axis=1)\n",
        "      average_precisions.append(temp * (1/(index + 1)))\n",
        "    AP = Add()(average_precisions) / relevant\n",
        "    mAP = K.mean(AP,axis=0)\n",
        "    return mAP\n",
        "\n",
        "def create_callbacks(model, training_model, prediction_model, validation_generator, args):\n",
        "    \"\"\" Creates the callbacks to use during training.\n",
        "\n",
        "    Args\n",
        "        model: The base model.\n",
        "        training_model: The model that is used for training.\n",
        "        prediction_model: The model that should be used for validation.\n",
        "        validation_generator: The generator for creating validation data.\n",
        "        args: parseargs args object.\n",
        "\n",
        "    Returns:\n",
        "        A list of callbacks used for training.\n",
        "    \"\"\"\n",
        "    callbacks = []\n",
        "\n",
        "    tensorboard_callback = None\n",
        "\n",
        "    if args.tensorboard_dir:\n",
        "        makedirs(args.tensorboard_dir)\n",
        "        tensorboard_callback = keras.callbacks.TensorBoard(\n",
        "            log_dir                = args.tensorboard_dir,\n",
        "            histogram_freq         = 0,\n",
        "            batch_size             = args.batch_size,\n",
        "            write_graph            = True,\n",
        "            write_grads            = False,\n",
        "            write_images           = False,\n",
        "            embeddings_freq        = 0,\n",
        "            embeddings_layer_names = None,\n",
        "            embeddings_metadata    = None\n",
        "        )\n",
        "\n",
        "    if args.evaluation and validation_generator:\n",
        "        if args.dataset_type == 'coco':\n",
        "            from ..callbacks.coco import CocoEval\n",
        "\n",
        "            # use prediction model for evaluation\n",
        "            evaluation = CocoEval(validation_generator, tensorboard=tensorboard_callback)\n",
        "        else:\n",
        "            evaluation = Evaluate(validation_generator, tensorboard=tensorboard_callback, weighted_average=args.weighted_average)\n",
        "        evaluation = RedirectModel(evaluation, prediction_model)\n",
        "        callbacks.append(evaluation)\n",
        "\n",
        "    # save the model\n",
        "    if args.snapshots:\n",
        "        # ensure directory created first; otherwise h5py will error after epoch.\n",
        "        makedirs(args.snapshot_path)\n",
        "        checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(\n",
        "                args.snapshot_path,\n",
        "                '{backbone}_{dataset_type}_{{epoch:02d}}.h5'.format(backbone=args.backbone, dataset_type=args.dataset_type)\n",
        "            ),\n",
        "            verbose=1,\n",
        "            # save_best_only=True,\n",
        "            # monitor=\"mAP\",\n",
        "            # mode='max'\n",
        "        )\n",
        "        checkpoint = RedirectModel(checkpoint, model)\n",
        "        callbacks.append(checkpoint)\n",
        "\n",
        "    callbacks.append(keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor    = 'loss',\n",
        "        factor     = args.reduce_lr_factor,\n",
        "        patience   = args.reduce_lr_patience,\n",
        "        verbose    = 1,\n",
        "        mode       = 'auto',\n",
        "        min_delta  = 0.0001,\n",
        "        cooldown   = 0,\n",
        "        min_lr     = 0\n",
        "    ))\n",
        "\n",
        "    callbacks.append(keras.callbacks.EarlyStopping(\n",
        "        monitor    = 'classification_loss',\n",
        "        patience   = 20,\n",
        "        mode       = 'max',\n",
        "        min_delta  = 0.01\n",
        "    ))\n",
        "\n",
        "    if args.tensorboard_dir:\n",
        "        callbacks.append(tensorboard_callback)\n",
        "\n",
        "    return callbacks\n",
        "\n",
        "def create_generators(args, preprocess_image):\n",
        "    \"\"\" Create generators for training and validation.\n",
        "\n",
        "    Args\n",
        "        args             : parseargs object containing configuration for generators.\n",
        "        preprocess_image : Function that preprocesses an image for the network.\n",
        "    \"\"\"\n",
        "    common_args = {\n",
        "        'batch_size'       : args.batch_size,\n",
        "        'config'           : args.config,\n",
        "        'image_min_side'   : args.image_min_side,\n",
        "        'image_max_side'   : args.image_max_side,\n",
        "        'no_resize'        : args.no_resize,\n",
        "        'preprocess_image' : preprocess_image,\n",
        "    }\n",
        "\n",
        "    # create random transform generator for augmenting training data\n",
        "    if args.random_transform:\n",
        "        transform_generator = random_transform_generator(\n",
        "            min_rotation=-0.1,\n",
        "            max_rotation=0.1,\n",
        "            min_translation=(-0.1, -0.1),\n",
        "            max_translation=(0.1, 0.1),\n",
        "            min_shear=-0.1,\n",
        "            max_shear=0.1,\n",
        "            min_scaling=(0.9, 0.9),\n",
        "            max_scaling=(1.1, 1.1),\n",
        "            flip_x_chance=0.5,\n",
        "            flip_y_chance=0.5,\n",
        "        )\n",
        "        visual_effect_generator = random_visual_effect_generator(\n",
        "            contrast_range=(0.9, 1.1),\n",
        "            brightness_range=(-.1, .1),\n",
        "            hue_range=(-0.05, 0.05),\n",
        "            saturation_range=(0.95, 1.05)\n",
        "        )\n",
        "    else:\n",
        "        transform_generator = random_transform_generator(flip_x_chance=0.5)\n",
        "        visual_effect_generator = None\n",
        "\n",
        "    if args.dataset_type == 'coco':\n",
        "        # import here to prevent unnecessary dependency on cocoapi\n",
        "        from ..preprocessing.coco import CocoGenerator\n",
        "\n",
        "        train_generator = CocoGenerator(\n",
        "            args.coco_path,\n",
        "            'train2017',\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "\n",
        "        validation_generator = CocoGenerator(\n",
        "            args.coco_path,\n",
        "            'val2017',\n",
        "            shuffle_groups=False,\n",
        "            **common_args\n",
        "        )\n",
        "    elif args.dataset_type == 'pascal':\n",
        "        train_generator = PascalVocGenerator(\n",
        "            args.pascal_path,\n",
        "            'train',\n",
        "            image_extension=args.image_extension,\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "\n",
        "        validation_generator = PascalVocGenerator(\n",
        "            args.pascal_path,\n",
        "            'val',\n",
        "            image_extension=args.image_extension,\n",
        "            shuffle_groups=False,\n",
        "            **common_args\n",
        "        )\n",
        "    elif args.dataset_type == 'csv':\n",
        "        train_generator = CSVGenerator(\n",
        "            args.annotations,\n",
        "            args.classes,\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "\n",
        "        if args.val_annotations:\n",
        "            validation_generator = CSVGenerator(\n",
        "                args.val_annotations,\n",
        "                args.classes,\n",
        "                shuffle_groups=False,\n",
        "                **common_args\n",
        "            )\n",
        "        else:\n",
        "            validation_generator = None\n",
        "    elif args.dataset_type == 'oid':\n",
        "        train_generator = OpenImagesGenerator(\n",
        "            args.main_dir,\n",
        "            subset='train',\n",
        "            version=args.version,\n",
        "            labels_filter=args.labels_filter,\n",
        "            annotation_cache_dir=args.annotation_cache_dir,\n",
        "            parent_label=args.parent_label,\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "\n",
        "        validation_generator = OpenImagesGenerator(\n",
        "            args.main_dir,\n",
        "            subset='validation',\n",
        "            version=args.version,\n",
        "            labels_filter=args.labels_filter,\n",
        "            annotation_cache_dir=args.annotation_cache_dir,\n",
        "            parent_label=args.parent_label,\n",
        "            shuffle_groups=False,\n",
        "            **common_args\n",
        "        )\n",
        "    elif args.dataset_type == 'kitti':\n",
        "        train_generator = KittiGenerator(\n",
        "            args.kitti_path,\n",
        "            subset='train',\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "\n",
        "        validation_generator = KittiGenerator(\n",
        "            args.kitti_path,\n",
        "            subset='val',\n",
        "            shuffle_groups=False,\n",
        "            **common_args\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError('Invalid data type received: {}'.format(args.dataset_type))\n",
        "\n",
        "    return train_generator, validation_generator\n",
        "\n",
        "train_gen, valid_gen = create_generators(args, b.preprocess_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG5KCse1dkQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# # from keras_retinanet.bin.train import create_generators\n",
        "# # from keras_retinanet.models import backbone\n",
        "# from keras_retinanet.utils.config import read_config_file,parse_anchor_parameters\n",
        "# # # from keras_retinanet.bin.debug import run\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# from keras_retinanet.utils.anchors import anchors_for_shape, compute_gt_annotations\n",
        "# from keras_retinanet.utils.visualization import draw_annotations, draw_boxes, draw_caption\n",
        "\n",
        "\n",
        "# def run(generator, args, anchor_params):\n",
        "#     \"\"\" Main loop.\n",
        "\n",
        "#     Args\n",
        "#         generator: The generator to debug.\n",
        "#         args: parseargs args object.\n",
        "#     \"\"\"\n",
        "#     # display images, one at a time\n",
        "#     i = 0\n",
        "#     while True:\n",
        "#         # load the data\n",
        "#         image       = generator.load_image(i)\n",
        "#         annotations = generator.load_annotations(i)\n",
        "#         if len(annotations['labels']) > 0 :\n",
        "#             # apply random transformations\n",
        "#             if args.random_transform:\n",
        "#                 image, annotations = generator.random_transform_group_entry(image, annotations)\n",
        "#                 image, annotations = generator.random_visual_effect_group_entry(image, annotations)\n",
        "\n",
        "#             # resize the image and annotations\n",
        "#             if args.resize:\n",
        "#                 image, image_scale = generator.resize_image(image)\n",
        "#                 annotations['bboxes'] *= image_scale\n",
        "\n",
        "#             anchors = anchors_for_shape(image.shape, anchor_params=anchor_params)\n",
        "#             positive_indices, _, max_indices = compute_gt_annotations(anchors, annotations['bboxes'])\n",
        "\n",
        "#             # draw anchors on the image\n",
        "#             if args.anchors:\n",
        "#                 draw_boxes(image, anchors[positive_indices], (255, 255, 0), thickness=1)\n",
        "\n",
        "#             # draw annotations on the image\n",
        "#             if args.annotations:\n",
        "#                 # draw annotations in red\n",
        "#                 draw_annotations(image, annotations, color=(0, 0, 255), label_to_name=generator.label_to_name)\n",
        "\n",
        "#                 # draw regressed anchors in green to override most red annotations\n",
        "#                 # result is that annotations without anchors are red, with anchors are green\n",
        "#                 draw_boxes(image, annotations['bboxes'][max_indices[positive_indices], :], (0, 255, 0))\n",
        "\n",
        "#             # display name on the image\n",
        "#             if args.display_name:\n",
        "#                 draw_caption(image, [0, image.shape[0]], os.path.basename(generator.image_path(i)))\n",
        "\n",
        "#         # write to file and advance if no-gui selected\n",
        "#         if args.no_gui:\n",
        "#             output_path = make_output_path(args.output_dir, generator.image_path(i), flatten=args.flatten_output)\n",
        "#             os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "#             cv2.imwrite(output_path, image)\n",
        "#             i += 1\n",
        "#             if i == generator.size():  # have written all images\n",
        "#                 break\n",
        "#             else:\n",
        "#                 continue\n",
        "\n",
        "#         # if we are using the GUI, then show an image\n",
        "#         cv2_imshow(image)\n",
        "#         key = cv2.waitKeyEx()\n",
        "\n",
        "#         # press right for next image and left for previous (linux or windows, doesn't work for macOS)\n",
        "#         # if you run macOS, press \"n\" or \"m\" (will also work on linux and windows)\n",
        "\n",
        "#         if key in rightkeys:\n",
        "#             i = (i + 1) % generator.size()\n",
        "#         if key in leftkeys:\n",
        "#             i -= 1\n",
        "#             if i < 0:\n",
        "#                 i = generator.size() - 1\n",
        "\n",
        "#         # press q or Esc to quit\n",
        "#         if (key == ord('q')) or (key == 27):\n",
        "#             return False\n",
        "\n",
        "#     return True\n",
        "\n",
        "\n",
        "# # train_gen, valid_gen = create_generators(args,b.preprocess_image)\n",
        "# while run(train_gen, args, parse_anchor_parameters(args.config)):\n",
        "#     pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSNyKpepHl5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15bcca96-0f51-446c-d70d-6afaacf6399a"
      },
      "source": [
        "import imgaug as ia\n",
        "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "# Define our sequence of augmentation steps that will be applied to every image.\n",
        "seq = iaa.Sequential(\n",
        "[      \n",
        "        # sometimes(iaa.Crop(percent=(0, 0.2))),\n",
        " \n",
        "        #  sometimes(iaa.Affine(\n",
        "        #     scale={\"x\": (0.9, 1.2), \"y\": (0.9, 1.2)},\n",
        "        #     translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "        #     rotate=(-45, 45),\n",
        "        #     shear=(-16, 16),\n",
        "        #     order=[0, 1],\n",
        "        #     cval=(0, 255),\n",
        "        #     mode=ia.ALL\n",
        "        # )),\n",
        "\n",
        "        #   # Change brightness of images (85-115% of original value).\n",
        "        #   iaa.Multiply((0.90, 1.15), per_channel=0.5),\n",
        "\n",
        "        #   # # Improve or worsen the contrast of images.\n",
        "        #   iaa.ContrastNormalization((0.75, 1.25), per_channel=0.5),\n",
        "\n",
        "          # Convert each image to grayscale and then overlay the\n",
        "          # result with the original with random alpha. I.e. remove\n",
        "          # colors with varying strengths.\n",
        "          # iaa.Grayscale(alpha=(0.0, 0.25)),\n",
        "#\n",
        "        # Execute 1 to 9 of the following (less important) augmenters per\n",
        "        # image. Don't execute all of them, as that would often be way too\n",
        "        # strong.\n",
        "        #\n",
        "        iaa.SomeOf((1, 9),\n",
        "            [\n",
        "\n",
        "                        # Blur each image with varying strength using\n",
        "                        # gaussian blur (sigma between 0 and .5),\n",
        "                        # average/uniform blur (kernel size 1x1)\n",
        "                        # median blur (kernel size 1x1).\n",
        "                        iaa.OneOf([\n",
        "                            iaa.GaussianBlur((0,1)),\n",
        "                            iaa.AverageBlur(k=(2,2)),\n",
        "                            iaa.MedianBlur(k=(1,1)),\n",
        "                        ]),\n",
        "\n",
        "                        # Sharpen each image, overlay the result with the original\n",
        "                        # image using an alpha between 0 (no sharpening) and 1\n",
        "                        # (full sharpening effect).\n",
        "                        iaa.Sharpen(alpha=(0, 0.25), lightness=(0.75, 1.5)),\n",
        "\n",
        "                        # Add gaussian noise to some images.\n",
        "                        # In 50% of these cases, the noise is randomly sampled per\n",
        "                        # channel and pixel.\n",
        "                        # In the other 50% of all cases it is sampled once per\n",
        "                        # pixel (i.e. brightness change).\n",
        "                        iaa.AdditiveGaussianNoise(\n",
        "                            loc=0, scale=(0.0, 0.01*255), per_channel=0.5\n",
        "                        ),\n",
        "\n",
        "                        # Either drop randomly 1 to 10% of all pixels (i.e. set\n",
        "                        # them to black) or drop them on an image with 2-5% percent\n",
        "                        # of the original size, leading to large dropped\n",
        "                        # rectangles.\n",
        "                        # iaa.OneOf([\n",
        "                        #     iaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
        "                        #     iaa.CoarseDropout(\n",
        "                        #         (0.03, 0.15), size_percent=(0.02, 0.05),\n",
        "                        #         per_channel=0.2\n",
        "                        #     ),\n",
        "                        # ]),\n",
        "             \n",
        "                        \n",
        "                        # Invert each image's channel with 5% probability.\n",
        "                        # This sets each pixel value v to 255-v.\n",
        "                        iaa.Invert(0.05, per_channel=True), # invert color channels\n",
        "\n",
        "                        # Add a value of -5 to 5 to each pixel.\n",
        "                        iaa.Add((-5, 5), per_channel=0.5),\n",
        "\n",
        "                        # # Change brightness of images (85-115% of original value).\n",
        "                        iaa.Multiply((0.85, 1.15), per_channel=0.5),\n",
        "\n",
        "                        # # Improve or worsen the contrast of images.\n",
        "                        iaa.ContrastNormalization((0.75, 1.25), per_channel=0.5),\n",
        "\n",
        "                        # Convert each image to grayscale and then overlay the\n",
        "                        # result with the original with random alpha. I.e. remove\n",
        "                        # colors with varying strengths.\n",
        "                        # iaa.Grayscale(alpha=(0.0, 0.25)),\n",
        "\n",
        "                        # In some images distort local areas with varying strength.\n",
        "                        sometimes(iaa.PiecewiseAffine(scale=(0.001, 0.01)))\n",
        "                    ],\n",
        "            # do all of the above augmentations in random order\n",
        "            random_order=True\n",
        "        )\n",
        "    ],\n",
        "    # do all of the above augmentations in random order\n",
        "    random_order=True\n",
        ")\n",
        "\n",
        "def augment_train_gen(train_gen, visualize=False):\n",
        "    '''\n",
        "    Creates a generator using another generator with applied image augmentation.\n",
        "    Args\n",
        "        train_gen  : keras-retinanet generator object.\n",
        "        visualize  : Boolean; False will convert bounding boxes to their anchor box targets for the model.\n",
        "    '''\n",
        "    imgs = []\n",
        "    boxes = []\n",
        "    targets = []\n",
        "    size = train_gen.size()\n",
        "    idx = 0\n",
        "    while True:\n",
        "        while len(imgs) < args.batch_size:\n",
        "            image       = train_gen.load_image(idx % size)\n",
        "            annotations = train_gen.load_annotations(idx % size)\n",
        "            image,annotations = train_gen.random_transform_group_entry(image,annotations)\n",
        "            imgs.append(image)            \n",
        "            boxes.append(annotations['bboxes'])\n",
        "            targets.append(annotations)\n",
        "            idx += 1\n",
        "        if visualize:\n",
        "            imgs = seq.augment_images(imgs)\n",
        "            imgs = np.array(imgs)\n",
        "            boxes = np.array(boxes)\n",
        "            yield imgs,boxes\n",
        "        else:\n",
        "            imgs = seq.augment_images(imgs)\n",
        "            imgs,targets = train_gen.preprocess_group(imgs,targets)\n",
        "            imgs = train_gen.compute_inputs(imgs)\n",
        "            targets = train_gen.compute_targets(imgs,targets)\n",
        "            imgs = np.array(imgs)\n",
        "            yield imgs,targets\n",
        "        imgs = []\n",
        "        boxes = []\n",
        "        targets = []\n",
        "\t\t\n",
        "\t\t\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# skip_batches = 5\n",
        "# i = 0\n",
        "# for imgs,boxes in augment_train_gen(train_gen,visualize=True):\n",
        "#     if i > skip_batches:\n",
        "#         fig=plt.figure(figsize=(24,96))\n",
        "#         columns = 2\n",
        "#         rows = 8\n",
        "#         for i in range(1, columns*rows + 1):\n",
        "#             draw_boxes(imgs[i], boxes[i], (0, 255, 0), thickness=1)\n",
        "#             fig.add_subplot(rows, columns, i)\n",
        "#             plt.imshow(cv2.cvtColor(imgs[i],cv2.COLOR_BGR2RGB))\n",
        "#         plt.show()\n",
        "#         break\n",
        "#     else:\n",
        "#         i += 1\n",
        "\n",
        "model, training_model, prediction_model = create_models(\n",
        "            backbone_retinanet=b.retinanet,\n",
        "            num_classes=train_gen.num_classes(),\n",
        "            weights=None,\n",
        "            multi_gpu=False,\n",
        "            freeze_backbone=True,\n",
        "            lr=1e-3,\n",
        "            config=args.config\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable:0' shape=(15, 4) dtype=float32, numpy=\n",
            "array([[-16.       , -16.       ,  16.       ,  16.       ],\n",
            "       [-19.2      , -19.2      ,  19.2      ,  19.2      ],\n",
            "       [-25.6      , -25.6      ,  25.6      ,  25.6      ],\n",
            "       [-13.063946 , -19.595919 ,  13.063946 ,  19.595919 ],\n",
            "       [-15.676735 , -23.515102 ,  15.676735 ,  23.515102 ],\n",
            "       [-20.902313 , -31.35347  ,  20.902313 ,  31.35347  ],\n",
            "       [-11.313708 , -22.627417 ,  11.313708 ,  22.627417 ],\n",
            "       [-13.57645  , -27.1529   ,  13.57645  ,  27.1529   ],\n",
            "       [-18.101934 , -36.20387  ,  18.101934 ,  36.20387  ],\n",
            "       [-10.119288 , -25.298222 ,  10.119288 ,  25.298222 ],\n",
            "       [-12.1431465, -30.357866 ,  12.1431465,  30.357866 ],\n",
            "       [-16.190863 , -40.477154 ,  16.190863 ,  40.477154 ],\n",
            "       [ -9.237604 , -27.712812 ,   9.237604 ,  27.712812 ],\n",
            "       [-11.085126 , -33.25538  ,  11.085126 ,  33.25538  ],\n",
            "       [-14.780168 , -44.3405   ,  14.780168 ,  44.3405   ]],\n",
            "      dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(15, 4) dtype=float32, numpy=\n",
            "array([[-32.      , -32.      ,  32.      ,  32.      ],\n",
            "       [-38.4     , -38.4     ,  38.4     ,  38.4     ],\n",
            "       [-51.2     , -51.2     ,  51.2     ,  51.2     ],\n",
            "       [-26.127892, -39.191837,  26.127892,  39.191837],\n",
            "       [-31.35347 , -47.030205,  31.35347 ,  47.030205],\n",
            "       [-41.804626, -62.70694 ,  41.804626,  62.70694 ],\n",
            "       [-22.627417, -45.254833,  22.627417,  45.254833],\n",
            "       [-27.1529  , -54.3058  ,  27.1529  ,  54.3058  ],\n",
            "       [-36.20387 , -72.40774 ,  36.20387 ,  72.40774 ],\n",
            "       [-20.238577, -50.596443,  20.238577,  50.596443],\n",
            "       [-24.286293, -60.715733,  24.286293,  60.715733],\n",
            "       [-32.381725, -80.95431 ,  32.381725,  80.95431 ],\n",
            "       [-18.475208, -55.425625,  18.475208,  55.425625],\n",
            "       [-22.170252, -66.51076 ,  22.170252,  66.51076 ],\n",
            "       [-29.560335, -88.681   ,  29.560335,  88.681   ]], dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(15, 4) dtype=float32, numpy=\n",
            "array([[ -64.      ,  -64.      ,   64.      ,   64.      ],\n",
            "       [ -76.8     ,  -76.8     ,   76.8     ,   76.8     ],\n",
            "       [-102.4     , -102.4     ,  102.4     ,  102.4     ],\n",
            "       [ -52.255783,  -78.383675,   52.255783,   78.383675],\n",
            "       [ -62.70694 ,  -94.06041 ,   62.70694 ,   94.06041 ],\n",
            "       [ -83.60925 , -125.41388 ,   83.60925 ,  125.41388 ],\n",
            "       [ -45.254833,  -90.50967 ,   45.254833,   90.50967 ],\n",
            "       [ -54.3058  , -108.6116  ,   54.3058  ,  108.6116  ],\n",
            "       [ -72.40774 , -144.81548 ,   72.40774 ,  144.81548 ],\n",
            "       [ -40.477154, -101.19289 ,   40.477154,  101.19289 ],\n",
            "       [ -48.572586, -121.431465,   48.572586,  121.431465],\n",
            "       [ -64.76345 , -161.90862 ,   64.76345 ,  161.90862 ],\n",
            "       [ -36.950417, -110.85125 ,   36.950417,  110.85125 ],\n",
            "       [ -44.340504, -133.02151 ,   44.340504,  133.02151 ],\n",
            "       [ -59.12067 , -177.362   ,   59.12067 ,  177.362   ]],\n",
            "      dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(15, 4) dtype=float32, numpy=\n",
            "array([[-128.      , -128.      ,  128.      ,  128.      ],\n",
            "       [-153.6     , -153.6     ,  153.6     ,  153.6     ],\n",
            "       [-204.8     , -204.8     ,  204.8     ,  204.8     ],\n",
            "       [-104.511566, -156.76735 ,  104.511566,  156.76735 ],\n",
            "       [-125.41388 , -188.12082 ,  125.41388 ,  188.12082 ],\n",
            "       [-167.2185  , -250.82776 ,  167.2185  ,  250.82776 ],\n",
            "       [ -90.50967 , -181.01933 ,   90.50967 ,  181.01933 ],\n",
            "       [-108.6116  , -217.2232  ,  108.6116  ,  217.2232  ],\n",
            "       [-144.81548 , -289.63095 ,  144.81548 ,  289.63095 ],\n",
            "       [ -80.95431 , -202.38577 ,   80.95431 ,  202.38577 ],\n",
            "       [ -97.14517 , -242.86293 ,   97.14517 ,  242.86293 ],\n",
            "       [-129.5269  , -323.81723 ,  129.5269  ,  323.81723 ],\n",
            "       [ -73.90083 , -221.7025  ,   73.90083 ,  221.7025  ],\n",
            "       [ -88.68101 , -266.04303 ,   88.68101 ,  266.04303 ],\n",
            "       [-118.24134 , -354.724   ,  118.24134 ,  354.724   ]],\n",
            "      dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(15, 4) dtype=float32, numpy=\n",
            "array([[-256.     , -256.     ,  256.     ,  256.     ],\n",
            "       [-307.2    , -307.2    ,  307.2    ,  307.2    ],\n",
            "       [-409.6    , -409.6    ,  409.6    ,  409.6    ],\n",
            "       [-209.02313, -313.5347 ,  209.02313,  313.5347 ],\n",
            "       [-250.82776, -376.24164,  250.82776,  376.24164],\n",
            "       [-334.437  , -501.65552,  334.437  ,  501.65552],\n",
            "       [-181.01933, -362.03867,  181.01933,  362.03867],\n",
            "       [-217.2232 , -434.4464 ,  217.2232 ,  434.4464 ],\n",
            "       [-289.63095, -579.2619 ,  289.63095,  579.2619 ],\n",
            "       [-161.90862, -404.77155,  161.90862,  404.77155],\n",
            "       [-194.29034, -485.72586,  194.29034,  485.72586],\n",
            "       [-259.0538 , -647.63446,  259.0538 ,  647.63446],\n",
            "       [-147.80167, -443.405  ,  147.80167,  443.405  ],\n",
            "       [-177.36201, -532.08606,  177.36201,  532.08606],\n",
            "       [-236.48268, -709.448  ,  236.48268,  709.448  ]], dtype=float32)> anchors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k4kPiPem4dc",
        "colab_type": "text"
      },
      "source": [
        "# Retraining the foot model with better parameters\n",
        "- config file  -> # Updating the parameters\n",
        "# New ratios parameters for training as previous didn't help\n",
        "with open('config.ini','w') as f:\n",
        "    f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512 1024\\nstrides = 8 16 32 64 128 256\\nratios  = 1 1.5 2 2.5 3\\nscales  = 1 1.2 1.6\\n')\n",
        "- iou 0.6 NMS 0.5 and score threshold 0.15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlqsEOlvUcHQ",
        "colab_type": "text"
      },
      "source": [
        "## Continued training from 21 epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-jIy5oUUbTo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e89bb03-b812-4336-bca1-589730976d8d"
      },
      "source": [
        "from keras_retinanet.callbacks import RedirectModel \n",
        "callbacks = create_callbacks(\n",
        "    model,\n",
        "    training_model,\n",
        "    prediction_model,\n",
        "    valid_gen,\n",
        "    args,\n",
        ")\n",
        "training_model.load_weights('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_21_class_loss_0.0973.h5', skip_mismatch = True, by_name = True)\n",
        "\n",
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks, ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "44/44 [==============================] - 258s 6s/step - loss: 1.4052 - regression_loss: 1.2905 - classification_loss: 0.1146\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_01.h5\n",
            "Epoch 2/200\n",
            "44/44 [==============================] - 227s 5s/step - loss: 1.4634 - regression_loss: 1.3478 - classification_loss: 0.1156\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_02.h5\n",
            "Epoch 3/200\n",
            "44/44 [==============================] - 242s 5s/step - loss: 1.4614 - regression_loss: 1.3458 - classification_loss: 0.1157\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_03.h5\n",
            "Epoch 4/200\n",
            "44/44 [==============================] - 234s 5s/step - loss: 1.4859 - regression_loss: 1.3659 - classification_loss: 0.1200\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_04.h5\n",
            "Epoch 5/200\n",
            "44/44 [==============================] - 244s 6s/step - loss: 1.4409 - regression_loss: 1.3247 - classification_loss: 0.1162\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_05.h5\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 6/200\n",
            "44/44 [==============================] - 245s 6s/step - loss: 1.3897 - regression_loss: 1.2784 - classification_loss: 0.1114\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_06.h5\n",
            "Epoch 7/200\n",
            "44/44 [==============================] - 208s 5s/step - loss: 1.3457 - regression_loss: 1.2432 - classification_loss: 0.1025\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_07.h5\n",
            "Epoch 8/200\n",
            "44/44 [==============================] - 236s 5s/step - loss: 1.3330 - regression_loss: 1.2351 - classification_loss: 0.0979\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_08.h5\n",
            "Epoch 9/200\n",
            "44/44 [==============================] - 240s 5s/step - loss: 1.3067 - regression_loss: 1.2155 - classification_loss: 0.0911\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_09.h5\n",
            "Epoch 10/200\n",
            "44/44 [==============================] - 238s 5s/step - loss: 1.2904 - regression_loss: 1.1966 - classification_loss: 0.0938\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_10.h5\n",
            "Epoch 11/200\n",
            "44/44 [==============================] - 241s 5s/step - loss: 1.3014 - regression_loss: 1.2102 - classification_loss: 0.0911\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_11.h5\n",
            "Epoch 12/200\n",
            "44/44 [==============================] - 233s 5s/step - loss: 1.3038 - regression_loss: 1.2132 - classification_loss: 0.0906\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_12.h5\n",
            "Epoch 13/200\n",
            "44/44 [==============================] - 237s 5s/step - loss: 1.2689 - regression_loss: 1.1816 - classification_loss: 0.0873\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_13.h5\n",
            "Epoch 14/200\n",
            "44/44 [==============================] - 230s 5s/step - loss: 1.2629 - regression_loss: 1.1742 - classification_loss: 0.0887\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_14.h5\n",
            "Epoch 15/200\n",
            "44/44 [==============================] - 247s 6s/step - loss: 1.2790 - regression_loss: 1.1863 - classification_loss: 0.0927\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_15.h5\n",
            "Epoch 16/200\n",
            "44/44 [==============================] - 226s 5s/step - loss: 1.2724 - regression_loss: 1.1834 - classification_loss: 0.0890\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_16.h5\n",
            "Epoch 17/200\n",
            "44/44 [==============================] - 224s 5s/step - loss: 1.2727 - regression_loss: 1.1843 - classification_loss: 0.0884\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_17.h5\n",
            "Epoch 18/200\n",
            "44/44 [==============================] - 229s 5s/step - loss: 1.2765 - regression_loss: 1.1861 - classification_loss: 0.0904\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_18.h5\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 19/200\n",
            "44/44 [==============================] - 229s 5s/step - loss: 1.2427 - regression_loss: 1.1542 - classification_loss: 0.0885\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_19.h5\n",
            "Epoch 20/200\n",
            "44/44 [==============================] - 245s 6s/step - loss: 1.2512 - regression_loss: 1.1658 - classification_loss: 0.0854\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_20.h5\n",
            "Epoch 21/200\n",
            "44/44 [==============================] - 245s 6s/step - loss: 1.2303 - regression_loss: 1.1455 - classification_loss: 0.0848\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2 continued/resnet50_csv_21.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f5363289860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7M2teljm4LW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "692de267-b7e7-4c15-ad07-a1efadc150c0"
      },
      "source": [
        "from keras_retinanet.callbacks import RedirectModel \n",
        "callbacks = create_callbacks(\n",
        "    model,\n",
        "    training_model,\n",
        "    prediction_model,\n",
        "    valid_gen,\n",
        "    args,\n",
        ")\n",
        "training_model.load_weights('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_98_class_loss_0.2137.h5', skip_mismatch = True, by_name = True)\n",
        "\n",
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks, ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer regression_submodel due to mismatch in shape ((3, 3, 256, 60) vs (132, 256, 3, 3)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer regression_submodel due to mismatch in shape ((60,) vs (132,)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((3, 3, 256, 75) vs (165, 256, 3, 3)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((75,) vs (165,)).\n",
            "  weight_values[i].shape))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "44/44 [==============================] - 281s 6s/step - loss: 2.2704 - regression_loss: 1.8941 - classification_loss: 0.3763\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_01.h5\n",
            "Epoch 2/200\n",
            "44/44 [==============================] - 215s 5s/step - loss: 1.6377 - regression_loss: 1.4739 - classification_loss: 0.1638\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_02.h5\n",
            "Epoch 3/200\n",
            "44/44 [==============================] - 225s 5s/step - loss: 1.5598 - regression_loss: 1.4174 - classification_loss: 0.1424\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_03.h5\n",
            "Epoch 4/200\n",
            "44/44 [==============================] - 217s 5s/step - loss: 1.5352 - regression_loss: 1.4024 - classification_loss: 0.1329\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_04.h5\n",
            "Epoch 5/200\n",
            "44/44 [==============================] - 225s 5s/step - loss: 1.5089 - regression_loss: 1.3790 - classification_loss: 0.1299\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_05.h5\n",
            "Epoch 6/200\n",
            "44/44 [==============================] - 226s 5s/step - loss: 1.5225 - regression_loss: 1.3886 - classification_loss: 0.1340\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_06.h5\n",
            "Epoch 7/200\n",
            "44/44 [==============================] - 192s 4s/step - loss: 1.5119 - regression_loss: 1.3844 - classification_loss: 0.1275\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_07.h5\n",
            "Epoch 8/200\n",
            "44/44 [==============================] - 218s 5s/step - loss: 1.5360 - regression_loss: 1.3942 - classification_loss: 0.1418\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_08.h5\n",
            "Epoch 9/200\n",
            "44/44 [==============================] - 221s 5s/step - loss: 1.5211 - regression_loss: 1.3901 - classification_loss: 0.1309\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_09.h5\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 10/200\n",
            "44/44 [==============================] - 217s 5s/step - loss: 1.4550 - regression_loss: 1.3377 - classification_loss: 0.1173\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_10.h5\n",
            "Epoch 11/200\n",
            "44/44 [==============================] - 223s 5s/step - loss: 1.4162 - regression_loss: 1.3090 - classification_loss: 0.1072\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_11.h5\n",
            "Epoch 12/200\n",
            "44/44 [==============================] - 216s 5s/step - loss: 1.3843 - regression_loss: 1.2796 - classification_loss: 0.1047\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_12.h5\n",
            "Epoch 13/200\n",
            "44/44 [==============================] - 220s 5s/step - loss: 1.3460 - regression_loss: 1.2483 - classification_loss: 0.0977\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_13.h5\n",
            "Epoch 14/200\n",
            "44/44 [==============================] - 213s 5s/step - loss: 1.3390 - regression_loss: 1.2427 - classification_loss: 0.0963\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_14.h5\n",
            "Epoch 15/200\n",
            "44/44 [==============================] - 227s 5s/step - loss: 1.3637 - regression_loss: 1.2589 - classification_loss: 0.1049\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_15.h5\n",
            "Epoch 16/200\n",
            "44/44 [==============================] - 211s 5s/step - loss: 1.3387 - regression_loss: 1.2423 - classification_loss: 0.0964\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_16.h5\n",
            "Epoch 17/200\n",
            "44/44 [==============================] - 207s 5s/step - loss: 1.3088 - regression_loss: 1.2138 - classification_loss: 0.0950\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_17.h5\n",
            "Epoch 18/200\n",
            "44/44 [==============================] - 212s 5s/step - loss: 1.3408 - regression_loss: 1.2410 - classification_loss: 0.0998\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_18.h5\n",
            "Epoch 19/200\n",
            "44/44 [==============================] - 211s 5s/step - loss: 1.3220 - regression_loss: 1.2244 - classification_loss: 0.0976\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_19.h5\n",
            "Epoch 20/200\n",
            "44/44 [==============================] - 225s 5s/step - loss: 1.3249 - regression_loss: 1.2265 - classification_loss: 0.0984\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_20.h5\n",
            "Epoch 21/200\n",
            "44/44 [==============================] - 226s 5s/step - loss: 1.3050 - regression_loss: 1.2077 - classification_loss: 0.0973\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/Model v2/resnet50_csv_21.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb7221799b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebvMYs3mnaOt",
        "colab_type": "text"
      },
      "source": [
        "# Previous version -> picked up class loss 0.2137 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtB95pB_Hlt5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "aa476dd5-723b-4086-88e3-2d81c91fedd3"
      },
      "source": [
        "callbacks = create_callbacks(\n",
        "    model,\n",
        "    training_model,\n",
        "    prediction_model,\n",
        "    valid_gen,\n",
        "    args,\n",
        ")\n",
        "training_model.load_weights('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/resnet50_coco_best_v2.0.1.h5', skip_mismatch = True, by_name = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer regression_submodel due to mismatch in shape ((3, 3, 256, 132) vs (36, 256, 3, 3)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer regression_submodel due to mismatch in shape ((132,) vs (36,)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((3, 3, 256, 165) vs (720, 256, 3, 3)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((165,) vs (720,)).\n",
            "  weight_values[i].shape))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiSi6hsYeJzq",
        "colab_type": "text"
      },
      "source": [
        "## Will train a new model for Foot joints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWaSEKVoeKRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5RhvEoEnjaE",
        "colab_type": "text"
      },
      "source": [
        "# Training the final foot model with below parameters\n",
        "- Iou = 0.5\n",
        "- NMS = 0.5\n",
        "- score threshold = 0.05\n",
        "- Ratios = 0.8 1 1.3 1.5 1.8 2 2.2 2.5 2.8 3.0 3.2\n",
        "- scales  = 1 1.2 1.6\n",
        "- IMgae min size = 1000\n",
        "- Image max side = 1400 \n",
        "- Evaluation = False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSIzwCL2okIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "043890a9-ee0c-4253-b45b-42d72d6d1e1d"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks, ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "88/88 [==============================] - 270s 3s/step - loss: 2.9207 - regression_loss: 2.2971 - classification_loss: 0.6236\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_01.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `mAP` which is not available. Available metrics are: loss,regression_loss,classification_loss,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/200\n",
            "88/88 [==============================] - 263s 3s/step - loss: 2.5422 - regression_loss: 2.1210 - classification_loss: 0.4212\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_02.h5\n",
            "Epoch 3/200\n",
            "88/88 [==============================] - 285s 3s/step - loss: 2.4291 - regression_loss: 2.0359 - classification_loss: 0.3932\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_03.h5\n",
            "Epoch 4/200\n",
            "88/88 [==============================] - 264s 3s/step - loss: 2.3440 - regression_loss: 1.9798 - classification_loss: 0.3642\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_04.h5\n",
            "Epoch 5/200\n",
            "88/88 [==============================] - 278s 3s/step - loss: 2.3815 - regression_loss: 2.0251 - classification_loss: 0.3564\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_05.h5\n",
            "Epoch 6/200\n",
            "88/88 [==============================] - 260s 3s/step - loss: 2.2749 - regression_loss: 1.9476 - classification_loss: 0.3273\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_06.h5\n",
            "Epoch 7/200\n",
            "88/88 [==============================] - 259s 3s/step - loss: 2.3171 - regression_loss: 1.9680 - classification_loss: 0.3491\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_07.h5\n",
            "Epoch 8/200\n",
            "88/88 [==============================] - 253s 3s/step - loss: 2.2952 - regression_loss: 1.9750 - classification_loss: 0.3202\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_08.h5\n",
            "Epoch 9/200\n",
            "88/88 [==============================] - 265s 3s/step - loss: 2.2286 - regression_loss: 1.9220 - classification_loss: 0.3066\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_09.h5\n",
            "Epoch 10/200\n",
            "88/88 [==============================] - 262s 3s/step - loss: 2.2647 - regression_loss: 1.9425 - classification_loss: 0.3222\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_10.h5\n",
            "Epoch 11/200\n",
            "88/88 [==============================] - 278s 3s/step - loss: 2.2195 - regression_loss: 1.9148 - classification_loss: 0.3046\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_11.h5\n",
            "Epoch 12/200\n",
            "88/88 [==============================] - 259s 3s/step - loss: 2.2300 - regression_loss: 1.9169 - classification_loss: 0.3131\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_12.h5\n",
            "Epoch 13/200\n",
            "88/88 [==============================] - 262s 3s/step - loss: 2.2385 - regression_loss: 1.9377 - classification_loss: 0.3008\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_13.h5\n",
            "Epoch 14/200\n",
            "88/88 [==============================] - 294s 3s/step - loss: 2.2400 - regression_loss: 1.9443 - classification_loss: 0.2958\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_14.h5\n",
            "Epoch 15/200\n",
            "88/88 [==============================] - 277s 3s/step - loss: 2.2425 - regression_loss: 1.9454 - classification_loss: 0.2970\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_15.h5\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 16/200\n",
            "88/88 [==============================] - 284s 3s/step - loss: 2.1697 - regression_loss: 1.9039 - classification_loss: 0.2659\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_16.h5\n",
            "Epoch 17/200\n",
            "88/88 [==============================] - 257s 3s/step - loss: 2.0829 - regression_loss: 1.8363 - classification_loss: 0.2466\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_17.h5\n",
            "Epoch 18/200\n",
            "88/88 [==============================] - 277s 3s/step - loss: 2.0904 - regression_loss: 1.8453 - classification_loss: 0.2450\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_18.h5\n",
            "Epoch 19/200\n",
            "88/88 [==============================] - 263s 3s/step - loss: 2.0699 - regression_loss: 1.8222 - classification_loss: 0.2477\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_19.h5\n",
            "Epoch 20/200\n",
            "88/88 [==============================] - 261s 3s/step - loss: 2.1073 - regression_loss: 1.8564 - classification_loss: 0.2509\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_20.h5\n",
            "Epoch 21/200\n",
            "88/88 [==============================] - 254s 3s/step - loss: 2.0711 - regression_loss: 1.8294 - classification_loss: 0.2417\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_21.h5\n",
            "Epoch 22/200\n",
            "88/88 [==============================] - 276s 3s/step - loss: 2.0946 - regression_loss: 1.8496 - classification_loss: 0.2450\n",
            "\n",
            "Epoch 00022: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_22.h5\n",
            "Epoch 23/200\n",
            "88/88 [==============================] - 265s 3s/step - loss: 2.0608 - regression_loss: 1.8218 - classification_loss: 0.2390\n",
            "\n",
            "Epoch 00023: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_23.h5\n",
            "Epoch 24/200\n",
            "88/88 [==============================] - 256s 3s/step - loss: 2.0492 - regression_loss: 1.8149 - classification_loss: 0.2344\n",
            "\n",
            "Epoch 00024: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_24.h5\n",
            "Epoch 25/200\n",
            "88/88 [==============================] - 251s 3s/step - loss: 2.0389 - regression_loss: 1.8046 - classification_loss: 0.2343\n",
            "\n",
            "Epoch 00025: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_25.h5\n",
            "Epoch 26/200\n",
            "88/88 [==============================] - 254s 3s/step - loss: 2.0626 - regression_loss: 1.8251 - classification_loss: 0.2374\n",
            "\n",
            "Epoch 00026: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_26.h5\n",
            "Epoch 27/200\n",
            "88/88 [==============================] - 258s 3s/step - loss: 1.9915 - regression_loss: 1.7659 - classification_loss: 0.2256\n",
            "\n",
            "Epoch 00027: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_27.h5\n",
            "Epoch 28/200\n",
            "88/88 [==============================] - 260s 3s/step - loss: 2.0295 - regression_loss: 1.7976 - classification_loss: 0.2318\n",
            "\n",
            "Epoch 00028: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_28.h5\n",
            "Epoch 29/200\n",
            "88/88 [==============================] - 274s 3s/step - loss: 2.0305 - regression_loss: 1.7958 - classification_loss: 0.2347\n",
            "\n",
            "Epoch 00029: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_29.h5\n",
            "Epoch 30/200\n",
            "88/88 [==============================] - 269s 3s/step - loss: 1.9700 - regression_loss: 1.7433 - classification_loss: 0.2268\n",
            "\n",
            "Epoch 00030: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_30.h5\n",
            "Epoch 31/200\n",
            "88/88 [==============================] - 262s 3s/step - loss: 2.0320 - regression_loss: 1.7970 - classification_loss: 0.2350\n",
            "\n",
            "Epoch 00031: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_31.h5\n",
            "Epoch 32/200\n",
            "88/88 [==============================] - 277s 3s/step - loss: 2.0062 - regression_loss: 1.7741 - classification_loss: 0.2321\n",
            "\n",
            "Epoch 00032: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_32.h5\n",
            "Epoch 33/200\n",
            "88/88 [==============================] - 268s 3s/step - loss: 1.9895 - regression_loss: 1.7637 - classification_loss: 0.2258\n",
            "\n",
            "Epoch 00033: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_33.h5\n",
            "Epoch 34/200\n",
            "88/88 [==============================] - 280s 3s/step - loss: 2.0141 - regression_loss: 1.7844 - classification_loss: 0.2297\n",
            "\n",
            "Epoch 00034: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_34.h5\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 35/200\n",
            "88/88 [==============================] - 262s 3s/step - loss: 2.0037 - regression_loss: 1.7801 - classification_loss: 0.2235\n",
            "\n",
            "Epoch 00035: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_35.h5\n",
            "Epoch 36/200\n",
            "88/88 [==============================] - 256s 3s/step - loss: 1.9997 - regression_loss: 1.7702 - classification_loss: 0.2296\n",
            "\n",
            "Epoch 00036: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_36.h5\n",
            "Epoch 37/200\n",
            "88/88 [==============================] - 268s 3s/step - loss: 1.9414 - regression_loss: 1.7236 - classification_loss: 0.2178\n",
            "\n",
            "Epoch 00037: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_37.h5\n",
            "Epoch 38/200\n",
            "88/88 [==============================] - 268s 3s/step - loss: 2.0133 - regression_loss: 1.7867 - classification_loss: 0.2266\n",
            "\n",
            "Epoch 00038: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_38.h5\n",
            "Epoch 39/200\n",
            "88/88 [==============================] - 264s 3s/step - loss: 1.9675 - regression_loss: 1.7462 - classification_loss: 0.2213\n",
            "\n",
            "Epoch 00039: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_39.h5\n",
            "Epoch 40/200\n",
            "88/88 [==============================] - 294s 3s/step - loss: 1.9830 - regression_loss: 1.7523 - classification_loss: 0.2307\n",
            "\n",
            "Epoch 00040: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_40.h5\n",
            "Epoch 41/200\n",
            "88/88 [==============================] - 264s 3s/step - loss: 2.0411 - regression_loss: 1.8061 - classification_loss: 0.2351\n",
            "\n",
            "Epoch 00041: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_41.h5\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 42/200\n",
            "88/88 [==============================] - 263s 3s/step - loss: 1.9722 - regression_loss: 1.7487 - classification_loss: 0.2235\n",
            "\n",
            "Epoch 00042: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_42.h5\n",
            "Epoch 43/200\n",
            "88/88 [==============================] - 270s 3s/step - loss: 1.9613 - regression_loss: 1.7426 - classification_loss: 0.2187\n",
            "\n",
            "Epoch 00043: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_43.h5\n",
            "Epoch 44/200\n",
            "88/88 [==============================] - 276s 3s/step - loss: 2.0379 - regression_loss: 1.8044 - classification_loss: 0.2335\n",
            "\n",
            "Epoch 00044: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_44.h5\n",
            "Epoch 45/200\n",
            "88/88 [==============================] - 274s 3s/step - loss: 2.0391 - regression_loss: 1.8058 - classification_loss: 0.2333\n",
            "\n",
            "Epoch 00045: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_45.h5\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "Epoch 46/200\n",
            "88/88 [==============================] - 271s 3s/step - loss: 1.9609 - regression_loss: 1.7395 - classification_loss: 0.2214\n",
            "\n",
            "Epoch 00046: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_46.h5\n",
            "Epoch 47/200\n",
            "88/88 [==============================] - 272s 3s/step - loss: 2.0333 - regression_loss: 1.7960 - classification_loss: 0.2373\n",
            "\n",
            "Epoch 00047: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_47.h5\n",
            "Epoch 48/200\n",
            "88/88 [==============================] - 268s 3s/step - loss: 2.0107 - regression_loss: 1.7782 - classification_loss: 0.2325\n",
            "\n",
            "Epoch 00048: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_48.h5\n",
            "Epoch 49/200\n",
            "88/88 [==============================] - 267s 3s/step - loss: 1.9835 - regression_loss: 1.7611 - classification_loss: 0.2224\n",
            "\n",
            "Epoch 00049: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_49.h5\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "Epoch 50/200\n",
            "88/88 [==============================] - 269s 3s/step - loss: 2.0170 - regression_loss: 1.7837 - classification_loss: 0.2333\n",
            "\n",
            "Epoch 00050: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_50.h5\n",
            "Epoch 51/200\n",
            "88/88 [==============================] - 272s 3s/step - loss: 1.9678 - regression_loss: 1.7501 - classification_loss: 0.2177\n",
            "\n",
            "Epoch 00051: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_51.h5\n",
            "Epoch 52/200\n",
            "88/88 [==============================] - 282s 3s/step - loss: 1.9747 - regression_loss: 1.7558 - classification_loss: 0.2189\n",
            "\n",
            "Epoch 00052: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_52.h5\n",
            "Epoch 53/200\n",
            "88/88 [==============================] - 271s 3s/step - loss: 2.0476 - regression_loss: 1.8115 - classification_loss: 0.2361\n",
            "\n",
            "Epoch 00053: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_53.h5\n",
            "\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
            "Epoch 54/200\n",
            "88/88 [==============================] - 261s 3s/step - loss: 1.9818 - regression_loss: 1.7618 - classification_loss: 0.2199\n",
            "\n",
            "Epoch 00054: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_54.h5\n",
            "Epoch 55/200\n",
            "88/88 [==============================] - 253s 3s/step - loss: 2.0347 - regression_loss: 1.8018 - classification_loss: 0.2329\n",
            "\n",
            "Epoch 00055: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_55.h5\n",
            "Epoch 56/200\n",
            "88/88 [==============================] - 271s 3s/step - loss: 1.9765 - regression_loss: 1.7552 - classification_loss: 0.2213\n",
            "\n",
            "Epoch 00056: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_56.h5\n",
            "Epoch 57/200\n",
            "88/88 [==============================] - 279s 3s/step - loss: 1.9709 - regression_loss: 1.7505 - classification_loss: 0.2205\n",
            "\n",
            "Epoch 00057: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_57.h5\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
            "Epoch 58/200\n",
            "88/88 [==============================] - 260s 3s/step - loss: 1.9457 - regression_loss: 1.7261 - classification_loss: 0.2196\n",
            "\n",
            "Epoch 00058: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_58.h5\n",
            "Epoch 59/200\n",
            "88/88 [==============================] - 280s 3s/step - loss: 2.0136 - regression_loss: 1.7848 - classification_loss: 0.2288\n",
            "\n",
            "Epoch 00059: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_59.h5\n",
            "Epoch 60/200\n",
            "88/88 [==============================] - 253s 3s/step - loss: 1.9886 - regression_loss: 1.7636 - classification_loss: 0.2250\n",
            "\n",
            "Epoch 00060: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_60.h5\n",
            "Epoch 61/200\n",
            "88/88 [==============================] - 261s 3s/step - loss: 1.9783 - regression_loss: 1.7576 - classification_loss: 0.2207\n",
            "\n",
            "Epoch 00061: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_61.h5\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
            "Epoch 62/200\n",
            "88/88 [==============================] - 260s 3s/step - loss: 1.9885 - regression_loss: 1.7602 - classification_loss: 0.2283\n",
            "\n",
            "Epoch 00062: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_62.h5\n",
            "Epoch 63/200\n",
            "88/88 [==============================] - 276s 3s/step - loss: 2.0326 - regression_loss: 1.7984 - classification_loss: 0.2342\n",
            "\n",
            "Epoch 00063: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_63.h5\n",
            "Epoch 64/200\n",
            "88/88 [==============================] - 261s 3s/step - loss: 1.9815 - regression_loss: 1.7616 - classification_loss: 0.2199\n",
            "\n",
            "Epoch 00064: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_64.h5\n",
            "Epoch 65/200\n",
            "88/88 [==============================] - 262s 3s/step - loss: 1.9606 - regression_loss: 1.7420 - classification_loss: 0.2186\n",
            "\n",
            "Epoch 00065: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_65.h5\n",
            "\n",
            "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
            "Epoch 66/200\n",
            "88/88 [==============================] - 267s 3s/step - loss: 1.9964 - regression_loss: 1.7724 - classification_loss: 0.2240\n",
            "\n",
            "Epoch 00066: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_66.h5\n",
            "Epoch 67/200\n",
            "88/88 [==============================] - 266s 3s/step - loss: 2.0677 - regression_loss: 1.8281 - classification_loss: 0.2396\n",
            "\n",
            "Epoch 00067: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_67.h5\n",
            "Epoch 68/200\n",
            "88/88 [==============================] - 260s 3s/step - loss: 1.9709 - regression_loss: 1.7475 - classification_loss: 0.2234\n",
            "\n",
            "Epoch 00068: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_68.h5\n",
            "Epoch 69/200\n",
            "88/88 [==============================] - 280s 3s/step - loss: 2.0347 - regression_loss: 1.7970 - classification_loss: 0.2377\n",
            "\n",
            "Epoch 00069: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_69.h5\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
            "Epoch 70/200\n",
            "88/88 [==============================] - 280s 3s/step - loss: 1.9377 - regression_loss: 1.7211 - classification_loss: 0.2166\n",
            "\n",
            "Epoch 00070: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_70.h5\n",
            "Epoch 71/200\n",
            "88/88 [==============================] - 273s 3s/step - loss: 1.9646 - regression_loss: 1.7392 - classification_loss: 0.2254\n",
            "\n",
            "Epoch 00071: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_71.h5\n",
            "Epoch 72/200\n",
            "88/88 [==============================] - 257s 3s/step - loss: 1.9797 - regression_loss: 1.7544 - classification_loss: 0.2253\n",
            "\n",
            "Epoch 00072: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_72.h5\n",
            "Epoch 73/200\n",
            "88/88 [==============================] - 254s 3s/step - loss: 1.9818 - regression_loss: 1.7558 - classification_loss: 0.2260\n",
            "\n",
            "Epoch 00073: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_73.h5\n",
            "Epoch 74/200\n",
            "88/88 [==============================] - 279s 3s/step - loss: 1.9639 - regression_loss: 1.7410 - classification_loss: 0.2229\n",
            "\n",
            "Epoch 00074: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_74.h5\n",
            "\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
            "Epoch 75/200\n",
            "88/88 [==============================] - 264s 3s/step - loss: 2.0423 - regression_loss: 1.8008 - classification_loss: 0.2415\n",
            "\n",
            "Epoch 00075: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_75.h5\n",
            "Epoch 76/200\n",
            "88/88 [==============================] - 259s 3s/step - loss: 1.9870 - regression_loss: 1.7621 - classification_loss: 0.2249\n",
            "\n",
            "Epoch 00076: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_76.h5\n",
            "Epoch 77/200\n",
            "88/88 [==============================] - 259s 3s/step - loss: 1.9741 - regression_loss: 1.7532 - classification_loss: 0.2209\n",
            "\n",
            "Epoch 00077: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_77.h5\n",
            "Epoch 78/200\n",
            "88/88 [==============================] - 274s 3s/step - loss: 1.9855 - regression_loss: 1.7638 - classification_loss: 0.2217\n",
            "\n",
            "Epoch 00078: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_78.h5\n",
            "\n",
            "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
            "Epoch 79/200\n",
            "88/88 [==============================] - 281s 3s/step - loss: 1.9652 - regression_loss: 1.7469 - classification_loss: 0.2184\n",
            "\n",
            "Epoch 00079: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_79.h5\n",
            "Epoch 80/200\n",
            "88/88 [==============================] - 269s 3s/step - loss: 1.9826 - regression_loss: 1.7604 - classification_loss: 0.2222\n",
            "\n",
            "Epoch 00080: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_80.h5\n",
            "Epoch 81/200\n",
            "88/88 [==============================] - 266s 3s/step - loss: 1.9485 - regression_loss: 1.7302 - classification_loss: 0.2183\n",
            "\n",
            "Epoch 00081: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_81.h5\n",
            "Epoch 82/200\n",
            "88/88 [==============================] - 268s 3s/step - loss: 1.9898 - regression_loss: 1.7647 - classification_loss: 0.2251\n",
            "\n",
            "Epoch 00082: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_82.h5\n",
            "\n",
            "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
            "Epoch 83/200\n",
            "88/88 [==============================] - 263s 3s/step - loss: 1.9640 - regression_loss: 1.7440 - classification_loss: 0.2200\n",
            "\n",
            "Epoch 00083: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_83.h5\n",
            "Epoch 84/200\n",
            "88/88 [==============================] - 281s 3s/step - loss: 2.0285 - regression_loss: 1.7978 - classification_loss: 0.2307\n",
            "\n",
            "Epoch 00084: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_84.h5\n",
            "Epoch 85/200\n",
            "88/88 [==============================] - 290s 3s/step - loss: 2.0256 - regression_loss: 1.7948 - classification_loss: 0.2308\n",
            "\n",
            "Epoch 00085: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_85.h5\n",
            "Epoch 86/200\n",
            "88/88 [==============================] - 259s 3s/step - loss: 1.9755 - regression_loss: 1.7535 - classification_loss: 0.2220\n",
            "\n",
            "Epoch 00086: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_86.h5\n",
            "\n",
            "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
            "Epoch 87/200\n",
            "88/88 [==============================] - 261s 3s/step - loss: 1.9783 - regression_loss: 1.7581 - classification_loss: 0.2202\n",
            "\n",
            "Epoch 00087: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_87.h5\n",
            "Epoch 88/200\n",
            "88/88 [==============================] - 293s 3s/step - loss: 1.9882 - regression_loss: 1.7590 - classification_loss: 0.2292\n",
            "\n",
            "Epoch 00088: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_88.h5\n",
            "Epoch 89/200\n",
            "88/88 [==============================] - 282s 3s/step - loss: 1.9838 - regression_loss: 1.7618 - classification_loss: 0.2220\n",
            "\n",
            "Epoch 00089: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_89.h5\n",
            "Epoch 90/200\n",
            "88/88 [==============================] - 260s 3s/step - loss: 2.0973 - regression_loss: 1.8484 - classification_loss: 0.2489\n",
            "\n",
            "Epoch 00090: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_90.h5\n",
            "\n",
            "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
            "Epoch 91/200\n",
            "88/88 [==============================] - 279s 3s/step - loss: 1.9830 - regression_loss: 1.7599 - classification_loss: 0.2232\n",
            "\n",
            "Epoch 00091: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_91.h5\n",
            "Epoch 92/200\n",
            "88/88 [==============================] - 287s 3s/step - loss: 2.0263 - regression_loss: 1.7986 - classification_loss: 0.2277\n",
            "\n",
            "Epoch 00092: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_92.h5\n",
            "Epoch 93/200\n",
            "88/88 [==============================] - 265s 3s/step - loss: 2.0318 - regression_loss: 1.8000 - classification_loss: 0.2318\n",
            "\n",
            "Epoch 00093: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_93.h5\n",
            "Epoch 94/200\n",
            "88/88 [==============================] - 267s 3s/step - loss: 1.9799 - regression_loss: 1.7555 - classification_loss: 0.2244\n",
            "\n",
            "Epoch 00094: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_94.h5\n",
            "\n",
            "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
            "Epoch 95/200\n",
            "88/88 [==============================] - 281s 3s/step - loss: 1.9819 - regression_loss: 1.7592 - classification_loss: 0.2227\n",
            "\n",
            "Epoch 00095: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_95.h5\n",
            "Epoch 96/200\n",
            "88/88 [==============================] - 261s 3s/step - loss: 1.9932 - regression_loss: 1.7697 - classification_loss: 0.2236\n",
            "\n",
            "Epoch 00096: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_96.h5\n",
            "Epoch 97/200\n",
            "88/88 [==============================] - 272s 3s/step - loss: 1.9480 - regression_loss: 1.7341 - classification_loss: 0.2140\n",
            "\n",
            "Epoch 00097: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_97.h5\n",
            "Epoch 98/200\n",
            "88/88 [==============================] - 268s 3s/step - loss: 1.9269 - regression_loss: 1.7133 - classification_loss: 0.2135\n",
            "\n",
            "Epoch 00098: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_98.h5\n",
            "Epoch 99/200\n",
            "88/88 [==============================] - 258s 3s/step - loss: 1.9711 - regression_loss: 1.7505 - classification_loss: 0.2206\n",
            "\n",
            "Epoch 00099: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_99.h5\n",
            "Epoch 100/200\n",
            "88/88 [==============================] - 267s 3s/step - loss: 1.9547 - regression_loss: 1.7391 - classification_loss: 0.2157\n",
            "\n",
            "Epoch 00100: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_100.h5\n",
            "Epoch 101/200\n",
            "88/88 [==============================] - 255s 3s/step - loss: 1.9858 - regression_loss: 1.7588 - classification_loss: 0.2270\n",
            "\n",
            "Epoch 00101: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_101.h5\n",
            "Epoch 102/200\n",
            "88/88 [==============================] - 257s 3s/step - loss: 1.9926 - regression_loss: 1.7664 - classification_loss: 0.2262\n",
            "\n",
            "Epoch 00102: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_102.h5\n",
            "\n",
            "Epoch 00102: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
            "Epoch 103/200\n",
            "88/88 [==============================] - 271s 3s/step - loss: 1.9751 - regression_loss: 1.7517 - classification_loss: 0.2233\n",
            "\n",
            "Epoch 00103: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_103.h5\n",
            "Epoch 104/200\n",
            "88/88 [==============================] - 272s 3s/step - loss: 2.0734 - regression_loss: 1.8349 - classification_loss: 0.2385\n",
            "\n",
            "Epoch 00104: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_104.h5\n",
            "Epoch 105/200\n",
            "88/88 [==============================] - 252s 3s/step - loss: 1.9615 - regression_loss: 1.7404 - classification_loss: 0.2211\n",
            "\n",
            "Epoch 00105: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_105.h5\n",
            "Epoch 106/200\n",
            "88/88 [==============================] - 258s 3s/step - loss: 1.9680 - regression_loss: 1.7451 - classification_loss: 0.2229\n",
            "\n",
            "Epoch 00106: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_106.h5\n",
            "\n",
            "Epoch 00106: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
            "Epoch 107/200\n",
            "88/88 [==============================] - 267s 3s/step - loss: 1.9547 - regression_loss: 1.7320 - classification_loss: 0.2226\n",
            "\n",
            "Epoch 00107: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_107.h5\n",
            "Epoch 108/200\n",
            "88/88 [==============================] - 267s 3s/step - loss: 1.9276 - regression_loss: 1.7140 - classification_loss: 0.2136\n",
            "\n",
            "Epoch 00108: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_108.h5\n",
            "Epoch 109/200\n",
            "88/88 [==============================] - 274s 3s/step - loss: 1.9602 - regression_loss: 1.7408 - classification_loss: 0.2195\n",
            "\n",
            "Epoch 00109: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_109.h5\n",
            "Epoch 110/200\n",
            "88/88 [==============================] - 262s 3s/step - loss: 1.9762 - regression_loss: 1.7558 - classification_loss: 0.2203\n",
            "\n",
            "Epoch 00110: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_110.h5\n",
            "\n",
            "Epoch 00110: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
            "Epoch 111/200\n",
            "33/88 [==========>...................] - ETA: 2:45 - loss: 1.9816 - regression_loss: 1.7635 - classification_loss: 0.2181"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b089b81d7728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         callbacks=callbacks, ) \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8F7QmoZGA2i",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing anchor boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i816KvVloj7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras_retinanet.bin.train import create_generators\n",
        "from keras_retinanet.models import backbone\n",
        "from keras_retinanet.utils.config import read_config_file,parse_anchor_parameters\n",
        "from keras_retinanet.bin.debug import run\n",
        "\n",
        "\n",
        "b = backbone('resnet50')\n",
        "\n",
        "class args:\n",
        "    batch_size = 4\n",
        "    config = read_config_file('config.ini')\n",
        "    random_transform = True # Image augmentation\n",
        "    annotations = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/all_hand_train_03_06.csv'\n",
        "    val_annotations = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/all_hand_val_03_06.csv'\n",
        "    classes = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/Hands class names.csv'\n",
        "    image_min_side = 1000\n",
        "    image_max_side = 1400\n",
        "    no_resize = False\n",
        "    dataset_type = 'csv'\n",
        "    tensorboard_dir = ''\n",
        "    evaluation = False\n",
        "    snapshots = True\n",
        "    snapshot_path = \"/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations\"\n",
        "    backbone = 'resnet50'\n",
        "    epochs = 200\n",
        "    steps = len(train_ids)//(batch_size)\n",
        "    weighted_average = True\n",
        "    reduce_lr_factor = 0.1\n",
        "    reduce_lr_patience = 4\n",
        "    compute_val_loss = True\n",
        "    iou_threshold = 0.5\n",
        "    nms_threshold = 0.5\n",
        "    score_threshold = 0.05\n",
        "\n",
        "train_gen, valid_gen = create_generators(args,b.preprocess_image)\n",
        "while run(valid_gen,args,parse_anchor_parameters(args.config)):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Kw3NXwFENZ",
        "colab_type": "text"
      },
      "source": [
        "## Retraining from epoch 16 Freeze previousm model\n",
        "-- mAP -> 0.40 class loss 0.41"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea5X259YFPG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "072bf7e7-76f4-4eb4-e61a-e116a4208731"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks, ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "88/88 [==============================] - 276s 3s/step - loss: 2.7548 - regression_loss: 2.3195 - classification_loss: 0.4353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:29 Time:  0:00:29\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.3656\n",
            "60 instances of class fin_1 with average precision: 0.3669\n",
            "60 instances of class fin_2 with average precision: 0.3945\n",
            "60 instances of class fin_3 with average precision: 0.2406\n",
            "61 instances of class fin_4 with average precision: 0.4197\n",
            "mAP: 0.3577\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_01.h5\n",
            "Epoch 2/150\n",
            "88/88 [==============================] - 223s 3s/step - loss: 2.7775 - regression_loss: 2.3440 - classification_loss: 0.4335\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:12 Time:  0:00:12\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.2231\n",
            "60 instances of class fin_1 with average precision: 0.3517\n",
            "60 instances of class fin_2 with average precision: 0.4389\n",
            "60 instances of class fin_3 with average precision: 0.2556\n",
            "61 instances of class fin_4 with average precision: 0.6506\n",
            "mAP: 0.3848\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_02.h5\n",
            "Epoch 3/150\n",
            "88/88 [==============================] - 229s 3s/step - loss: 2.7782 - regression_loss: 2.3422 - classification_loss: 0.4360\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.6294\n",
            "60 instances of class fin_1 with average precision: 0.4482\n",
            "60 instances of class fin_2 with average precision: 0.3505\n",
            "60 instances of class fin_3 with average precision: 0.2305\n",
            "61 instances of class fin_4 with average precision: 0.5710\n",
            "mAP: 0.4463\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_03.h5\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 4/150\n",
            "88/88 [==============================] - 251s 3s/step - loss: 2.7731 - regression_loss: 2.3483 - classification_loss: 0.4248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.3403\n",
            "60 instances of class fin_1 with average precision: 0.4502\n",
            "60 instances of class fin_2 with average precision: 0.4152\n",
            "60 instances of class fin_3 with average precision: 0.3538\n",
            "61 instances of class fin_4 with average precision: 0.7662\n",
            "mAP: 0.4661\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_04.h5\n",
            "Epoch 5/150\n",
            "88/88 [==============================] - 238s 3s/step - loss: 2.7293 - regression_loss: 2.3103 - classification_loss: 0.4190\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.3114\n",
            "60 instances of class fin_1 with average precision: 0.3850\n",
            "60 instances of class fin_2 with average precision: 0.4101\n",
            "60 instances of class fin_3 with average precision: 0.3947\n",
            "61 instances of class fin_4 with average precision: 0.8027\n",
            "mAP: 0.4619\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_05.h5\n",
            "Epoch 6/150\n",
            "88/88 [==============================] - 254s 3s/step - loss: 2.7435 - regression_loss: 2.3223 - classification_loss: 0.4212\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.4898\n",
            "60 instances of class fin_1 with average precision: 0.4105\n",
            "60 instances of class fin_2 with average precision: 0.3536\n",
            "60 instances of class fin_3 with average precision: 0.2953\n",
            "61 instances of class fin_4 with average precision: 0.8283\n",
            "mAP: 0.4767\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_06.h5\n",
            "Epoch 7/150\n",
            "88/88 [==============================] - 213s 2s/step - loss: 2.7240 - regression_loss: 2.3088 - classification_loss: 0.4152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.4338\n",
            "60 instances of class fin_1 with average precision: 0.4220\n",
            "60 instances of class fin_2 with average precision: 0.3849\n",
            "60 instances of class fin_3 with average precision: 0.3900\n",
            "61 instances of class fin_4 with average precision: 0.8368\n",
            "mAP: 0.4946\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_07.h5\n",
            "Epoch 8/150\n",
            "88/88 [==============================] - 251s 3s/step - loss: 2.7338 - regression_loss: 2.3170 - classification_loss: 0.4168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.4960\n",
            "60 instances of class fin_1 with average precision: 0.4406\n",
            "60 instances of class fin_2 with average precision: 0.3262\n",
            "60 instances of class fin_3 with average precision: 0.3061\n",
            "61 instances of class fin_4 with average precision: 0.8129\n",
            "mAP: 0.4775\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_08.h5\n",
            "Epoch 9/150\n",
            "88/88 [==============================] - 233s 3s/step - loss: 2.7137 - regression_loss: 2.3008 - classification_loss: 0.4129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.4811\n",
            "60 instances of class fin_1 with average precision: 0.4522\n",
            "60 instances of class fin_2 with average precision: 0.3401\n",
            "60 instances of class fin_3 with average precision: 0.3289\n",
            "61 instances of class fin_4 with average precision: 0.8568\n",
            "mAP: 0.4930\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_09.h5\n",
            "Epoch 10/150\n",
            "88/88 [==============================] - 235s 3s/step - loss: 2.7345 - regression_loss: 2.3135 - classification_loss: 0.4210\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.5952\n",
            "60 instances of class fin_1 with average precision: 0.4475\n",
            "60 instances of class fin_2 with average precision: 0.3150\n",
            "60 instances of class fin_3 with average precision: 0.3896\n",
            "61 instances of class fin_4 with average precision: 0.8185\n",
            "mAP: 0.5142\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_10.h5\n",
            "Epoch 11/150\n",
            "88/88 [==============================] - 238s 3s/step - loss: 2.6967 - regression_loss: 2.2868 - classification_loss: 0.4098\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:14 Time:  0:00:14\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.4382\n",
            "60 instances of class fin_1 with average precision: 0.4309\n",
            "60 instances of class fin_2 with average precision: 0.3298\n",
            "60 instances of class fin_3 with average precision: 0.3052\n",
            "61 instances of class fin_4 with average precision: 0.8021\n",
            "mAP: 0.4624\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_11.h5\n",
            "Epoch 12/150\n",
            "88/88 [==============================] - 253s 3s/step - loss: 2.7495 - regression_loss: 2.3239 - classification_loss: 0.4257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:14 Time:  0:00:14\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.5337\n",
            "60 instances of class fin_1 with average precision: 0.4427\n",
            "60 instances of class fin_2 with average precision: 0.3211\n",
            "60 instances of class fin_3 with average precision: 0.3353\n",
            "61 instances of class fin_4 with average precision: 0.8192\n",
            "mAP: 0.4915\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_12.h5\n",
            "Epoch 13/150\n",
            "88/88 [==============================] - 248s 3s/step - loss: 2.7274 - regression_loss: 2.3148 - classification_loss: 0.4126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.5531\n",
            "60 instances of class fin_1 with average precision: 0.4473\n",
            "60 instances of class fin_2 with average precision: 0.3587\n",
            "60 instances of class fin_3 with average precision: 0.3921\n",
            "61 instances of class fin_4 with average precision: 0.8056\n",
            "mAP: 0.5123\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_13.h5\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 14/150\n",
            "88/88 [==============================] - 246s 3s/step - loss: 2.6953 - regression_loss: 2.2876 - classification_loss: 0.4077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.6430\n",
            "60 instances of class fin_1 with average precision: 0.4377\n",
            "60 instances of class fin_2 with average precision: 0.3704\n",
            "60 instances of class fin_3 with average precision: 0.3344\n",
            "61 instances of class fin_4 with average precision: 0.8179\n",
            "mAP: 0.5217\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_14.h5\n",
            "Epoch 15/150\n",
            "88/88 [==============================] - 241s 3s/step - loss: 2.7134 - regression_loss: 2.3005 - classification_loss: 0.4130\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.6200\n",
            "60 instances of class fin_1 with average precision: 0.4274\n",
            "60 instances of class fin_2 with average precision: 0.3576\n",
            "60 instances of class fin_3 with average precision: 0.3356\n",
            "61 instances of class fin_4 with average precision: 0.8076\n",
            "mAP: 0.5106\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_15.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fbf32afc5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUhd2GAuF0qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhQAMNWSF0hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i-GpFr3Fczx",
        "colab_type": "text"
      },
      "source": [
        "## Have reduced the ratios in this run and changed the anchors sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQXpYgkynkEg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4617242b-82f0-4b8a-c096-2d2097592134"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks, ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "88/88 [==============================] - 299s 3s/step - loss: 3.0064 - regression_loss: 2.4081 - classification_loss: 0.5983\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:30 Time:  0:00:30\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0623\n",
            "60 instances of class fin_1 with average precision: 0.2203\n",
            "60 instances of class fin_2 with average precision: 0.3422\n",
            "60 instances of class fin_3 with average precision: 0.1276\n",
            "61 instances of class fin_4 with average precision: 0.1462\n",
            "mAP: 0.1796\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_01.h5\n",
            "Epoch 2/150\n",
            "88/88 [==============================] - 224s 3s/step - loss: 2.9202 - regression_loss: 2.4307 - classification_loss: 0.4895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:12 Time:  0:00:12\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1000\n",
            "60 instances of class fin_1 with average precision: 0.2944\n",
            "60 instances of class fin_2 with average precision: 0.3563\n",
            "60 instances of class fin_3 with average precision: 0.1577\n",
            "61 instances of class fin_4 with average precision: 0.1751\n",
            "mAP: 0.2166\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_02.h5\n",
            "Epoch 3/150\n",
            "88/88 [==============================] - 231s 3s/step - loss: 2.9192 - regression_loss: 2.4162 - classification_loss: 0.5030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1003\n",
            "60 instances of class fin_1 with average precision: 0.2211\n",
            "60 instances of class fin_2 with average precision: 0.5007\n",
            "60 instances of class fin_3 with average precision: 0.1727\n",
            "61 instances of class fin_4 with average precision: 0.1047\n",
            "mAP: 0.2195\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_03.h5\n",
            "Epoch 4/150\n",
            "88/88 [==============================] - 251s 3s/step - loss: 2.9124 - regression_loss: 2.4148 - classification_loss: 0.4976\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:12 Time:  0:00:12\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0912\n",
            "60 instances of class fin_1 with average precision: 0.2869\n",
            "60 instances of class fin_2 with average precision: 0.3149\n",
            "60 instances of class fin_3 with average precision: 0.1605\n",
            "61 instances of class fin_4 with average precision: 0.0696\n",
            "mAP: 0.1842\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_04.h5\n",
            "Epoch 5/150\n",
            "88/88 [==============================] - 239s 3s/step - loss: 2.8912 - regression_loss: 2.4122 - classification_loss: 0.4790\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:14 Time:  0:00:14\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1431\n",
            "60 instances of class fin_1 with average precision: 0.2865\n",
            "60 instances of class fin_2 with average precision: 0.3025\n",
            "60 instances of class fin_3 with average precision: 0.1572\n",
            "61 instances of class fin_4 with average precision: 0.1046\n",
            "mAP: 0.1985\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_05.h5\n",
            "Epoch 6/150\n",
            "88/88 [==============================] - 254s 3s/step - loss: 2.9186 - regression_loss: 2.4161 - classification_loss: 0.5026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0835\n",
            "60 instances of class fin_1 with average precision: 0.3466\n",
            "60 instances of class fin_2 with average precision: 0.3424\n",
            "60 instances of class fin_3 with average precision: 0.1625\n",
            "61 instances of class fin_4 with average precision: 0.1367\n",
            "mAP: 0.2141\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_06.h5\n",
            "Epoch 7/150\n",
            "88/88 [==============================] - 214s 2s/step - loss: 2.8987 - regression_loss: 2.3979 - classification_loss: 0.5008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:12 Time:  0:00:12\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.2176\n",
            "60 instances of class fin_1 with average precision: 0.3489\n",
            "60 instances of class fin_2 with average precision: 0.3604\n",
            "60 instances of class fin_3 with average precision: 0.1605\n",
            "61 instances of class fin_4 with average precision: 0.1574\n",
            "mAP: 0.2486\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_07.h5\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 8/150\n",
            "88/88 [==============================] - 252s 3s/step - loss: 2.8241 - regression_loss: 2.3744 - classification_loss: 0.4497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1844\n",
            "60 instances of class fin_1 with average precision: 0.3788\n",
            "60 instances of class fin_2 with average precision: 0.3945\n",
            "60 instances of class fin_3 with average precision: 0.3208\n",
            "61 instances of class fin_4 with average precision: 0.1757\n",
            "mAP: 0.2905\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_08.h5\n",
            "Epoch 9/150\n",
            "88/88 [==============================] - 234s 3s/step - loss: 2.7888 - regression_loss: 2.3519 - classification_loss: 0.4369\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:14 Time:  0:00:14\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1865\n",
            "60 instances of class fin_1 with average precision: 0.4087\n",
            "60 instances of class fin_2 with average precision: 0.3781\n",
            "60 instances of class fin_3 with average precision: 0.2008\n",
            "61 instances of class fin_4 with average precision: 0.3133\n",
            "mAP: 0.2976\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_09.h5\n",
            "Epoch 10/150\n",
            "88/88 [==============================] - 234s 3s/step - loss: 2.8016 - regression_loss: 2.3541 - classification_loss: 0.4475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:15 Time:  0:00:15\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.2059\n",
            "60 instances of class fin_1 with average precision: 0.3945\n",
            "60 instances of class fin_2 with average precision: 0.4345\n",
            "60 instances of class fin_3 with average precision: 0.2843\n",
            "61 instances of class fin_4 with average precision: 0.2527\n",
            "mAP: 0.3142\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_10.h5\n",
            "Epoch 11/150\n",
            "88/88 [==============================] - 237s 3s/step - loss: 2.7803 - regression_loss: 2.3404 - classification_loss: 0.4399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:15 Time:  0:00:15\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1949\n",
            "60 instances of class fin_1 with average precision: 0.3625\n",
            "60 instances of class fin_2 with average precision: 0.3892\n",
            "60 instances of class fin_3 with average precision: 0.1766\n",
            "61 instances of class fin_4 with average precision: 0.3868\n",
            "mAP: 0.3023\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_11.h5\n",
            "Epoch 12/150\n",
            "88/88 [==============================] - 255s 3s/step - loss: 2.7758 - regression_loss: 2.3386 - classification_loss: 0.4373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:15 Time:  0:00:15\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.2112\n",
            "60 instances of class fin_1 with average precision: 0.3590\n",
            "60 instances of class fin_2 with average precision: 0.3505\n",
            "60 instances of class fin_3 with average precision: 0.1880\n",
            "61 instances of class fin_4 with average precision: 0.5196\n",
            "mAP: 0.3263\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_12.h5\n",
            "Epoch 13/150\n",
            "88/88 [==============================] - 248s 3s/step - loss: 2.7692 - regression_loss: 2.3351 - classification_loss: 0.4341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:15 Time:  0:00:15\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.2093\n",
            "60 instances of class fin_1 with average precision: 0.3462\n",
            "60 instances of class fin_2 with average precision: 0.3865\n",
            "60 instances of class fin_3 with average precision: 0.1602\n",
            "61 instances of class fin_4 with average precision: 0.6504\n",
            "mAP: 0.3515\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_13.h5\n",
            "Epoch 14/150\n",
            "88/88 [==============================] - 246s 3s/step - loss: 2.7612 - regression_loss: 2.3390 - classification_loss: 0.4222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:14 Time:  0:00:14\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.2025\n",
            "60 instances of class fin_1 with average precision: 0.3687\n",
            "60 instances of class fin_2 with average precision: 0.3556\n",
            "60 instances of class fin_3 with average precision: 0.1974\n",
            "61 instances of class fin_4 with average precision: 0.7383\n",
            "mAP: 0.3737\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_14.h5\n",
            "Epoch 15/150\n",
            "88/88 [==============================] - 242s 3s/step - loss: 2.7622 - regression_loss: 2.3305 - classification_loss: 0.4318\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:14 Time:  0:00:14\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1651\n",
            "60 instances of class fin_1 with average precision: 0.3519\n",
            "60 instances of class fin_2 with average precision: 0.3839\n",
            "60 instances of class fin_3 with average precision: 0.2164\n",
            "61 instances of class fin_4 with average precision: 0.6151\n",
            "mAP: 0.3474\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_15.h5\n",
            "Epoch 16/150\n",
            "88/88 [==============================] - 248s 3s/step - loss: 2.7372 - regression_loss: 2.3196 - classification_loss: 0.4177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:14 Time:  0:00:14\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.2127\n",
            "60 instances of class fin_1 with average precision: 0.3680\n",
            "60 instances of class fin_2 with average precision: 0.3330\n",
            "60 instances of class fin_3 with average precision: 0.2545\n",
            "61 instances of class fin_4 with average precision: 0.8429\n",
            "mAP: 0.4037\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_16.h5\n",
            "Epoch 17/150\n",
            "88/88 [==============================] - 247s 3s/step - loss: 2.7610 - regression_loss: 2.3323 - classification_loss: 0.4287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:15 Time:  0:00:15\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1399\n",
            "60 instances of class fin_1 with average precision: 0.2733\n",
            "60 instances of class fin_2 with average precision: 0.4269\n",
            "60 instances of class fin_3 with average precision: 0.2331\n",
            "61 instances of class fin_4 with average precision: 0.6943\n",
            "mAP: 0.3546\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_17.h5\n",
            "Epoch 18/150\n",
            "88/88 [==============================] - 256s 3s/step - loss: 2.7475 - regression_loss: 2.3285 - classification_loss: 0.4190\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:15 Time:  0:00:15\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1809\n",
            "60 instances of class fin_1 with average precision: 0.3401\n",
            "60 instances of class fin_2 with average precision: 0.3614\n",
            "60 instances of class fin_3 with average precision: 0.2204\n",
            "61 instances of class fin_4 with average precision: 0.7675\n",
            "mAP: 0.3754\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_18.h5\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 19/150\n",
            "88/88 [==============================] - 234s 3s/step - loss: 2.7323 - regression_loss: 2.3155 - classification_loss: 0.4167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:14 Time:  0:00:14\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1816\n",
            "60 instances of class fin_1 with average precision: 0.2788\n",
            "60 instances of class fin_2 with average precision: 0.4365\n",
            "60 instances of class fin_3 with average precision: 0.1774\n",
            "61 instances of class fin_4 with average precision: 0.7414\n",
            "mAP: 0.3644\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_19.h5\n",
            "Epoch 20/150\n",
            "88/88 [==============================] - 231s 3s/step - loss: 2.7165 - regression_loss: 2.3052 - classification_loss: 0.4114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1949\n",
            "60 instances of class fin_1 with average precision: 0.3115\n",
            "60 instances of class fin_2 with average precision: 0.4527\n",
            "60 instances of class fin_3 with average precision: 0.1827\n",
            "61 instances of class fin_4 with average precision: 0.7172\n",
            "mAP: 0.3730\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_20.h5\n",
            "Epoch 21/150\n",
            "88/88 [==============================] - 232s 3s/step - loss: 2.7139 - regression_loss: 2.3040 - classification_loss: 0.4098\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:13 Time:  0:00:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.1939\n",
            "60 instances of class fin_1 with average precision: 0.2819\n",
            "60 instances of class fin_2 with average precision: 0.4331\n",
            "60 instances of class fin_3 with average precision: 0.1656\n",
            "61 instances of class fin_4 with average precision: 0.7158\n",
            "mAP: 0.3592\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_21.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f3a86e3aba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBs75XwyrItk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Logs for few changes made\n",
        "- Ratios changed\n",
        "- IOU threshold, NMS threshold = 0.6 and score threshold = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prd_5aWuHlip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c12649a-212a-4cdc-a27b-65671cafc798"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks, ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "88/88 [==============================] - 558s 6s/step - loss: 6.1760 - regression_loss: 2.4610 - classification_loss: 3.7149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:01:13 Time:  0:01:13\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0011\n",
            "60 instances of class fin_1 with average precision: 0.0066\n",
            "60 instances of class fin_2 with average precision: 0.0004\n",
            "60 instances of class fin_3 with average precision: 0.0000\n",
            "61 instances of class fin_4 with average precision: 0.0000\n",
            "mAP: 0.0016\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_01.h5\n",
            "Epoch 2/150\n",
            "88/88 [==============================] - 346s 4s/step - loss: 4.7206 - regression_loss: 2.4290 - classification_loss: 2.2915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:26 Time:  0:00:26\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0000\n",
            "60 instances of class fin_1 with average precision: 0.0000\n",
            "60 instances of class fin_2 with average precision: 0.0000\n",
            "60 instances of class fin_3 with average precision: 0.0023\n",
            "61 instances of class fin_4 with average precision: 0.0059\n",
            "mAP: 0.0017\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_02.h5\n",
            "Epoch 3/150\n",
            "88/88 [==============================] - 326s 4s/step - loss: 3.2642 - regression_loss: 2.4303 - classification_loss: 0.8339\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:24 Time:  0:00:24\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0199\n",
            "60 instances of class fin_1 with average precision: 0.0496\n",
            "60 instances of class fin_2 with average precision: 0.0069\n",
            "60 instances of class fin_3 with average precision: 0.0093\n",
            "61 instances of class fin_4 with average precision: 0.0335\n",
            "mAP: 0.0239\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_03.h5\n",
            "Epoch 4/150\n",
            "88/88 [==============================] - 319s 4s/step - loss: 3.0651 - regression_loss: 2.4138 - classification_loss: 0.6513\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:26 Time:  0:00:26\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0364\n",
            "60 instances of class fin_1 with average precision: 0.0746\n",
            "60 instances of class fin_2 with average precision: 0.0994\n",
            "60 instances of class fin_3 with average precision: 0.0652\n",
            "61 instances of class fin_4 with average precision: 0.0089\n",
            "mAP: 0.0567\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_04.h5\n",
            "Epoch 5/150\n",
            "88/88 [==============================] - 306s 3s/step - loss: 2.9783 - regression_loss: 2.4199 - classification_loss: 0.5584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:26 Time:  0:00:26\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0277\n",
            "60 instances of class fin_1 with average precision: 0.0570\n",
            "60 instances of class fin_2 with average precision: 0.0613\n",
            "60 instances of class fin_3 with average precision: 0.0336\n",
            "61 instances of class fin_4 with average precision: 0.0103\n",
            "mAP: 0.0379\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_05.h5\n",
            "Epoch 6/150\n",
            "88/88 [==============================] - 311s 4s/step - loss: 2.9676 - regression_loss: 2.4086 - classification_loss: 0.5590\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:27 Time:  0:00:27\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0325\n",
            "60 instances of class fin_1 with average precision: 0.0358\n",
            "60 instances of class fin_2 with average precision: 0.0662\n",
            "60 instances of class fin_3 with average precision: 0.0026\n",
            "61 instances of class fin_4 with average precision: 0.0143\n",
            "mAP: 0.0302\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_06.h5\n",
            "Epoch 7/150\n",
            "88/88 [==============================] - 306s 3s/step - loss: 3.0086 - regression_loss: 2.3784 - classification_loss: 0.6302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:25 Time:  0:00:25\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0443\n",
            "60 instances of class fin_1 with average precision: 0.1044\n",
            "60 instances of class fin_2 with average precision: 0.0959\n",
            "60 instances of class fin_3 with average precision: 0.0974\n",
            "61 instances of class fin_4 with average precision: 0.0182\n",
            "mAP: 0.0719\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_07.h5\n",
            "Epoch 8/150\n",
            "88/88 [==============================] - 307s 3s/step - loss: 2.9148 - regression_loss: 2.3812 - classification_loss: 0.5336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:27 Time:  0:00:27\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0680\n",
            "60 instances of class fin_1 with average precision: 0.1749\n",
            "60 instances of class fin_2 with average precision: 0.2470\n",
            "60 instances of class fin_3 with average precision: 0.1105\n",
            "61 instances of class fin_4 with average precision: 0.0856\n",
            "mAP: 0.1371\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_08.h5\n",
            "Epoch 9/150\n",
            "88/88 [==============================] - 305s 3s/step - loss: 2.9044 - regression_loss: 2.3711 - classification_loss: 0.5333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:27 Time:  0:00:27\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0638\n",
            "60 instances of class fin_1 with average precision: 0.0748\n",
            "60 instances of class fin_2 with average precision: 0.0757\n",
            "60 instances of class fin_3 with average precision: 0.0070\n",
            "61 instances of class fin_4 with average precision: 0.0185\n",
            "mAP: 0.0479\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_09.h5\n",
            "Epoch 10/150\n",
            "88/88 [==============================] - 305s 3s/step - loss: 3.0073 - regression_loss: 2.3976 - classification_loss: 0.6097\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:27 Time:  0:00:27\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0366\n",
            "60 instances of class fin_1 with average precision: 0.0546\n",
            "60 instances of class fin_2 with average precision: 0.0523\n",
            "60 instances of class fin_3 with average precision: 0.0214\n",
            "61 instances of class fin_4 with average precision: 0.0438\n",
            "mAP: 0.0417\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_10.h5\n",
            "Epoch 11/150\n",
            "88/88 [==============================] - 304s 3s/step - loss: 133.5236 - regression_loss: 3.1047 - classification_loss: 130.4189\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:25 Time:  0:00:25\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0000\n",
            "60 instances of class fin_1 with average precision: 0.0000\n",
            "60 instances of class fin_2 with average precision: 0.0000\n",
            "60 instances of class fin_3 with average precision: 0.0012\n",
            "61 instances of class fin_4 with average precision: 0.0000\n",
            "mAP: 0.0002\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_11.h5\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 12/150\n",
            "88/88 [==============================] - 317s 4s/step - loss: 2.9970 - regression_loss: 2.4027 - classification_loss: 0.5943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:27 Time:  0:00:27\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0425\n",
            "60 instances of class fin_1 with average precision: 0.0530\n",
            "60 instances of class fin_2 with average precision: 0.0521\n",
            "60 instances of class fin_3 with average precision: 0.0099\n",
            "61 instances of class fin_4 with average precision: 0.0162\n",
            "mAP: 0.0347\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_12.h5\n",
            "Epoch 13/150\n",
            "88/88 [==============================] - 317s 4s/step - loss: 2.8637 - regression_loss: 2.3463 - classification_loss: 0.5174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (61 of 61) |#######| Elapsed Time: 0:00:27 Time:  0:00:27\n",
            "Parsing annotations: 100% (61 of 61) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60 instances of class fin_ip with average precision: 0.0315\n",
            "60 instances of class fin_1 with average precision: 0.0831\n",
            "60 instances of class fin_2 with average precision: 0.0748\n",
            "60 instances of class fin_3 with average precision: 0.0182\n",
            "61 instances of class fin_4 with average precision: 0.0204\n",
            "mAP: 0.0455\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Foot model with new annotations/resnet50_csv_13.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f7bab3a3eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6YRDvhnkYw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptGlRWcSkYha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVK6jDpjkXzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXkp8zRIzbV1",
        "colab_type": "text"
      },
      "source": [
        "# Right foot joint detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4IVA1GjyzZs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "27bc7add-9f93-4f0f-8274-1e20c1ec33ea"
      },
      "source": [
        "\n",
        "# tf.set_random_seed(31) # SEEDS MAKE RESULTS MORE REPRODUCABLE\n",
        "np.random.seed(17)\n",
        "classes = np.arange(0, 11, 1).tolist()\n",
        "\n",
        "# def convert_annotation(image_id,filename):\n",
        "#     in_file = open('training_data/labels/%s.xml'%(image_id))\n",
        "#     out_file = open(filename, 'a')\n",
        "#     tree=ET.parse(in_file)\n",
        "#     root = tree.getroot()\n",
        "    \n",
        "#     if root.iter('object') is not None:\n",
        "#         for obj in root.iter('object'):\n",
        "#             cls = obj.find('name').text\n",
        "#             if cls not in classes:\n",
        "#                 continue\n",
        "#             cls_id = classes.index(cls)\n",
        "            \n",
        "#             xmlbox = obj.find('bndbox')\n",
        "#             x1 = math.ceil(float(xmlbox.find('xmin').text))\n",
        "#             y1 = math.ceil(float(xmlbox.find('ymin').text))\n",
        "#             x2 = math.ceil(float(xmlbox.find('xmax').text))\n",
        "#             y2 = math.ceil(float(xmlbox.find('ymax').text))\n",
        "#             if x1 == x2 or y1 == y2:\n",
        "#                 continue\n",
        "                \n",
        "#             out_file.write(f'training_data/images/{image_id}.jpg,{x1},{y1},{x2},{y2},{cls}\\n')\n",
        "#     else:\n",
        "#         out_file.write(f'training_data/images/{image_id}.jpg,,,,,\\n')\n",
        "\n",
        "\n",
        "# _,_,image_ids = next(walk('training_data/images'))\n",
        "# image_ids = [i[:-4] for i in image_ids]\n",
        "# open('annotations.csv','w')\n",
        "# open('val_annotations.csv','w')\n",
        "\n",
        "# train_ids,val_ids = train_test_split(image_ids,random_state=31,test_size=0)\n",
        "\n",
        "# for image_id in train_ids:\n",
        "#     convert_annotation(image_id,'annotations.csv')\n",
        "        \n",
        "# for image_id in val_ids:\n",
        "#     convert_annotation(image_id,'val_annotations.csv')\n",
        "    \n",
        "# print(len(train_ids),len(val_ids))\n",
        "\n",
        "import pandas as pd\n",
        "tr_annots = pd.read_csv('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/Right foot train.csv', header = None)\n",
        "tr_annots['ids'] = tr_annots[0].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
        "train_ids = tr_annots['ids'].unique().tolist()\n",
        "\n",
        "import pandas as pd\n",
        "tr_annots = pd.read_csv('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/Right foot val.csv', header = None)\n",
        "tr_annots['ids'] = tr_annots[0].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
        "val_ids = tr_annots['ids'].unique().tolist()\n",
        "\n",
        "print(len(train_ids), len(val_ids))\n",
        "\n",
        "with open('config.ini','w') as f:\n",
        "    f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 1 1.2 1.5 2 \\nscales  = 2 3 4 \\n')\n",
        "\t\n",
        "b = backbone('resnet50')\n",
        "\n",
        "class args:\n",
        "    batch_size = 2\n",
        "    config = read_config_file('config.ini')\n",
        "    random_transform = True # Image augmentation\n",
        "    annotations = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/Right foot train.csv'\n",
        "    val_annotations = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/Right foot val.csv'\n",
        "    classes = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/foot_class_names.csv'\n",
        "    image_min_side = 900\n",
        "    image_max_side = 1200\n",
        "    no_resize = False\n",
        "    dataset_type = 'csv'\n",
        "    tensorboard_dir = ''\n",
        "    evaluation = False\n",
        "    snapshots = True\n",
        "    snapshot_path = \"/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/save right foot model/\"\n",
        "    backbone = 'resnet50'\n",
        "    epochs = 100\n",
        "    steps = len(train_ids)//(batch_size)\n",
        "    weighted_average = True\n",
        "\t\n",
        "train_gen, valid_gen = create_generators(args, b.preprocess_image)\n",
        "\n",
        "\n",
        "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "# Define our sequence of augmentation steps that will be applied to every image.\n",
        "seq = iaa.Sequential(\n",
        "    [\n",
        "        #\n",
        "        # Execute 1 to 9 of the following (less important) augmenters per\n",
        "        # image. Don't execute all of them, as that would often be way too\n",
        "        # strong.\n",
        "        #\n",
        "        iaa.SomeOf((1, 9),\n",
        "            [\n",
        "\n",
        "                        # Blur each image with varying strength using\n",
        "                        # gaussian blur (sigma between 0 and .5),\n",
        "                        # average/uniform blur (kernel size 1x1)\n",
        "                        # median blur (kernel size 1x1).\n",
        "                        iaa.OneOf([\n",
        "                            iaa.GaussianBlur((0,0.5)),\n",
        "                            iaa.AverageBlur(k=(1)),\n",
        "                            iaa.MedianBlur(k=(1)),\n",
        "                        ]),\n",
        "\n",
        "                        # Sharpen each image, overlay the result with the original\n",
        "                        # image using an alpha between 0 (no sharpening) and 1\n",
        "                        # (full sharpening effect).\n",
        "                        iaa.Sharpen(alpha=(0, 0.25), lightness=(0.75, 1.5)),\n",
        "\n",
        "                        # Add gaussian noise to some images.\n",
        "                        # In 50% of these cases, the noise is randomly sampled per\n",
        "                        # channel and pixel.\n",
        "                        # In the other 50% of all cases it is sampled once per\n",
        "                        # pixel (i.e. brightness change).\n",
        "                        iaa.AdditiveGaussianNoise(\n",
        "                            loc=0, scale=(0.0, 0.01*255), per_channel=0.5\n",
        "                        ),\n",
        "\n",
        "                        # Either drop randomly 1 to 10% of all pixels (i.e. set\n",
        "                        # them to black) or drop them on an image with 2-5% percent\n",
        "                        # of the original size, leading to large dropped\n",
        "                        # rectangles.\n",
        "                        iaa.OneOf([\n",
        "                            iaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
        "                            iaa.CoarseDropout(\n",
        "                                (0.03, 0.15), size_percent=(0.02, 0.05),\n",
        "                                per_channel=0.2\n",
        "                            ),\n",
        "                        ]),\n",
        "\n",
        "                        # Add a value of -5 to 5 to each pixel.\n",
        "                        iaa.Add((-5, 5), per_channel=0.5),\n",
        "\n",
        "                        # Change brightness of images (85-115% of original value).\n",
        "                        iaa.Multiply((0.85, 1.15), per_channel=0.5),\n",
        "\n",
        "                        # Improve or worsen the contrast of images.\n",
        "                        iaa.ContrastNormalization((0.75, 1.25), per_channel=0.5),\n",
        "\n",
        "                        # Convert each image to grayscale and then overlay the\n",
        "                        # result with the original with random alpha. I.e. remove\n",
        "                        # colors with varying strengths.\n",
        "                        iaa.Grayscale(alpha=(0.0, 0.25)),\n",
        "\n",
        "                        # In some images distort local areas with varying strength.\n",
        "                        sometimes(iaa.PiecewiseAffine(scale=(0.001, 0.01)))\n",
        "                    ],\n",
        "            # do all of the above augmentations in random order\n",
        "            random_order=True\n",
        "        )\n",
        "    ],\n",
        "    # do all of the above augmentations in random order\n",
        "    random_order=True\n",
        ")\n",
        "\n",
        "\n",
        "def augment_train_gen(train_gen, visualize=False):\n",
        "    '''\n",
        "    Creates a generator using another generator with applied image augmentation.\n",
        "    Args\n",
        "        train_gen  : keras-retinanet generator object.\n",
        "        visualize  : Boolean; False will convert bounding boxes to their anchor box targets for the model.\n",
        "    '''\n",
        "    imgs = []\n",
        "    boxes = []\n",
        "    targets = []\n",
        "    size = train_gen.size()\n",
        "    idx = 0\n",
        "    while True:\n",
        "        while len(imgs) < args.batch_size:\n",
        "            image       = train_gen.load_image(idx % size)\n",
        "            annotations = train_gen.load_annotations(idx % size)\n",
        "            image,annotations = train_gen.random_transform_group_entry(image,annotations)\n",
        "            imgs.append(image)            \n",
        "            boxes.append(annotations['bboxes'])\n",
        "            targets.append(annotations)\n",
        "            idx += 1\n",
        "        if visualize:\n",
        "            imgs = seq.augment_images(imgs)\n",
        "            imgs = np.array(imgs)\n",
        "            boxes = np.array(boxes)\n",
        "            yield imgs,boxes\n",
        "        else:\n",
        "            imgs = seq.augment_images(imgs)\n",
        "            imgs,targets = train_gen.preprocess_group(imgs,targets)\n",
        "            imgs = train_gen.compute_inputs(imgs)\n",
        "            targets = train_gen.compute_targets(imgs,targets)\n",
        "            imgs = np.array(imgs)\n",
        "            yield imgs,targets\n",
        "        imgs = []\n",
        "        boxes = []\n",
        "        targets = []\n",
        "\t\t\n",
        "\t\t\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# skip_batches = 5\n",
        "# i = 0\n",
        "# for imgs,boxes in augment_train_gen(train_gen,visualize=True):\n",
        "#     if i > skip_batches:\n",
        "#         fig=plt.figure(figsize=(24,96))\n",
        "#         columns = 2\n",
        "#         rows = 8\n",
        "#         for i in range(1, columns*rows + 1):\n",
        "#             draw_boxes(imgs[i], boxes[i], (0, 255, 0), thickness=1)\n",
        "#             fig.add_subplot(rows, columns, i)\n",
        "#             plt.imshow(cv2.cvtColor(imgs[i],cv2.COLOR_BGR2RGB))\n",
        "#         plt.show()\n",
        "#         break\n",
        "#     else:\n",
        "#         i += 1\n",
        "\n",
        "model, training_model, prediction_model = create_models(\n",
        "            backbone_retinanet=b.retinanet,\n",
        "            num_classes=train_gen.num_classes(),\n",
        "            weights=None,\n",
        "            multi_gpu=False,\n",
        "            freeze_backbone=True,\n",
        "            lr=1e-3,\n",
        "            config=args.config\n",
        "        )\n",
        "\n",
        "\t\t\n",
        "callbacks = create_callbacks(\n",
        "    model,\n",
        "    training_model,\n",
        "    prediction_model,\n",
        "    valid_gen,\n",
        "    args,\n",
        ")\n",
        "training_model.load_weights('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/resnet50_csv_27.h5', skip_mismatch = True, by_name = True)\n",
        "\n",
        "##m Final model for Right hand joints detections\n",
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks, ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/34 [===========>..................] - ETA: 32s - loss: 1.1881 - regression_loss: 1.0194 - classification_loss: 0.1686"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-59962edcb8fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         callbacks=callbacks, ) \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQyVmQXLyzG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWhDsP2Pyydk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df2KYOJUyyS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEjfgQQ0yyIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bPuxen7yx2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}