{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Retinanet-Training hand model 06_06_2020 final version R1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuCOm-NuF88-",
        "colab_type": "text"
      },
      "source": [
        "# Installing retinanet library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Nj5byPVPIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "37b6cd61-4b90-4de7-fdd5-013119aa405d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeV8um3iGFwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a = []\n",
        "# while(1):\n",
        "#     a.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kkE0rKhUjA8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08acca2f-f970-4367-a562-29b098206169"
      },
      "source": [
        "!git clone --recursive https://github.com/fizyr/keras-retinanet.git\n",
        "!ls\n",
        "!cd keras-retinanet && pip install . --user\n",
        "!pip install Cython\n",
        "!pip install --user git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
        "!cd keras-retinanet/snapshots && \\\n",
        "# wget https://github.com/fizyr/keras-retinanet/releases/download/0.2/resnet50_coco_best_v2.0.3.h5\n",
        "# wget https://github.com/ZFTurbo/Keras-RetinaNet-for-Open-Images-Challenge-2018/releases/download/v1.3/retinanet_resnet101_500_classes_0.4986.h5\n",
        "!pip install --upgrade git+https://github.com/fizyr/keras-retinanet\n",
        "!pip install --upgrade git+https://github.com/broadinstitute/keras-resnet\n",
        "import keras\n",
        "import keras_resnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-retinanet'...\n",
            "remote: Enumerating objects: 1, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 5758 (delta 0), reused 0 (delta 0), pack-reused 5757\u001b[K\n",
            "Receiving objects: 100% (5758/5758), 13.37 MiB | 7.34 MiB/s, done.\n",
            "Resolving deltas: 100% (3865/3865), done.\n",
            "Submodule 'tests/test-data' (https://github.com/fizyr/keras-retinanet-test-data.git) registered for path 'tests/test-data'\n",
            "Cloning into '/content/keras-retinanet/tests/test-data'...\n",
            "remote: Enumerating objects: 45, done.        \n",
            "remote: Total 45 (delta 0), reused 0 (delta 0), pack-reused 45        \n",
            "Submodule path 'tests/test-data': checked out '98404379fbf1ff1273d01db835c10cc83a4f8007'\n",
            "drive  keras-retinanet\tsample_data\n",
            "Processing /content/keras-retinanet\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (2.3.1)\n",
            "Collecting keras-resnet==0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/05/46/ad0b2d1a05d9497bd80c98a2c3f4d8be38a4601ace69af72814f5fafd851/keras-resnet-0.1.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (1.4.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (0.29.19)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (7.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (4.1.2.30)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (3.38.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.18.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.1.2)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->keras-retinanet==0.5.1) (2.4.0)\n",
            "Building wheels for collected packages: keras-retinanet, keras-resnet\n",
            "  Building wheel for keras-retinanet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-retinanet: filename=keras_retinanet-0.5.1-cp36-cp36m-linux_x86_64.whl size=169705 sha256=e7236ca928dca31c668f8bf78d17f90501423975fadb6e1af98e0a7637787c86\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/9f/57/cb0305f6f5a41fc3c11ad67b8cedfbe9127775b563337827ba\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-resnet: filename=keras_resnet-0.1.0-py2.py3-none-any.whl size=13346 sha256=4e1339409af630a061ac3feda69cb3c820941cdc9acf82b3e174fd0f3c201772\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/dd/ac/842235b63dddac12faa4b48ebe58b8944e8c2e57c2e38dddb6\n",
            "Successfully built keras-retinanet keras-resnet\n",
            "Installing collected packages: keras-resnet, keras-retinanet\n",
            "\u001b[33m  WARNING: The scripts retinanet-convert-model, retinanet-debug, retinanet-evaluate and retinanet-train are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed keras-resnet-0.1.0 keras-retinanet-0.5.1\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.19)\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-lslpkaxh\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-lslpkaxh\n",
            "Requirement already satisfied (use --upgrade to upgrade): pycocotools==2.0 from git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (47.1.1)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.19)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.18.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.12.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=267009 sha256=3579ac8d72dde00fa00f034d5d5938ce85a7e17d46655789758d875922d414bb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ho3umgk5/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n",
            "Collecting git+https://github.com/fizyr/keras-retinanet\n",
            "  Cloning https://github.com/fizyr/keras-retinanet to /tmp/pip-req-build-00zkspc7\n",
            "  Running command git clone -q https://github.com/fizyr/keras-retinanet /tmp/pip-req-build-00zkspc7\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied, skipping upgrade: keras in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-resnet==0.1.0 in /root/.local/lib/python3.6/site-packages (from keras-retinanet==0.5.1) (0.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cython in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (0.29.19)\n",
            "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (3.38.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->keras-retinanet==0.5.1) (2.4.0)\n",
            "Building wheels for collected packages: keras-retinanet\n",
            "  Building wheel for keras-retinanet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-retinanet: filename=keras_retinanet-0.5.1-cp36-cp36m-linux_x86_64.whl size=169714 sha256=51e76ffc06efa22ec5def8e5f1e7c49e62c81c1725b8211ca01836d112fae285\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rlwtz1cq/wheels/3e/f1/75/4b42a59887b48ec1022cc76889b1e48da866f1482fd7a0f3df\n",
            "Successfully built keras-retinanet\n",
            "Installing collected packages: keras-retinanet\n",
            "  Found existing installation: keras-retinanet 0.5.1\n",
            "    Uninstalling keras-retinanet-0.5.1:\n",
            "      Successfully uninstalled keras-retinanet-0.5.1\n",
            "Successfully installed keras-retinanet-0.5.1\n",
            "Collecting git+https://github.com/broadinstitute/keras-resnet\n",
            "  Cloning https://github.com/broadinstitute/keras-resnet to /tmp/pip-req-build-6yco5vd9\n",
            "  Running command git clone -q https://github.com/broadinstitute/keras-resnet /tmp/pip-req-build-6yco5vd9\n",
            "Requirement already satisfied, skipping upgrade: keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from keras-resnet==0.2.0) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.2.4->keras-resnet==0.2.0) (2.10.0)\n",
            "Building wheels for collected packages: keras-resnet\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-resnet: filename=keras_resnet-0.2.0-py2.py3-none-any.whl size=22144 sha256=092a5b39f4b003b50739f0faddfbef794ea5bbe68019c587d3710f4055f50d20\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ggg6brvp/wheels/10/52/f3/6a1fdbfb022ce9abfdf00a1ca7e90cef71dea99976edbcb53f\n",
            "Successfully built keras-resnet\n",
            "\u001b[31mERROR: keras-retinanet 0.5.1 has requirement keras-resnet==0.1.0, but you'll have keras-resnet 0.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-resnet\n",
            "  Found existing installation: keras-resnet 0.1.0\n",
            "    Uninstalling keras-resnet-0.1.0:\n",
            "      Successfully uninstalled keras-resnet-0.1.0\n",
            "Successfully installed keras-resnet-0.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnBrHba0FOFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from os import listdir, walk\n",
        "from os.path import join\n",
        "from keras_retinanet.bin.train import create_models\n",
        "from keras_retinanet.models import backbone,load_model, convert_model\n",
        "from keras_retinanet.utils.config import read_config_file, parse_anchor_parameters\n",
        "from keras_retinanet.utils.visualization import draw_boxes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imgaug import augmenters as iaa\n",
        "import keras\n",
        "import keras_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjl4eT8PHmBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dfb3d731-c0fb-4fbc-f734-6c6c7a80234e"
      },
      "source": [
        "\n",
        "# tf.set_random_seed(31) # SEEDS MAKE RESULTS MORE REPRODUCABLE\n",
        "np.random.seed(17)\n",
        "classes = np.arange(0, 6, 1).tolist()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "tr_annots = pd.read_csv('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/all_hand_train_03_06.csv', header = None)\n",
        "tr_annots['ids'] = tr_annots[0].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
        "train_ids = tr_annots['ids'].unique().tolist()\n",
        "\n",
        "import pandas as pd\n",
        "tr_annots = pd.read_csv('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/all_hand_val_03_06.csv', header = None)\n",
        "tr_annots['ids'] = tr_annots[0].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
        "val_ids = tr_annots['ids'].unique().tolist()\n",
        "\n",
        "print(len(train_ids), len(val_ids))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "341 57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIATkoaWwEuA",
        "colab_type": "text"
      },
      "source": [
        "# Previous config files for hand models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLj_7ZuZwEWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Version 3 had this config file\n",
        "# with open('config.ini','w') as f:\n",
        "#     f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 1 1.5 2 2.5 \\nscales  = 1 2\\n')\n",
        "\n",
        "# #Changing the config file to increase the area of the bounding boxes and including a ratio 3 in the config file\n",
        "# This gives mAP 0.9446 class loss 0.3669\n",
        "# with open('config.ini','w') as f:\n",
        "#     f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512 1024\\nstrides = 8 16 32 64 128 256\\nratios  = 1 1.2 1.5 1.8 2 2.5 2.8 3\\nscales  =1 1.2 1.6\\n')\n",
        "\n",
        "## Changing the config file for the retraining\n",
        "# #Changing the config file to increase the area of the bounding boxes and including a ratio 3 in the config file\n",
        "# with open('config.ini','w') as f:\n",
        "#     f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 1 1.5 2 2.2\\nscales  =1 2\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFE_cLrPwKDp",
        "colab_type": "text"
      },
      "source": [
        "## Final config file for Hand models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8bJTmr1wD1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Config file for models generated on 06 june\n",
        "# with open('config.ini','w') as f:\n",
        "#     f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 0.8 1 1.3 1.5 2 2.2 2.8 3\\nscales  =1 1.2 1.6\\n')\n",
        "\n",
        "# # New config file after judging from the anchors that are generated\n",
        "# # Testing the config file ratios \n",
        "# with open('config.ini','w') as f:\n",
        "#     f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 0.5 1 1.5 2 2.5 3\\nscales  =1 1.2 1.6\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4NsyZEWARqO",
        "colab_type": "text"
      },
      "source": [
        "# Changing the ratios since the wrist joint is not being detected properly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llQIMxiFAQ-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('config.ini','w') as f:\n",
        "    f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512 1024 2048\\nstrides = 8 16 32 64 128 256 512\\nratios  = 0.5 1 1.5 2 2.5\\nscales  =1 1.5 2\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD6Z4rVwH01Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "b = backbone('resnet50')\n",
        "\n",
        "class args:\n",
        "    batch_size = 8\n",
        "    config = read_config_file('config.ini')\n",
        "    random_transform = True # Image augmentation\n",
        "    annotations = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/all_hand_train_03_06.csv'\n",
        "    val_annotations = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/all_hand_val_03_06.csv'\n",
        "    classes = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/retinanet train val files new/Hands class names.csv'\n",
        "    image_min_side = 1000\n",
        "    image_max_side = 1400\n",
        "    no_resize = False\n",
        "    dataset_type = 'csv'\n",
        "    tensorboard_dir = ''\n",
        "    evaluation = False\n",
        "    snapshots = True\n",
        "    snapshot_path = \"/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v6\"\n",
        "    backbone = 'resnet50'\n",
        "    epochs = 200\n",
        "    steps = len(train_ids)//(batch_size)\n",
        "    weighted_average = True\n",
        "    reduce_lr_factor = 0.1\n",
        "    reduce_lr_patience = 4\n",
        "    compute_val_loss = True\n",
        "    iou_threshold = 0.6\n",
        "    nms_threshold = 0.5\n",
        "    score_threshold = 0.15\n",
        "    anchors = True\n",
        "    resize = True\n",
        "    display_name = 'Anchors'\n",
        "    no_gui = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm2Rs_RYI5Gl",
        "colab_type": "text"
      },
      "source": [
        "## Create generators function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnZhcoKDI4QI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras_retinanet.preprocessing.generator import Generator\n",
        "# from ..utils.image import read_image_bgr\n",
        "from keras_retinanet.utils.transform import random_transform_generator\n",
        "from keras_retinanet.utils.image import random_visual_effect_generator\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from six import raise_from\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "import os.path\n",
        "from collections import OrderedDict\n",
        "\n",
        "import PIL.Image\n",
        "import PIL.ImageOps\n",
        "import numpy as np\n",
        "from os import makedirs \n",
        "\n",
        "\n",
        "def exif_transpose(img):\n",
        "    if not img:\n",
        "        return img\n",
        "\n",
        "    exif_orientation_tag = 274\n",
        "\n",
        "    # Check for EXIF data (only present on some files)\n",
        "    if hasattr(img, \"_getexif\") and isinstance(img._getexif(), dict) and exif_orientation_tag in img._getexif():\n",
        "        exif_data = img._getexif()\n",
        "        orientation = exif_data[exif_orientation_tag]\n",
        "\n",
        "        # Handle EXIF Orientation\n",
        "        if orientation == 1:\n",
        "            # Normal image - nothing to do!\n",
        "            pass\n",
        "        elif orientation == 2:\n",
        "            # Mirrored left to right\n",
        "            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "        elif orientation == 3:\n",
        "            # Rotated 180 degrees\n",
        "            img = img.rotate(180)\n",
        "        elif orientation == 4:\n",
        "            # Mirrored top to bottom\n",
        "            img = img.rotate(180).transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "        elif orientation == 5:\n",
        "            # Mirrored along top-left diagonal\n",
        "            img = img.rotate(-90, expand=True).transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "        elif orientation == 6:\n",
        "            # Rotated 90 degrees\n",
        "            img = img.rotate(-90, expand=True)\n",
        "        elif orientation == 7:\n",
        "            # Mirrored along top-right diagonal\n",
        "            img = img.rotate(90, expand=True).transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "        elif orientation == 8:\n",
        "            # Rotated 270 degrees\n",
        "            img = img.rotate(90, expand=True)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_image_file(file, mode='RGB'):\n",
        "    # Load the image with PIL\n",
        "    img = PIL.Image.open(file)\n",
        "\n",
        "    if hasattr(PIL.ImageOps, 'exif_transpose'):\n",
        "        # Very recent versions of PIL can do exit transpose internally\n",
        "        img = PIL.ImageOps.exif_transpose(img)\n",
        "    else:\n",
        "        # Otherwise, do the exif transpose ourselves\n",
        "        img = exif_transpose(img)\n",
        "\n",
        "    img = img.convert(mode)\n",
        "\n",
        "    return np.array(img)\n",
        "\n",
        "def _parse(value, function, fmt):\n",
        "    \"\"\"\n",
        "    Parse a string into a value, and format a nice ValueError if it fails.\n",
        "\n",
        "    Returns `function(value)`.\n",
        "    Any `ValueError` raised is catched and a new `ValueError` is raised\n",
        "    with message `fmt.format(e)`, where `e` is the caught `ValueError`.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return function(value)\n",
        "    except ValueError as e:\n",
        "        raise_from(ValueError(fmt.format(e)), None)\n",
        "\n",
        "\n",
        "def _read_classes(csv_reader):\n",
        "    \"\"\" Parse the classes file given by csv_reader.\n",
        "    \"\"\"\n",
        "    result = OrderedDict()\n",
        "    for line, row in enumerate(csv_reader):\n",
        "        line += 1\n",
        "\n",
        "        try:\n",
        "            class_name, class_id = row\n",
        "        except ValueError:\n",
        "            raise_from(ValueError('line {}: format should be \\'class_name,class_id\\''.format(line)), None)\n",
        "        class_id = _parse(class_id, int, 'line {}: malformed class ID: {{}}'.format(line))\n",
        "\n",
        "        if class_name in result:\n",
        "            raise ValueError('line {}: duplicate class name: \\'{}\\''.format(line, class_name))\n",
        "        result[class_name] = class_id\n",
        "    return result\n",
        "\n",
        "\n",
        "def _read_annotations(csv_reader, classes):\n",
        "    \"\"\" Read annotations from the csv_reader.\n",
        "    \"\"\"\n",
        "    result = OrderedDict()\n",
        "    for line, row in enumerate(csv_reader):\n",
        "        line += 1\n",
        "\n",
        "        try:\n",
        "            img_file, x1, y1, x2, y2, class_name = row[:6]\n",
        "        except ValueError:\n",
        "            raise_from(ValueError('line {}: format should be \\'img_file,x1,y1,x2,y2,class_name\\' or \\'img_file,,,,,\\''.format(line)), None)\n",
        "\n",
        "        if img_file not in result:\n",
        "            result[img_file] = []\n",
        "\n",
        "        # If a row contains only an image path, it's an image without annotations.\n",
        "        if (x1, y1, x2, y2, class_name) == ('', '', '', '', ''):\n",
        "            continue\n",
        "\n",
        "        x1 = _parse(x1, int, 'line {}: malformed x1: {{}}'.format(line))\n",
        "        y1 = _parse(y1, int, 'line {}: malformed y1: {{}}'.format(line))\n",
        "        x2 = _parse(x2, int, 'line {}: malformed x2: {{}}'.format(line))\n",
        "        y2 = _parse(y2, int, 'line {}: malformed y2: {{}}'.format(line))\n",
        "\n",
        "        # Check that the bounding box is valid.\n",
        "        if x2 <= x1:\n",
        "            raise ValueError('line {}: x2 ({}) must be higher than x1 ({})'.format(line, x2, x1))\n",
        "        if y2 <= y1:\n",
        "            raise ValueError('line {}: y2 ({}) must be higher than y1 ({})'.format(line, y2, y1))\n",
        "\n",
        "        # check if the current class name is correctly present\n",
        "        if class_name not in classes:\n",
        "            raise ValueError('line {}: unknown class name: \\'{}\\' (classes: {})'.format(line, class_name, classes))\n",
        "\n",
        "        result[img_file].append({'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': class_name})\n",
        "    return result\n",
        "\n",
        "\n",
        "def _open_for_csv(path):\n",
        "    \"\"\" Open a file with flags suitable for csv.reader.\n",
        "\n",
        "    This is different for python2 it means with mode 'rb',\n",
        "    for python3 this means 'r' with \"universal newlines\".\n",
        "    \"\"\"\n",
        "    if sys.version_info[0] < 3:\n",
        "        return open(path, 'rb')\n",
        "    else:\n",
        "        return open(path, 'r', newline='')\n",
        "\n",
        "\n",
        "class CSVGenerator(Generator):\n",
        "    \"\"\" Generate data for a custom CSV dataset.\n",
        "\n",
        "    See https://github.com/fizyr/keras-retinanet#csv-datasets for more information.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_data_file,\n",
        "        csv_class_file,\n",
        "        base_dir=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\" Initialize a CSV data generator.\n",
        "\n",
        "        Args\n",
        "            csv_data_file: Path to the CSV annotations file.\n",
        "            csv_class_file: Path to the CSV classes file.\n",
        "            base_dir: Directory w.r.t. where the files are to be searched (defaults to the directory containing the csv_data_file).\n",
        "        \"\"\"\n",
        "        self.image_names = []\n",
        "        self.image_data  = {}\n",
        "        self.base_dir    = base_dir\n",
        "\n",
        "        # Take base_dir from annotations file if not explicitly specified.\n",
        "        if self.base_dir is None:\n",
        "            self.base_dir = os.path.dirname(csv_data_file)\n",
        "\n",
        "        # parse the provided class file\n",
        "        try:\n",
        "            with _open_for_csv(csv_class_file) as file:\n",
        "                self.classes = _read_classes(csv.reader(file, delimiter=','))\n",
        "        except ValueError as e:\n",
        "            raise_from(ValueError('invalid CSV class file: {}: {}'.format(csv_class_file, e)), None)\n",
        "\n",
        "        self.labels = {}\n",
        "        for key, value in self.classes.items():\n",
        "            self.labels[value] = key\n",
        "\n",
        "        # csv with img_path, x1, y1, x2, y2, class_name\n",
        "        try:\n",
        "            with _open_for_csv(csv_data_file) as file:\n",
        "                self.image_data = _read_annotations(csv.reader(file, delimiter=','), self.classes)\n",
        "        except ValueError as e:\n",
        "            raise_from(ValueError('invalid CSV annotations file: {}: {}'.format(csv_data_file, e)), None)\n",
        "        self.image_names = list(self.image_data.keys())\n",
        "\n",
        "        super(CSVGenerator, self).__init__(**kwargs)\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\" Size of the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def num_classes(self):\n",
        "        \"\"\" Number of classes in the dataset.\n",
        "        \"\"\"\n",
        "        return max(self.classes.values()) + 1\n",
        "\n",
        "    def has_label(self, label):\n",
        "        \"\"\" Return True if label is a known label.\n",
        "        \"\"\"\n",
        "        return label in self.labels\n",
        "\n",
        "    def has_name(self, name):\n",
        "        \"\"\" Returns True if name is a known class.\n",
        "        \"\"\"\n",
        "        return name in self.classes\n",
        "\n",
        "    def name_to_label(self, name):\n",
        "        \"\"\" Map name to label.\n",
        "        \"\"\"\n",
        "        return self.classes[name]\n",
        "\n",
        "    def label_to_name(self, label):\n",
        "        \"\"\" Map label to name.\n",
        "        \"\"\"\n",
        "        return self.labels[label]\n",
        "\n",
        "    def image_path(self, image_index):\n",
        "        \"\"\" Returns the image path for image_index.\n",
        "        \"\"\"\n",
        "        return os.path.join(self.base_dir, self.image_names[image_index])\n",
        "\n",
        "    def image_aspect_ratio(self, image_index):\n",
        "        \"\"\" Compute the aspect ratio for an image with image_index.\n",
        "        \"\"\"\n",
        "        # PIL is fast for metadata\n",
        "        image = Image.open(self.image_path(image_index))\n",
        "        return float(image.width) / float(image.height)\n",
        "\n",
        "    def load_image(self, image_index):\n",
        "        \"\"\" Load an image at the image_index.\n",
        "        \"\"\"\n",
        "        return load_image_file(self.image_path(image_index))\n",
        "\n",
        "    def load_annotations(self, image_index):\n",
        "        \"\"\" Load annotations for an image_index.\n",
        "        \"\"\"\n",
        "        path        = self.image_names[image_index]\n",
        "        annotations = {'labels': np.empty((0,)), 'bboxes': np.empty((0, 4))}\n",
        "\n",
        "        for idx, annot in enumerate(self.image_data[path]):\n",
        "            annotations['labels'] = np.concatenate((annotations['labels'], [self.name_to_label(annot['class'])]))\n",
        "            annotations['bboxes'] = np.concatenate((annotations['bboxes'], [[\n",
        "                float(annot['x1']),\n",
        "                float(annot['y1']),\n",
        "                float(annot['x2']),\n",
        "                float(annot['y2']),\n",
        "            ]]))\n",
        "\n",
        "        return annotations\n",
        "\n",
        "def mAP(y_true, y_pred):\n",
        "    num_classes = y_true.shape[1]\n",
        "    average_precisions = []\n",
        "    relevant = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    tp_whole = K.round(K.clip(y_true * y_pred, 0, 1))\n",
        "    for index in range(num_classes):\n",
        "      temp = K.sum(tp_whole[:,:index+1],axis=1)\n",
        "      average_precisions.append(temp * (1/(index + 1)))\n",
        "    AP = Add()(average_precisions) / relevant\n",
        "    mAP = K.mean(AP,axis=0)\n",
        "    return mAP\n",
        "\n",
        "def create_callbacks(model, training_model, prediction_model, validation_generator, args):\n",
        "    \"\"\" Creates the callbacks to use during training.\n",
        "\n",
        "    Args\n",
        "        model: The base model.\n",
        "        training_model: The model that is used for training.\n",
        "        prediction_model: The model that should be used for validation.\n",
        "        validation_generator: The generator for creating validation data.\n",
        "        args: parseargs args object.\n",
        "\n",
        "    Returns:\n",
        "        A list of callbacks used for training.\n",
        "    \"\"\"\n",
        "    callbacks = []\n",
        "\n",
        "    tensorboard_callback = None\n",
        "\n",
        "    if args.tensorboard_dir:\n",
        "        makedirs(args.tensorboard_dir)\n",
        "        tensorboard_callback = keras.callbacks.TensorBoard(\n",
        "            log_dir                = args.tensorboard_dir,\n",
        "            histogram_freq         = 0,\n",
        "            batch_size             = args.batch_size,\n",
        "            write_graph            = True,\n",
        "            write_grads            = False,\n",
        "            write_images           = False,\n",
        "            embeddings_freq        = 0,\n",
        "            embeddings_layer_names = None,\n",
        "            embeddings_metadata    = None\n",
        "        )\n",
        "\n",
        "    if args.evaluation and validation_generator:\n",
        "        if args.dataset_type == 'coco':\n",
        "            from ..callbacks.coco import CocoEval\n",
        "\n",
        "            # use prediction model for evaluation\n",
        "            evaluation = CocoEval(validation_generator, tensorboard=tensorboard_callback)\n",
        "        else:\n",
        "            evaluation = Evaluate(validation_generator, tensorboard=tensorboard_callback, weighted_average=args.weighted_average)\n",
        "        evaluation = RedirectModel(evaluation, prediction_model)\n",
        "        callbacks.append(evaluation)\n",
        "\n",
        "    # save the model\n",
        "    if args.snapshots:\n",
        "        # ensure directory created first; otherwise h5py will error after epoch.\n",
        "        makedirs(args.snapshot_path)\n",
        "        checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(\n",
        "                args.snapshot_path,\n",
        "                '{backbone}_{dataset_type}_{{epoch:02d}}.h5'.format(backbone=args.backbone, dataset_type=args.dataset_type)\n",
        "            ),\n",
        "            verbose=1,\n",
        "            # save_best_only=True,\n",
        "            # monitor=\"mAP\",\n",
        "            # mode='max'\n",
        "        )\n",
        "        checkpoint = RedirectModel(checkpoint, model)\n",
        "        callbacks.append(checkpoint)\n",
        "\n",
        "    callbacks.append(keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor    = 'loss',\n",
        "        factor     = args.reduce_lr_factor,\n",
        "        patience   = args.reduce_lr_patience,\n",
        "        verbose    = 1,\n",
        "        mode       = 'auto',\n",
        "        min_delta  = 0.0001,\n",
        "        cooldown   = 0,\n",
        "        min_lr     = 0\n",
        "    ))\n",
        "\n",
        "    callbacks.append(keras.callbacks.EarlyStopping(\n",
        "        monitor    = 'classification_loss',\n",
        "        patience   = 200,\n",
        "        mode       = 'max',\n",
        "        min_delta  = 0.01\n",
        "    ))\n",
        "\n",
        "    if args.tensorboard_dir:\n",
        "        callbacks.append(tensorboard_callback)\n",
        "\n",
        "    return callbacks\n",
        "\n",
        "def create_generators(args, preprocess_image):\n",
        "    \"\"\" Create generators for training and validation.\n",
        "\n",
        "    Args\n",
        "        args             : parseargs object containing configuration for generators.\n",
        "        preprocess_image : Function that preprocesses an image for the network.\n",
        "    \"\"\"\n",
        "    common_args = {\n",
        "        'batch_size'       : args.batch_size,\n",
        "        'config'           : args.config,\n",
        "        'image_min_side'   : args.image_min_side,\n",
        "        'image_max_side'   : args.image_max_side,\n",
        "        'no_resize'        : args.no_resize,\n",
        "        'preprocess_image' : preprocess_image,\n",
        "    }\n",
        "\n",
        "    # create random transform generator for augmenting training data\n",
        "    if args.random_transform:\n",
        "        transform_generator = random_transform_generator(\n",
        "            min_rotation=-0.1,\n",
        "            max_rotation=0.1,\n",
        "            min_translation=(-0.1, -0.1),\n",
        "            max_translation=(0.1, 0.1),\n",
        "            min_shear=-0.1,\n",
        "            max_shear=0.1,\n",
        "            min_scaling=(0.9, 0.9),\n",
        "            max_scaling=(1.1, 1.1),\n",
        "            flip_x_chance=0.5,\n",
        "            flip_y_chance=0.5,\n",
        "        )\n",
        "        visual_effect_generator = random_visual_effect_generator(\n",
        "            contrast_range=(0.9, 1.1),\n",
        "            brightness_range=(-.1, .1),\n",
        "            hue_range=(-0.05, 0.05),\n",
        "            saturation_range=(0.95, 1.05)\n",
        "        )\n",
        "    else:\n",
        "        transform_generator = random_transform_generator(flip_x_chance=0.5)\n",
        "        visual_effect_generator = None\n",
        "\n",
        "    if args.dataset_type == 'coco':\n",
        "        # import here to prevent unnecessary dependency on cocoapi\n",
        "        from ..preprocessing.coco import CocoGenerator\n",
        "\n",
        "        train_generator = CocoGenerator(\n",
        "            args.coco_path,\n",
        "            'train2017',\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "\n",
        "        validation_generator = CocoGenerator(\n",
        "            args.coco_path,\n",
        "            'val2017',\n",
        "            shuffle_groups=False,\n",
        "            **common_args\n",
        "        )\n",
        "    elif args.dataset_type == 'pascal':\n",
        "        train_generator = PascalVocGenerator(\n",
        "            args.pascal_path,\n",
        "            'train',\n",
        "            image_extension=args.image_extension,\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "\n",
        "        validation_generator = PascalVocGenerator(\n",
        "            args.pascal_path,\n",
        "            'val',\n",
        "            image_extension=args.image_extension,\n",
        "            shuffle_groups=False,\n",
        "            **common_args\n",
        "        )\n",
        "    elif args.dataset_type == 'csv':\n",
        "        train_generator = CSVGenerator(\n",
        "            args.annotations,\n",
        "            args.classes,\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "\n",
        "        if args.val_annotations:\n",
        "            validation_generator = CSVGenerator(\n",
        "                args.val_annotations,\n",
        "                args.classes,\n",
        "                shuffle_groups=False,\n",
        "                **common_args\n",
        "            )\n",
        "        else:\n",
        "            validation_generator = None\n",
        "    elif args.dataset_type == 'oid':\n",
        "        train_generator = OpenImagesGenerator(\n",
        "            args.main_dir,\n",
        "            subset='train',\n",
        "            version=args.version,\n",
        "            labels_filter=args.labels_filter,\n",
        "            annotation_cache_dir=args.annotation_cache_dir,\n",
        "            parent_label=args.parent_label,\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "\n",
        "        validation_generator = OpenImagesGenerator(\n",
        "            args.main_dir,\n",
        "            subset='validation',\n",
        "            version=args.version,\n",
        "            labels_filter=args.labels_filter,\n",
        "            annotation_cache_dir=args.annotation_cache_dir,\n",
        "            parent_label=args.parent_label,\n",
        "            shuffle_groups=False,\n",
        "            **common_args\n",
        "        )\n",
        "    elif args.dataset_type == 'kitti':\n",
        "        train_generator = KittiGenerator(\n",
        "            args.kitti_path,\n",
        "            subset='train',\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "\n",
        "        validation_generator = KittiGenerator(\n",
        "            args.kitti_path,\n",
        "            subset='val',\n",
        "            shuffle_groups=False,\n",
        "            **common_args\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError('Invalid data type received: {}'.format(args.dataset_type))\n",
        "\n",
        "    return train_generator, validation_generator\n",
        "\n",
        "train_gen, valid_gen = create_generators(args, b.preprocess_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgvES4CUSP_x",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing anchor boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3gvbJMoIaM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# # from keras_retinanet.bin.train import create_generators\n",
        "# # from keras_retinanet.models import backbone\n",
        "# from keras_retinanet.utils.config import read_config_file,parse_anchor_parameters\n",
        "# # # from keras_retinanet.bin.debug import run\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# from keras_retinanet.utils.anchors import anchors_for_shape, compute_gt_annotations\n",
        "# from keras_retinanet.utils.visualization import draw_annotations, draw_boxes, draw_caption\n",
        "\n",
        "\n",
        "# def run(generator, args, anchor_params):\n",
        "#     \"\"\" Main loop.\n",
        "\n",
        "#     Args\n",
        "#         generator: The generator to debug.\n",
        "#         args: parseargs args object.\n",
        "#     \"\"\"\n",
        "#     # display images, one at a time\n",
        "#     i = 0\n",
        "#     while True:\n",
        "#         # load the data\n",
        "#         image       = generator.load_image(i)\n",
        "\n",
        "#         annotations = generator.load_annotations(i)\n",
        "#         if len(annotations['labels']) > 0 :\n",
        "#             # apply random transformations\n",
        "#             if args.random_transform:\n",
        "#                 image, annotations = generator.random_transform_group_entry(image, annotations)\n",
        "#                 image, annotations = generator.random_visual_effect_group_entry(image, annotations)\n",
        "\n",
        "#             # resize the image and annotations\n",
        "#             if args.resize:\n",
        "#                 image, image_scale = generator.resize_image(image)\n",
        "#                 annotations['bboxes'] *= image_scale\n",
        "\n",
        "#             anchors = anchors_for_shape(image.shape, anchor_params=anchor_params)\n",
        "#             positive_indices, _, max_indices = compute_gt_annotations(anchors, annotations['bboxes'])\n",
        "\n",
        "#             # draw anchors on the image\n",
        "#             if args.anchors:\n",
        "#                 draw_boxes(image, anchors[positive_indices], (255, 255, 0), thickness=1)\n",
        "\n",
        "#             # draw annotations on the image\n",
        "#             if args.annotations:\n",
        "#                 # draw annotations in red\n",
        "#                 draw_annotations(image, annotations, color=(0, 0, 255), label_to_name=generator.label_to_name)\n",
        "\n",
        "#                 # draw regressed anchors in green to override most red annotations\n",
        "#                 # result is that annotations without anchors are red, with anchors are green\n",
        "#                 draw_boxes(image, annotations['bboxes'][max_indices[positive_indices], :], (0, 255, 0))\n",
        "\n",
        "#             # display name on the image\n",
        "#             if args.display_name:\n",
        "#                 draw_caption(image, [0, image.shape[0]], os.path.basename(generator.image_path(i)))\n",
        "\n",
        "#         # write to file and advance if no-gui selected\n",
        "#         if args.no_gui:\n",
        "#             output_path = make_output_path(args.output_dir, generator.image_path(i), flatten=args.flatten_output)\n",
        "#             os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "#             cv2.imwrite(output_path, image)\n",
        "#             i += 1\n",
        "#             if i == generator.size():  # have written all images\n",
        "#                 break\n",
        "#             else:\n",
        "#                 continue\n",
        "\n",
        "#         # if we are using the GUI, then show an image\n",
        "#         cv2_imshow(image)\n",
        "#         key = cv2.waitKeyEx()\n",
        "\n",
        "#         # press right for next image and left for previous (linux or windows, doesn't work for macOS)\n",
        "#         # if you run macOS, press \"n\" or \"m\" (will also work on linux and windows)\n",
        "\n",
        "#         if key in rightkeys:\n",
        "#             i = (i + 1) % generator.size()\n",
        "#         if key in leftkeys:\n",
        "#             i -= 1\n",
        "#             if i < 0:\n",
        "#                 i = generator.size() - 1\n",
        "\n",
        "#         # press q or Esc to quit\n",
        "#         if (key == ord('q')) or (key == 27):\n",
        "#             return False\n",
        "\n",
        "#     return True\n",
        "\n",
        "\n",
        "# # train_gen, valid_gen = create_generators(args,b.preprocess_image)\n",
        "# while run(train_gen, args, parse_anchor_parameters(args.config)):\n",
        "#     pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSNyKpepHl5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4a9eadc-282a-4359-ef5e-a52396776810"
      },
      "source": [
        "import imgaug as ia\n",
        "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "# Define our sequence of augmentation steps that will be applied to every image.\n",
        "seq = iaa.Sequential(\n",
        "[      \n",
        "        # sometimes(iaa.Crop(percent=(0, 0.2))),\n",
        " \n",
        "        #  sometimes(iaa.Affine(\n",
        "        #     scale={\"x\": (0.9, 1.2), \"y\": (0.9, 1.2)},\n",
        "        #     translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "        #     rotate=(-45, 45),\n",
        "        #     shear=(-16, 16),\n",
        "        #     order=[0, 1],\n",
        "        #     cval=(0, 255),\n",
        "        #     mode=ia.ALL\n",
        "        # )),\n",
        "\n",
        "          # # Change brightness of images (85-115% of original value).\n",
        "          # iaa.Multiply((0.90, 1.15), per_channel=0.5),\n",
        "\n",
        "          # # # Improve or worsen the contrast of images.\n",
        "          # iaa.ContrastNormalization((0.75, 1.25), per_channel=0.5),\n",
        "\n",
        "          # Convert each image to grayscale and then overlay the\n",
        "          # result with the original with random alpha. I.e. remove\n",
        "          # colors with varying strengths.\n",
        "          # iaa.Grayscale(alpha=(0.0, 0.25)),\n",
        "#\n",
        "        # Execute 1 to 9 of the following (less important) augmenters per\n",
        "        # image. Don't execute all of them, as that would often be way too\n",
        "        # strong.\n",
        "        #\n",
        "        iaa.SomeOf((1, 9),\n",
        "            [\n",
        "\n",
        "                        # Blur each image with varying strength using\n",
        "                        # gaussian blur (sigma between 0 and .5),\n",
        "                        # average/uniform blur (kernel size 1x1)\n",
        "                        # median blur (kernel size 1x1).\n",
        "                        iaa.OneOf([\n",
        "                            iaa.GaussianBlur((0,1)),\n",
        "                            iaa.AverageBlur(k=(2,2)),\n",
        "                            iaa.MedianBlur(k=(1,1)),\n",
        "                        ]),\n",
        "\n",
        "                        # Sharpen each image, overlay the result with the original\n",
        "                        # image using an alpha between 0 (no sharpening) and 1\n",
        "                        # (full sharpening effect).\n",
        "                        iaa.Sharpen(alpha=(0, 0.25), lightness=(0.75, 1.5)),\n",
        "\n",
        "                        # Add gaussian noise to some images.\n",
        "                        # In 50% of these cases, the noise is randomly sampled per\n",
        "                        # channel and pixel.\n",
        "                        # In the other 50% of all cases it is sampled once per\n",
        "                        # pixel (i.e. brightness change).\n",
        "                        iaa.AdditiveGaussianNoise(\n",
        "                            loc=0, scale=(0.0, 0.01*255), per_channel=0.5\n",
        "                        ),\n",
        "\n",
        "                        # Either drop randomly 1 to 10% of all pixels (i.e. set\n",
        "                        # them to black) or drop them on an image with 2-5% percent\n",
        "                        # of the original size, leading to large dropped\n",
        "                        # rectangles.\n",
        "                        # iaa.OneOf([\n",
        "                        #     iaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
        "                        #     iaa.CoarseDropout(\n",
        "                        #         (0.03, 0.15), size_percent=(0.02, 0.05),\n",
        "                        #         per_channel=0.2\n",
        "                        #     ),\n",
        "                        # ]),\n",
        "             \n",
        "                        \n",
        "                        # Invert each image's channel with 5% probability.\n",
        "                        # This sets each pixel value v to 255-v.\n",
        "                        iaa.Invert(0.05, per_channel=True), # invert color channels\n",
        "\n",
        "                        # Add a value of -5 to 5 to each pixel.\n",
        "                        iaa.Add((-5, 5), per_channel=0.5),\n",
        "\n",
        "                        # # Change brightness of images (85-115% of original value).\n",
        "                        iaa.Multiply((0.85, 1.15), per_channel=0.5),\n",
        "\n",
        "                        # # Improve or worsen the contrast of images.\n",
        "                        iaa.ContrastNormalization((0.75, 1.25), per_channel=0.5),\n",
        "\n",
        "                        # Convert each image to grayscale and then overlay the\n",
        "                        # result with the original with random alpha. I.e. remove\n",
        "                        # colors with varying strengths.\n",
        "                        iaa.Grayscale(alpha=(0.0, 0.25)),\n",
        "\n",
        "                        # In some images distort local areas with varying strength.\n",
        "                        sometimes(iaa.PiecewiseAffine(scale=(0.001, 0.01)))\n",
        "                    ],\n",
        "            # do all of the above augmentations in random order\n",
        "            random_order=True\n",
        "        )\n",
        "    ],\n",
        "    # do all of the above augmentations in random order\n",
        "    random_order=True\n",
        ")\n",
        "\n",
        "\n",
        "def augment_train_gen(train_gen, visualize=False):\n",
        "    '''\n",
        "    Creates a generator using another generator with applied image augmentation.\n",
        "    Args\n",
        "        train_gen  : keras-retinanet generator object.\n",
        "        visualize  : Boolean; False will convert bounding boxes to their anchor box targets for the model.\n",
        "    '''\n",
        "    imgs = []\n",
        "    boxes = []\n",
        "    targets = []\n",
        "    size = train_gen.size()\n",
        "    idx = 0\n",
        "    while True:\n",
        "        while len(imgs) < args.batch_size:\n",
        "            image       = train_gen.load_image(idx % size)\n",
        "            annotations = train_gen.load_annotations(idx % size)\n",
        "            image,annotations = train_gen.random_transform_group_entry(image,annotations)\n",
        "            imgs.append(image)            \n",
        "            boxes.append(annotations['bboxes'])\n",
        "            targets.append(annotations)\n",
        "            idx += 1\n",
        "        if visualize:\n",
        "            imgs = seq.augment_images(imgs)\n",
        "            imgs = np.array(imgs)\n",
        "            boxes = np.array(boxes)\n",
        "            yield imgs,boxes\n",
        "        else:\n",
        "            imgs = seq.augment_images(imgs)\n",
        "            imgs,targets = train_gen.preprocess_group(imgs,targets)\n",
        "            imgs = train_gen.compute_inputs(imgs)\n",
        "            targets = train_gen.compute_targets(imgs,targets)\n",
        "            imgs = np.array(imgs)\n",
        "            yield imgs,targets\n",
        "        imgs = []\n",
        "        boxes = []\n",
        "        targets = []\n",
        "\t\t\n",
        "\n",
        "model, training_model, prediction_model = create_models(\n",
        "            backbone_retinanet=b.retinanet,\n",
        "            num_classes=train_gen.num_classes(),\n",
        "            weights=None,\n",
        "            multi_gpu=False,\n",
        "            freeze_backbone=True,\n",
        "            lr=1e-3,\n",
        "            config=args.config\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable:0' shape=(15, 4) dtype=float32, numpy=\n",
            "array([[-22.627417, -11.313708,  22.627417,  11.313708],\n",
            "       [-33.941124, -16.970562,  33.941124,  16.970562],\n",
            "       [-45.254833, -22.627417,  45.254833,  22.627417],\n",
            "       [-16.      , -16.      ,  16.      ,  16.      ],\n",
            "       [-24.      , -24.      ,  24.      ,  24.      ],\n",
            "       [-32.      , -32.      ,  32.      ,  32.      ],\n",
            "       [-13.063946, -19.595919,  13.063946,  19.595919],\n",
            "       [-19.595919, -29.393877,  19.595919,  29.393877],\n",
            "       [-26.127892, -39.191837,  26.127892,  39.191837],\n",
            "       [-11.313708, -22.627417,  11.313708,  22.627417],\n",
            "       [-16.970562, -33.941124,  16.970562,  33.941124],\n",
            "       [-22.627417, -45.254833,  22.627417,  45.254833],\n",
            "       [-10.119288, -25.298222,  10.119288,  25.298222],\n",
            "       [-15.178933, -37.94733 ,  15.178933,  37.94733 ],\n",
            "       [-20.238577, -50.596443,  20.238577,  50.596443]], dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(15, 4) dtype=float32, numpy=\n",
            "array([[ -45.254833,  -22.627417,   45.254833,   22.627417],\n",
            "       [ -67.88225 ,  -33.941124,   67.88225 ,   33.941124],\n",
            "       [ -90.50967 ,  -45.254833,   90.50967 ,   45.254833],\n",
            "       [ -32.      ,  -32.      ,   32.      ,   32.      ],\n",
            "       [ -48.      ,  -48.      ,   48.      ,   48.      ],\n",
            "       [ -64.      ,  -64.      ,   64.      ,   64.      ],\n",
            "       [ -26.127892,  -39.191837,   26.127892,   39.191837],\n",
            "       [ -39.191837,  -58.787754,   39.191837,   58.787754],\n",
            "       [ -52.255783,  -78.383675,   52.255783,   78.383675],\n",
            "       [ -22.627417,  -45.254833,   22.627417,   45.254833],\n",
            "       [ -33.941124,  -67.88225 ,   33.941124,   67.88225 ],\n",
            "       [ -45.254833,  -90.50967 ,   45.254833,   90.50967 ],\n",
            "       [ -20.238577,  -50.596443,   20.238577,   50.596443],\n",
            "       [ -30.357866,  -75.89466 ,   30.357866,   75.89466 ],\n",
            "       [ -40.477154, -101.19289 ,   40.477154,  101.19289 ]],\n",
            "      dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(15, 4) dtype=float32, numpy=\n",
            "array([[ -90.50967 ,  -45.254833,   90.50967 ,   45.254833],\n",
            "       [-135.7645  ,  -67.88225 ,  135.7645  ,   67.88225 ],\n",
            "       [-181.01933 ,  -90.50967 ,  181.01933 ,   90.50967 ],\n",
            "       [ -64.      ,  -64.      ,   64.      ,   64.      ],\n",
            "       [ -96.      ,  -96.      ,   96.      ,   96.      ],\n",
            "       [-128.      , -128.      ,  128.      ,  128.      ],\n",
            "       [ -52.255783,  -78.383675,   52.255783,   78.383675],\n",
            "       [ -78.383675, -117.57551 ,   78.383675,  117.57551 ],\n",
            "       [-104.511566, -156.76735 ,  104.511566,  156.76735 ],\n",
            "       [ -45.254833,  -90.50967 ,   45.254833,   90.50967 ],\n",
            "       [ -67.88225 , -135.7645  ,   67.88225 ,  135.7645  ],\n",
            "       [ -90.50967 , -181.01933 ,   90.50967 ,  181.01933 ],\n",
            "       [ -40.477154, -101.19289 ,   40.477154,  101.19289 ],\n",
            "       [ -60.715733, -151.78932 ,   60.715733,  151.78932 ],\n",
            "       [ -80.95431 , -202.38577 ,   80.95431 ,  202.38577 ]],\n",
            "      dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(15, 4) dtype=float32, numpy=\n",
            "array([[-181.01933 ,  -90.50967 ,  181.01933 ,   90.50967 ],\n",
            "       [-271.529   , -135.7645  ,  271.529   ,  135.7645  ],\n",
            "       [-362.03867 , -181.01933 ,  362.03867 ,  181.01933 ],\n",
            "       [-128.      , -128.      ,  128.      ,  128.      ],\n",
            "       [-192.      , -192.      ,  192.      ,  192.      ],\n",
            "       [-256.      , -256.      ,  256.      ,  256.      ],\n",
            "       [-104.511566, -156.76735 ,  104.511566,  156.76735 ],\n",
            "       [-156.76735 , -235.15102 ,  156.76735 ,  235.15102 ],\n",
            "       [-209.02313 , -313.5347  ,  209.02313 ,  313.5347  ],\n",
            "       [ -90.50967 , -181.01933 ,   90.50967 ,  181.01933 ],\n",
            "       [-135.7645  , -271.529   ,  135.7645  ,  271.529   ],\n",
            "       [-181.01933 , -362.03867 ,  181.01933 ,  362.03867 ],\n",
            "       [ -80.95431 , -202.38577 ,   80.95431 ,  202.38577 ],\n",
            "       [-121.431465, -303.57864 ,  121.431465,  303.57864 ],\n",
            "       [-161.90862 , -404.77155 ,  161.90862 ,  404.77155 ]],\n",
            "      dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(15, 4) dtype=float32, numpy=\n",
            "array([[-362.03867, -181.01933,  362.03867,  181.01933],\n",
            "       [-543.058  , -271.529  ,  543.058  ,  271.529  ],\n",
            "       [-724.07733, -362.03867,  724.07733,  362.03867],\n",
            "       [-256.     , -256.     ,  256.     ,  256.     ],\n",
            "       [-384.     , -384.     ,  384.     ,  384.     ],\n",
            "       [-512.     , -512.     ,  512.     ,  512.     ],\n",
            "       [-209.02313, -313.5347 ,  209.02313,  313.5347 ],\n",
            "       [-313.5347 , -470.30203,  313.5347 ,  470.30203],\n",
            "       [-418.04626, -627.0694 ,  418.04626,  627.0694 ],\n",
            "       [-181.01933, -362.03867,  181.01933,  362.03867],\n",
            "       [-271.529  , -543.058  ,  271.529  ,  543.058  ],\n",
            "       [-362.03867, -724.07733,  362.03867,  724.07733],\n",
            "       [-161.90862, -404.77155,  161.90862,  404.77155],\n",
            "       [-242.86293, -607.1573 ,  242.86293,  607.1573 ],\n",
            "       [-323.81723, -809.5431 ,  323.81723,  809.5431 ]], dtype=float32)> anchors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD_FWSIhfW40",
        "colab_type": "text"
      },
      "source": [
        "# Training with different parameters and ratios\n",
        "- sizes io24, 2048 , nratios  = 0.5 1 1.5 2 2.5\\nscales  =1 1.5 2\\n')\n",
        "- IOU = 0.6 and NMS back at normal, score threshold = 0.15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l00nBD3hfgmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_retinanet.callbacks import RedirectModel \n",
        "callbacks = create_callbacks(\n",
        "    model,\n",
        "    training_model,\n",
        "    prediction_model,\n",
        "    valid_gen,\n",
        "    args,\n",
        ")\n",
        "training_model.load_weights('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_25_classloss_0.0632.h5', skip_mismatch = True, by_name = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XENIk4e8fgdN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "outputId": "4118ae42-ed35-47e0-9997-785c35c34ab6"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks = callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "29/42 [===================>..........] - ETA: 1:43 - loss: 1.0934 - regression_loss: 0.9978 - classification_loss: 0.0957"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:590: UserWarning: Metadata Warning, tag 282 had too many entries: 2, expected 1\n",
            "  % (tag, len(values))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:590: UserWarning: Metadata Warning, tag 283 had too many entries: 2, expected 1\n",
            "  % (tag, len(values))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "42/42 [==============================] - 364s 9s/step - loss: 1.1152 - regression_loss: 1.0029 - classification_loss: 0.1124\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v6/resnet50_csv_01.h5\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - 301s 7s/step - loss: 1.0498 - regression_loss: 0.9531 - classification_loss: 0.0967\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v6/resnet50_csv_02.h5\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - 301s 7s/step - loss: 1.0139 - regression_loss: 0.9178 - classification_loss: 0.0961\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v6/resnet50_csv_03.h5\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - 277s 7s/step - loss: 1.0233 - regression_loss: 0.9296 - classification_loss: 0.0937\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v6/resnet50_csv_04.h5\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - 277s 7s/step - loss: 1.0394 - regression_loss: 0.9328 - classification_loss: 0.1066\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v6/resnet50_csv_05.h5\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - 297s 7s/step - loss: 1.0616 - regression_loss: 0.9546 - classification_loss: 0.1070\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v6/resnet50_csv_06.h5\n",
            "Epoch 7/200\n",
            "12/42 [=======>......................] - ETA: 3:34 - loss: 1.0145 - regression_loss: 0.9246 - classification_loss: 0.0899"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-3a759ca49519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         callbacks = callbacks,) \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkxXaoJYBjnu",
        "colab_type": "text"
      },
      "source": [
        "# Training a complete hand model currently"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtB95pB_Hlt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_retinanet.callbacks import RedirectModel \n",
        "callbacks = create_callbacks(\n",
        "    model,\n",
        "    training_model,\n",
        "    prediction_model,\n",
        "    valid_gen,\n",
        "    args,\n",
        ")\n",
        "training_model.load_weights('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_17_class_loss_0.0862.h5', skip_mismatch = True, by_name = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jSsWIZLHi9g",
        "colab_type": "text"
      },
      "source": [
        "- incrased size to 1024 and reduces io and nms by 0.05\n",
        "- config file with open('config.ini','w') as f:\n",
        "    f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512 1024\\nstrides = 8 16 32 64 128 256\\nratios  = 1.2 1.5 2 2.5 3\\nscales  =1 1.5 2\\n')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov1rVMHBHjWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c10492c5-7324-4646-a18a-058e0793ad39"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks = callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "29/42 [===================>..........] - ETA: 1:44 - loss: 0.9826 - regression_loss: 0.9180 - classification_loss: 0.0647"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:590: UserWarning: Metadata Warning, tag 282 had too many entries: 2, expected 1\n",
            "  % (tag, len(values))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:590: UserWarning: Metadata Warning, tag 283 had too many entries: 2, expected 1\n",
            "  % (tag, len(values))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "34/42 [=======================>......] - ETA: 1:03 - loss: 1.0818 - regression_loss: 0.9799 - classification_loss: 0.1019"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:718: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "42/42 [==============================] - 367s 9s/step - loss: 1.0688 - regression_loss: 0.9662 - classification_loss: 0.1027\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_01.h5\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - 415s 10s/step - loss: 1.1252 - regression_loss: 1.0097 - classification_loss: 0.1155\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_02.h5\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - 382s 9s/step - loss: 1.0552 - regression_loss: 0.9527 - classification_loss: 0.1025\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_03.h5\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - 355s 8s/step - loss: 1.0855 - regression_loss: 0.9825 - classification_loss: 0.1030\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_04.h5\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - 373s 9s/step - loss: 1.0728 - regression_loss: 0.9674 - classification_loss: 0.1054\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_05.h5\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - 394s 9s/step - loss: 1.0815 - regression_loss: 0.9738 - classification_loss: 0.1077\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_06.h5\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - 327s 8s/step - loss: 1.0728 - regression_loss: 0.9618 - classification_loss: 0.1110\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_07.h5\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - 383s 9s/step - loss: 1.0640 - regression_loss: 0.9538 - classification_loss: 0.1103\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_08.h5\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - 365s 9s/step - loss: 1.0199 - regression_loss: 0.9292 - classification_loss: 0.0907\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_09.h5\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - 356s 8s/step - loss: 0.9949 - regression_loss: 0.9097 - classification_loss: 0.0852\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_10.h5\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - 372s 9s/step - loss: 0.9945 - regression_loss: 0.9121 - classification_loss: 0.0824\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_11.h5\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - 383s 9s/step - loss: 0.9774 - regression_loss: 0.8971 - classification_loss: 0.0804\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_12.h5\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - 385s 9s/step - loss: 0.9820 - regression_loss: 0.9004 - classification_loss: 0.0816\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_13.h5\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - 375s 9s/step - loss: 0.9717 - regression_loss: 0.8914 - classification_loss: 0.0802\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_14.h5\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - 378s 9s/step - loss: 0.9580 - regression_loss: 0.8802 - classification_loss: 0.0779\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_15.h5\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - 372s 9s/step - loss: 0.9714 - regression_loss: 0.8904 - classification_loss: 0.0810\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_16.h5\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - 337s 8s/step - loss: 0.9622 - regression_loss: 0.8838 - classification_loss: 0.0785\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_17.h5\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - 360s 9s/step - loss: 0.9606 - regression_loss: 0.8851 - classification_loss: 0.0755\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_18.h5\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - 360s 9s/step - loss: 0.9645 - regression_loss: 0.8863 - classification_loss: 0.0781\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_19.h5\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - 353s 8s/step - loss: 0.9436 - regression_loss: 0.8740 - classification_loss: 0.0696\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_20.h5\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - 334s 8s/step - loss: 0.9173 - regression_loss: 0.8491 - classification_loss: 0.0682\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_21.h5\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - 371s 9s/step - loss: 0.9459 - regression_loss: 0.8722 - classification_loss: 0.0737\n",
            "\n",
            "Epoch 00022: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_22.h5\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - 370s 9s/step - loss: 0.9313 - regression_loss: 0.8578 - classification_loss: 0.0735\n",
            "\n",
            "Epoch 00023: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_23.h5\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - 340s 8s/step - loss: 0.9080 - regression_loss: 0.8426 - classification_loss: 0.0655\n",
            "\n",
            "Epoch 00024: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_24.h5\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - 415s 10s/step - loss: 0.9078 - regression_loss: 0.8442 - classification_loss: 0.0636\n",
            "\n",
            "Epoch 00025: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_25.h5\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - 379s 9s/step - loss: 0.9335 - regression_loss: 0.8616 - classification_loss: 0.0719\n",
            "\n",
            "Epoch 00026: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_26.h5\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - 351s 8s/step - loss: 0.9342 - regression_loss: 0.8610 - classification_loss: 0.0732\n",
            "\n",
            "Epoch 00027: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_27.h5\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - 374s 9s/step - loss: 0.9405 - regression_loss: 0.8677 - classification_loss: 0.0728\n",
            "\n",
            "Epoch 00028: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_28.h5\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - 361s 9s/step - loss: 0.9293 - regression_loss: 0.8596 - classification_loss: 0.0697\n",
            "\n",
            "Epoch 00029: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_29.h5\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - 402s 10s/step - loss: 0.9587 - regression_loss: 0.8826 - classification_loss: 0.0761\n",
            "\n",
            "Epoch 00030: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_30.h5\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - 354s 8s/step - loss: 0.9344 - regression_loss: 0.8616 - classification_loss: 0.0728\n",
            "\n",
            "Epoch 00031: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_31.h5\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - 361s 9s/step - loss: 0.9249 - regression_loss: 0.8533 - classification_loss: 0.0716\n",
            "\n",
            "Epoch 00032: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_32.h5\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - 355s 8s/step - loss: 0.9344 - regression_loss: 0.8612 - classification_loss: 0.0731\n",
            "\n",
            "Epoch 00033: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_33.h5\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "Epoch 34/200\n",
            "42/42 [==============================] - 345s 8s/step - loss: 0.9333 - regression_loss: 0.8607 - classification_loss: 0.0726\n",
            "\n",
            "Epoch 00034: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_34.h5\n",
            "Epoch 35/200\n",
            "42/42 [==============================] - 354s 8s/step - loss: 0.9365 - regression_loss: 0.8646 - classification_loss: 0.0719\n",
            "\n",
            "Epoch 00035: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_35.h5\n",
            "Epoch 36/200\n",
            "42/42 [==============================] - 404s 10s/step - loss: 0.9294 - regression_loss: 0.8586 - classification_loss: 0.0708\n",
            "\n",
            "Epoch 00036: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_36.h5\n",
            "Epoch 37/200\n",
            "42/42 [==============================] - 361s 9s/step - loss: 0.9437 - regression_loss: 0.8691 - classification_loss: 0.0746\n",
            "\n",
            "Epoch 00037: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_37.h5\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "Epoch 38/200\n",
            "42/42 [==============================] - 376s 9s/step - loss: 0.9446 - regression_loss: 0.8741 - classification_loss: 0.0705\n",
            "\n",
            "Epoch 00038: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_38.h5\n",
            "Epoch 39/200\n",
            "42/42 [==============================] - 346s 8s/step - loss: 0.9518 - regression_loss: 0.8761 - classification_loss: 0.0757\n",
            "\n",
            "Epoch 00039: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continuedsss/resnet50_csv_39.h5\n",
            "Epoch 40/200\n",
            " 8/42 [====>.........................] - ETA: 4:36 - loss: 0.8813 - regression_loss: 0.8119 - classification_loss: 0.0694"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOKpbCEOpuLI",
        "colab_type": "text"
      },
      "source": [
        "# Training continued of below logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E59ek02Lpus3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0df00858-9c19-4649-c36b-411b38258905"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks = callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "29/42 [===================>..........] - ETA: 1:46 - loss: 1.0149 - regression_loss: 0.9326 - classification_loss: 0.0823"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:590: UserWarning: Metadata Warning, tag 282 had too many entries: 2, expected 1\n",
            "  % (tag, len(values))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:590: UserWarning: Metadata Warning, tag 283 had too many entries: 2, expected 1\n",
            "  % (tag, len(values))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "42/42 [==============================] - 388s 9s/step - loss: 1.1280 - regression_loss: 0.9993 - classification_loss: 0.1287\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_01.h5\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - 352s 8s/step - loss: 1.1799 - regression_loss: 1.0382 - classification_loss: 0.1417\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_02.h5\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - 382s 9s/step - loss: 1.1375 - regression_loss: 1.0048 - classification_loss: 0.1328\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_03.h5\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - 352s 8s/step - loss: 1.1509 - regression_loss: 1.0061 - classification_loss: 0.1448\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_04.h5\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - 363s 9s/step - loss: 1.1245 - regression_loss: 0.9992 - classification_loss: 0.1253\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_05.h5\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - 350s 8s/step - loss: 1.1285 - regression_loss: 1.0006 - classification_loss: 0.1279\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_06.h5\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - 401s 10s/step - loss: 1.1118 - regression_loss: 0.9921 - classification_loss: 0.1197\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_07.h5\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - 376s 9s/step - loss: 1.1119 - regression_loss: 0.9887 - classification_loss: 0.1232\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_08.h5\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - 380s 9s/step - loss: 1.1123 - regression_loss: 0.9881 - classification_loss: 0.1242\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_09.h5\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - 395s 9s/step - loss: 1.1122 - regression_loss: 0.9938 - classification_loss: 0.1184\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_10.h5\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - 367s 9s/step - loss: 1.0961 - regression_loss: 0.9808 - classification_loss: 0.1152\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_11.h5\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - 383s 9s/step - loss: 1.0978 - regression_loss: 0.9708 - classification_loss: 0.1271\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_12.h5\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - 379s 9s/step - loss: 1.1245 - regression_loss: 0.9967 - classification_loss: 0.1278\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_13.h5\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - 372s 9s/step - loss: 1.1503 - regression_loss: 1.0146 - classification_loss: 0.1358\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_14.h5\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - 391s 9s/step - loss: 1.1252 - regression_loss: 1.0019 - classification_loss: 0.1233\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_15.h5\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - 353s 8s/step - loss: 1.0783 - regression_loss: 0.9615 - classification_loss: 0.1167\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_16.h5\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - 381s 9s/step - loss: 1.0093 - regression_loss: 0.9231 - classification_loss: 0.0862\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_17.h5\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - 359s 9s/step - loss: 1.0209 - regression_loss: 0.9312 - classification_loss: 0.0897\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5 continued/resnet50_csv_18.h5\n",
            "Epoch 19/200\n",
            "20/42 [=============>................] - ETA: 3:33 - loss: 0.9744 - regression_loss: 0.8918 - classification_loss: 0.0827"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3a759ca49519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         callbacks = callbacks,) \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWg7hzoOCeNg",
        "colab_type": "text"
      },
      "source": [
        "# Made few changes\n",
        "- nratios  = 1.2 1.3 1.5 2 2.2 2.8 3\\nscales  =1 1.5 2\\n')\n",
        "- score threshold = 0.1, iou and NMS unchanged\n",
        "- Added affine transformation\n",
        "- batch size 2\n",
        "- added mAP as early stopping criteria\n",
        "- Backbone is resnet coco"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3iksjAyCepm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2adc0bcf-caa4-4219-f2bf-2e0860849e6b"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks = callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "29/42 [===================>..........] - ETA: 1:47 - loss: 2.8215 - regression_loss: 2.1974 - classification_loss: 0.6241"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:590: UserWarning: Metadata Warning, tag 282 had too many entries: 2, expected 1\n",
            "  % (tag, len(values))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:590: UserWarning: Metadata Warning, tag 283 had too many entries: 2, expected 1\n",
            "  % (tag, len(values))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "42/42 [==============================] - 383s 9s/step - loss: 2.6237 - regression_loss: 2.0513 - classification_loss: 0.5724\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_01.h5\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - 343s 8s/step - loss: 1.8557 - regression_loss: 1.5202 - classification_loss: 0.3356\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_02.h5\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - 362s 9s/step - loss: 1.5811 - regression_loss: 1.3159 - classification_loss: 0.2652\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_03.h5\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - 356s 8s/step - loss: 1.4605 - regression_loss: 1.2267 - classification_loss: 0.2338\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_04.h5\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - 380s 9s/step - loss: 1.4378 - regression_loss: 1.2003 - classification_loss: 0.2375\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_05.h5\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - 373s 9s/step - loss: 1.4387 - regression_loss: 1.2161 - classification_loss: 0.2226\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_06.h5\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - 328s 8s/step - loss: 1.3697 - regression_loss: 1.1443 - classification_loss: 0.2253\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_07.h5\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - 339s 8s/step - loss: 1.3775 - regression_loss: 1.1623 - classification_loss: 0.2151\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_08.h5\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - 330s 8s/step - loss: 1.3105 - regression_loss: 1.1217 - classification_loss: 0.1888\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_09.h5\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - 373s 9s/step - loss: 1.2594 - regression_loss: 1.0792 - classification_loss: 0.1803\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_10.h5\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - 374s 9s/step - loss: 1.2727 - regression_loss: 1.1065 - classification_loss: 0.1662\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_11.h5\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - 374s 9s/step - loss: 1.2830 - regression_loss: 1.1174 - classification_loss: 0.1656\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_12.h5\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - 319s 8s/step - loss: 1.2605 - regression_loss: 1.0967 - classification_loss: 0.1638\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_13.h5\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - 337s 8s/step - loss: 1.2509 - regression_loss: 1.0931 - classification_loss: 0.1578\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_14.h5\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - 343s 8s/step - loss: 1.2198 - regression_loss: 1.0564 - classification_loss: 0.1634\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_15.h5\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - 356s 8s/step - loss: 1.1848 - regression_loss: 1.0329 - classification_loss: 0.1519\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_16.h5\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - 352s 8s/step - loss: 1.1846 - regression_loss: 1.0302 - classification_loss: 0.1544\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_17.h5\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - 366s 9s/step - loss: 1.1949 - regression_loss: 1.0361 - classification_loss: 0.1588\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_18.h5\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - 357s 9s/step - loss: 1.1840 - regression_loss: 1.0237 - classification_loss: 0.1603\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_19.h5\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - 339s 8s/step - loss: 1.1309 - regression_loss: 1.0054 - classification_loss: 0.1256\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_20.h5\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - 377s 9s/step - loss: 1.2149 - regression_loss: 1.0622 - classification_loss: 0.1526\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v5/resnet50_csv_21.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fde4f801128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJgsZVtXWrHR",
        "colab_type": "text"
      },
      "source": [
        "# Made few changes\n",
        "- nratios  = 0.5 1 1.5 2 2.5 3\\nscales  =1 1.2 1.6\\n')\n",
        "- score threshold = 0.1, iou and NMS unchanged\n",
        "- Added affine transformation\n",
        "- batch size 2\n",
        "- added mAP as early stopping criteria\n",
        "- Backbone is class loss 0.1714"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4ZcVzE-WruM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "outputId": "e9ac1699-cddf-44b8-8f05-685b8ebd9869"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks = callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "119/170 [====================>.........] - ETA: 1:58 - loss: 2.7448 - regression_loss: 2.1816 - classification_loss: 0.5632"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:590: UserWarning: Metadata Warning, tag 282 had too many entries: 2, expected 1\n",
            "  % (tag, len(values))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:590: UserWarning: Metadata Warning, tag 283 had too many entries: 2, expected 1\n",
            "  % (tag, len(values))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "170/170 [==============================] - 407s 2s/step - loss: 2.6722 - regression_loss: 2.1600 - classification_loss: 0.5121\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v2/resnet50_csv_01.h5\n",
            "Epoch 2/200\n",
            "170/170 [==============================] - 437s 3s/step - loss: 2.3889 - regression_loss: 2.0274 - classification_loss: 0.3616\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v2/resnet50_csv_02.h5\n",
            "Epoch 3/200\n",
            "170/170 [==============================] - 411s 2s/step - loss: 2.4034 - regression_loss: 2.0334 - classification_loss: 0.3701\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v2/resnet50_csv_03.h5\n",
            "Epoch 4/200\n",
            "170/170 [==============================] - 417s 2s/step - loss: 2.4080 - regression_loss: 2.0397 - classification_loss: 0.3683\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v2/resnet50_csv_04.h5\n",
            "Epoch 5/200\n",
            "170/170 [==============================] - 392s 2s/step - loss: 2.4016 - regression_loss: 2.0443 - classification_loss: 0.3573\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v2/resnet50_csv_05.h5\n",
            "Epoch 6/200\n",
            "170/170 [==============================] - 435s 3s/step - loss: 2.4267 - regression_loss: 2.0683 - classification_loss: 0.3584\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v2/resnet50_csv_06.h5\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 7/200\n",
            "170/170 [==============================] - 416s 2s/step - loss: 2.3396 - regression_loss: 2.0060 - classification_loss: 0.3335\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/Models v2/resnet50_csv_07.h5\n",
            "Epoch 8/200\n",
            "130/170 [=====================>........] - ETA: 1:26 - loss: 2.3925 - regression_loss: 2.0525 - classification_loss: 0.3401"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-3a759ca49519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         callbacks = callbacks,) \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agw9q97gxM8D",
        "colab_type": "text"
      },
      "source": [
        "# Final run for hand model\n",
        "- All parameters set to default\n",
        "- IOU  0.5\n",
        "- NMS = 0.5\n",
        "- SCORE THRESHOLD = 0.05\n",
        "- Image max side = 1400\n",
        "- Image min side = 1000\n",
        "- Ratios = 0.8 1 1.3 1.5 2 2.2 2.8 3\n",
        "- scales  =1 1.2 1.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0tRyxFSMJvn",
        "colab_type": "text"
      },
      "source": [
        "## Restarting training from 54th epoch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73jRCFTfMJVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a328d93-00ea-4f75-81c0-b75ddf3f80cf"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "85/85 [==============================] - 421s 5s/step - loss: 1.7830 - regression_loss: 1.5708 - classification_loss: 0.2121\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_01.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `mAP` which is not available. Available metrics are: loss,regression_loss,classification_loss,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/200\n",
            "85/85 [==============================] - 406s 5s/step - loss: 1.7969 - regression_loss: 1.5852 - classification_loss: 0.2117\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_02.h5\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 442s 5s/step - loss: 1.7954 - regression_loss: 1.5801 - classification_loss: 0.2153\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_03.h5\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 404s 5s/step - loss: 1.7720 - regression_loss: 1.5573 - classification_loss: 0.2147\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_04.h5\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 463s 5s/step - loss: 1.8315 - regression_loss: 1.6037 - classification_loss: 0.2278\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_05.h5\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 411s 5s/step - loss: 1.7878 - regression_loss: 1.5711 - classification_loss: 0.2167\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_06.h5\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 436s 5s/step - loss: 1.8054 - regression_loss: 1.5876 - classification_loss: 0.2178\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_07.h5\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 393s 5s/step - loss: 1.8426 - regression_loss: 1.6130 - classification_loss: 0.2296\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_08.h5\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 406s 5s/step - loss: 1.6949 - regression_loss: 1.4995 - classification_loss: 0.1954\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_09.h5\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 444s 5s/step - loss: 1.7090 - regression_loss: 1.5125 - classification_loss: 0.1965\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_10.h5\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 434s 5s/step - loss: 1.7228 - regression_loss: 1.5332 - classification_loss: 0.1895\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_11.h5\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 422s 5s/step - loss: 1.6959 - regression_loss: 1.5008 - classification_loss: 0.1950\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_12.h5\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 402s 5s/step - loss: 1.7412 - regression_loss: 1.5466 - classification_loss: 0.1947\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_13.h5\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 417s 5s/step - loss: 1.7423 - regression_loss: 1.5483 - classification_loss: 0.1940\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_14.h5\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 436s 5s/step - loss: 1.7054 - regression_loss: 1.5135 - classification_loss: 0.1919\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_15.h5\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 446s 5s/step - loss: 1.6437 - regression_loss: 1.4652 - classification_loss: 0.1785\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_16.h5\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 442s 5s/step - loss: 1.6822 - regression_loss: 1.4897 - classification_loss: 0.1925\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_17.h5\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 409s 5s/step - loss: 1.6116 - regression_loss: 1.4397 - classification_loss: 0.1719\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_18.h5\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 423s 5s/step - loss: 1.6961 - regression_loss: 1.5135 - classification_loss: 0.1826\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_19.h5\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 417s 5s/step - loss: 1.6518 - regression_loss: 1.4708 - classification_loss: 0.1810\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_20.h5\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 385s 5s/step - loss: 1.7000 - regression_loss: 1.5126 - classification_loss: 0.1873\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_21.h5\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 412s 5s/step - loss: 1.6816 - regression_loss: 1.4943 - classification_loss: 0.1873\n",
            "\n",
            "Epoch 00022: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_22.h5\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 414s 5s/step - loss: 1.7000 - regression_loss: 1.5108 - classification_loss: 0.1891\n",
            "\n",
            "Epoch 00023: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_23.h5\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 387s 5s/step - loss: 1.6455 - regression_loss: 1.4640 - classification_loss: 0.1815\n",
            "\n",
            "Epoch 00024: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_24.h5\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 389s 5s/step - loss: 1.6802 - regression_loss: 1.4987 - classification_loss: 0.1815\n",
            "\n",
            "Epoch 00025: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_25.h5\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 391s 5s/step - loss: 1.6724 - regression_loss: 1.4878 - classification_loss: 0.1846\n",
            "\n",
            "Epoch 00026: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_26.h5\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 388s 5s/step - loss: 1.6477 - regression_loss: 1.4699 - classification_loss: 0.1777\n",
            "\n",
            "Epoch 00027: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_27.h5\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 425s 5s/step - loss: 1.6371 - regression_loss: 1.4609 - classification_loss: 0.1762\n",
            "\n",
            "Epoch 00028: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_28.h5\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 415s 5s/step - loss: 1.6618 - regression_loss: 1.4813 - classification_loss: 0.1805\n",
            "\n",
            "Epoch 00029: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_29.h5\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 441s 5s/step - loss: 1.7033 - regression_loss: 1.5149 - classification_loss: 0.1884\n",
            "\n",
            "Epoch 00030: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_30.h5\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 418s 5s/step - loss: 1.6169 - regression_loss: 1.4455 - classification_loss: 0.1714\n",
            "\n",
            "Epoch 00031: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_31.h5\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 408s 5s/step - loss: 1.6839 - regression_loss: 1.4978 - classification_loss: 0.1862\n",
            "\n",
            "Epoch 00032: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_32.h5\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 417s 5s/step - loss: 1.6313 - regression_loss: 1.4501 - classification_loss: 0.1812\n",
            "\n",
            "Epoch 00033: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_33.h5\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 429s 5s/step - loss: 1.6664 - regression_loss: 1.4810 - classification_loss: 0.1854\n",
            "\n",
            "Epoch 00034: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_34.h5\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 398s 5s/step - loss: 1.6638 - regression_loss: 1.4852 - classification_loss: 0.1786\n",
            "\n",
            "Epoch 00035: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_35.h5\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 411s 5s/step - loss: 1.6978 - regression_loss: 1.5105 - classification_loss: 0.1873\n",
            "\n",
            "Epoch 00036: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_36.h5\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 403s 5s/step - loss: 1.6284 - regression_loss: 1.4493 - classification_loss: 0.1791\n",
            "\n",
            "Epoch 00037: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_37.h5\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 394s 5s/step - loss: 1.6760 - regression_loss: 1.4914 - classification_loss: 0.1847\n",
            "\n",
            "Epoch 00038: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_38.h5\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 393s 5s/step - loss: 1.6998 - regression_loss: 1.5169 - classification_loss: 0.1829\n",
            "\n",
            "Epoch 00039: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_39.h5\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 403s 5s/step - loss: 1.6375 - regression_loss: 1.4570 - classification_loss: 0.1805\n",
            "\n",
            "Epoch 00040: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_40.h5\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 423s 5s/step - loss: 1.6733 - regression_loss: 1.4856 - classification_loss: 0.1877\n",
            "\n",
            "Epoch 00041: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_41.h5\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 435s 5s/step - loss: 1.6650 - regression_loss: 1.4819 - classification_loss: 0.1831\n",
            "\n",
            "Epoch 00042: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_42.h5\n",
            "\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 395s 5s/step - loss: 1.6745 - regression_loss: 1.4913 - classification_loss: 0.1832\n",
            "\n",
            "Epoch 00043: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_43.h5\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 384s 5s/step - loss: 1.6604 - regression_loss: 1.4782 - classification_loss: 0.1822\n",
            "\n",
            "Epoch 00044: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_44.h5\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 397s 5s/step - loss: 1.6392 - regression_loss: 1.4637 - classification_loss: 0.1755\n",
            "\n",
            "Epoch 00045: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_45.h5\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 408s 5s/step - loss: 1.7644 - regression_loss: 1.5703 - classification_loss: 0.1940\n",
            "\n",
            "Epoch 00046: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_46.h5\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 387s 5s/step - loss: 1.6947 - regression_loss: 1.5059 - classification_loss: 0.1887\n",
            "\n",
            "Epoch 00047: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_47.h5\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 446s 5s/step - loss: 1.6520 - regression_loss: 1.4707 - classification_loss: 0.1813\n",
            "\n",
            "Epoch 00048: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_48.h5\n",
            "Epoch 49/200\n",
            "85/85 [==============================] - 406s 5s/step - loss: 1.6615 - regression_loss: 1.4794 - classification_loss: 0.1821\n",
            "\n",
            "Epoch 00049: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_49.h5\n",
            "Epoch 50/200\n",
            "85/85 [==============================] - 408s 5s/step - loss: 1.7087 - regression_loss: 1.5203 - classification_loss: 0.1884\n",
            "\n",
            "Epoch 00050: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_50.h5\n",
            "\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
            "Epoch 51/200\n",
            "85/85 [==============================] - 375s 4s/step - loss: 1.7046 - regression_loss: 1.5136 - classification_loss: 0.1910\n",
            "\n",
            "Epoch 00051: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_51.h5\n",
            "Epoch 52/200\n",
            "85/85 [==============================] - 401s 5s/step - loss: 1.6701 - regression_loss: 1.4859 - classification_loss: 0.1842\n",
            "\n",
            "Epoch 00052: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_52.h5\n",
            "Epoch 53/200\n",
            "85/85 [==============================] - 447s 5s/step - loss: 1.6409 - regression_loss: 1.4575 - classification_loss: 0.1834\n",
            "\n",
            "Epoch 00053: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_53.h5\n",
            "Epoch 54/200\n",
            "85/85 [==============================] - 402s 5s/step - loss: 1.6633 - regression_loss: 1.4826 - classification_loss: 0.1807\n",
            "\n",
            "Epoch 00054: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_54.h5\n",
            "\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
            "Epoch 55/200\n",
            "85/85 [==============================] - 412s 5s/step - loss: 1.7348 - regression_loss: 1.5385 - classification_loss: 0.1963\n",
            "\n",
            "Epoch 00055: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_55.h5\n",
            "Epoch 56/200\n",
            "85/85 [==============================] - 422s 5s/step - loss: 1.6572 - regression_loss: 1.4748 - classification_loss: 0.1824\n",
            "\n",
            "Epoch 00056: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_56.h5\n",
            "Epoch 57/200\n",
            "85/85 [==============================] - 405s 5s/step - loss: 1.7068 - regression_loss: 1.5157 - classification_loss: 0.1911\n",
            "\n",
            "Epoch 00057: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_57.h5\n",
            "Epoch 58/200\n",
            "85/85 [==============================] - 391s 5s/step - loss: 1.6959 - regression_loss: 1.5106 - classification_loss: 0.1853\n",
            "\n",
            "Epoch 00058: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_58.h5\n",
            "\n",
            "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
            "Epoch 59/200\n",
            "85/85 [==============================] - 393s 5s/step - loss: 1.6624 - regression_loss: 1.4826 - classification_loss: 0.1798\n",
            "\n",
            "Epoch 00059: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_59.h5\n",
            "Epoch 60/200\n",
            "85/85 [==============================] - 387s 5s/step - loss: 1.6360 - regression_loss: 1.4607 - classification_loss: 0.1753\n",
            "\n",
            "Epoch 00060: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_60.h5\n",
            "Epoch 61/200\n",
            "85/85 [==============================] - 417s 5s/step - loss: 1.6941 - regression_loss: 1.5082 - classification_loss: 0.1860\n",
            "\n",
            "Epoch 00061: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_61.h5\n",
            "Epoch 62/200\n",
            "85/85 [==============================] - 379s 4s/step - loss: 1.6294 - regression_loss: 1.4495 - classification_loss: 0.1799\n",
            "\n",
            "Epoch 00062: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_62.h5\n",
            "\n",
            "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
            "Epoch 63/200\n",
            "85/85 [==============================] - 379s 4s/step - loss: 1.6416 - regression_loss: 1.4638 - classification_loss: 0.1778\n",
            "\n",
            "Epoch 00063: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_63.h5\n",
            "Epoch 64/200\n",
            "85/85 [==============================] - 397s 5s/step - loss: 1.6537 - regression_loss: 1.4705 - classification_loss: 0.1832\n",
            "\n",
            "Epoch 00064: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_64.h5\n",
            "Epoch 65/200\n",
            "85/85 [==============================] - 406s 5s/step - loss: 1.7028 - regression_loss: 1.5154 - classification_loss: 0.1875\n",
            "\n",
            "Epoch 00065: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_65.h5\n",
            "Epoch 66/200\n",
            "85/85 [==============================] - 383s 5s/step - loss: 1.6690 - regression_loss: 1.4876 - classification_loss: 0.1814\n",
            "\n",
            "Epoch 00066: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_66.h5\n",
            "\n",
            "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
            "Epoch 67/200\n",
            "85/85 [==============================] - 365s 4s/step - loss: 1.6239 - regression_loss: 1.4497 - classification_loss: 0.1743\n",
            "\n",
            "Epoch 00067: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_67.h5\n",
            "Epoch 68/200\n",
            "85/85 [==============================] - 386s 5s/step - loss: 1.7314 - regression_loss: 1.5397 - classification_loss: 0.1917\n",
            "\n",
            "Epoch 00068: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_68.h5\n",
            "Epoch 69/200\n",
            "85/85 [==============================] - 363s 4s/step - loss: 1.6600 - regression_loss: 1.4768 - classification_loss: 0.1832\n",
            "\n",
            "Epoch 00069: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_69.h5\n",
            "Epoch 70/200\n",
            "85/85 [==============================] - 389s 5s/step - loss: 1.7231 - regression_loss: 1.5277 - classification_loss: 0.1953\n",
            "\n",
            "Epoch 00070: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_70.h5\n",
            "\n",
            "Epoch 00070: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
            "Epoch 71/200\n",
            "85/85 [==============================] - 428s 5s/step - loss: 1.7396 - regression_loss: 1.5423 - classification_loss: 0.1973\n",
            "\n",
            "Epoch 00071: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_71.h5\n",
            "Epoch 72/200\n",
            "85/85 [==============================] - 396s 5s/step - loss: 1.6350 - regression_loss: 1.4592 - classification_loss: 0.1759\n",
            "\n",
            "Epoch 00072: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_72.h5\n",
            "Epoch 73/200\n",
            "85/85 [==============================] - 417s 5s/step - loss: 1.6233 - regression_loss: 1.4484 - classification_loss: 0.1748\n",
            "\n",
            "Epoch 00073: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_73.h5\n",
            "Epoch 74/200\n",
            "85/85 [==============================] - 382s 4s/step - loss: 1.6278 - regression_loss: 1.4491 - classification_loss: 0.1787\n",
            "\n",
            "Epoch 00074: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_74.h5\n",
            "\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
            "Epoch 75/200\n",
            "85/85 [==============================] - 382s 4s/step - loss: 1.6519 - regression_loss: 1.4729 - classification_loss: 0.1790\n",
            "\n",
            "Epoch 00075: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_75.h5\n",
            "Epoch 76/200\n",
            "85/85 [==============================] - 401s 5s/step - loss: 1.6751 - regression_loss: 1.4869 - classification_loss: 0.1882\n",
            "\n",
            "Epoch 00076: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_76.h5\n",
            "Epoch 77/200\n",
            "85/85 [==============================] - 427s 5s/step - loss: 1.7009 - regression_loss: 1.5099 - classification_loss: 0.1910\n",
            "\n",
            "Epoch 00077: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_77.h5\n",
            "Epoch 78/200\n",
            " 7/85 [=>............................] - ETA: 7:46 - loss: 1.5029 - regression_loss: 1.3368 - classification_loss: 0.1661"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8e106f12b5e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         callbacks=callbacks,) \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzA-_BG2FgAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_PS3COnMDdl",
        "colab_type": "text"
      },
      "source": [
        "## Training stopped due to connectivity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm9aqWmVxNUf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4af0b4f-37e9-47b1-d23f-191ecfc9d3de"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "85/85 [==============================] - 400s 5s/step - loss: 2.7623 - regression_loss: 2.2401 - classification_loss: 0.5222\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_01.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `mAP` which is not available. Available metrics are: loss,regression_loss,classification_loss,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/150\n",
            "85/85 [==============================] - 395s 5s/step - loss: 2.2713 - regression_loss: 1.9052 - classification_loss: 0.3661\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_02.h5\n",
            "Epoch 3/150\n",
            "85/85 [==============================] - 425s 5s/step - loss: 2.1796 - regression_loss: 1.8473 - classification_loss: 0.3323\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_03.h5\n",
            "Epoch 4/150\n",
            "85/85 [==============================] - 385s 5s/step - loss: 2.0571 - regression_loss: 1.7523 - classification_loss: 0.3049\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_04.h5\n",
            "Epoch 5/150\n",
            "85/85 [==============================] - 450s 5s/step - loss: 2.1008 - regression_loss: 1.7919 - classification_loss: 0.3089\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_05.h5\n",
            "Epoch 6/150\n",
            "85/85 [==============================] - 412s 5s/step - loss: 2.0588 - regression_loss: 1.7643 - classification_loss: 0.2945\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_06.h5\n",
            "Epoch 7/150\n",
            "85/85 [==============================] - 438s 5s/step - loss: 2.0342 - regression_loss: 1.7403 - classification_loss: 0.2939\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_07.h5\n",
            "Epoch 8/150\n",
            "85/85 [==============================] - 398s 5s/step - loss: 2.0126 - regression_loss: 1.7369 - classification_loss: 0.2757\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_08.h5\n",
            "Epoch 9/150\n",
            "85/85 [==============================] - 410s 5s/step - loss: 1.9682 - regression_loss: 1.6927 - classification_loss: 0.2756\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_09.h5\n",
            "Epoch 10/150\n",
            "85/85 [==============================] - 449s 5s/step - loss: 1.9673 - regression_loss: 1.6981 - classification_loss: 0.2692\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_10.h5\n",
            "Epoch 11/150\n",
            "85/85 [==============================] - 438s 5s/step - loss: 1.9902 - regression_loss: 1.7119 - classification_loss: 0.2783\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_11.h5\n",
            "Epoch 12/150\n",
            "85/85 [==============================] - 425s 5s/step - loss: 1.9363 - regression_loss: 1.6818 - classification_loss: 0.2545\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_12.h5\n",
            "Epoch 13/150\n",
            "85/85 [==============================] - 405s 5s/step - loss: 1.9973 - regression_loss: 1.7386 - classification_loss: 0.2587\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_13.h5\n",
            "Epoch 14/150\n",
            "85/85 [==============================] - 409s 5s/step - loss: 2.0067 - regression_loss: 1.7419 - classification_loss: 0.2649\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_14.h5\n",
            "Epoch 15/150\n",
            "85/85 [==============================] - 432s 5s/step - loss: 1.9845 - regression_loss: 1.7157 - classification_loss: 0.2688\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_15.h5\n",
            "Epoch 16/150\n",
            "85/85 [==============================] - 442s 5s/step - loss: 1.9228 - regression_loss: 1.6629 - classification_loss: 0.2598\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_16.h5\n",
            "Epoch 17/150\n",
            "85/85 [==============================] - 427s 5s/step - loss: 1.9177 - regression_loss: 1.6704 - classification_loss: 0.2473\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_17.h5\n",
            "Epoch 18/150\n",
            "85/85 [==============================] - 398s 5s/step - loss: 1.8383 - regression_loss: 1.6016 - classification_loss: 0.2368\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_18.h5\n",
            "Epoch 19/150\n",
            "85/85 [==============================] - 408s 5s/step - loss: 1.8971 - regression_loss: 1.6569 - classification_loss: 0.2403\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_19.h5\n",
            "Epoch 20/150\n",
            "85/85 [==============================] - 400s 5s/step - loss: 1.8936 - regression_loss: 1.6497 - classification_loss: 0.2439\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_20.h5\n",
            "Epoch 21/150\n",
            "85/85 [==============================] - 371s 4s/step - loss: 1.9124 - regression_loss: 1.6733 - classification_loss: 0.2391\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_21.h5\n",
            "Epoch 22/150\n",
            "85/85 [==============================] - 402s 5s/step - loss: 1.8917 - regression_loss: 1.6500 - classification_loss: 0.2416\n",
            "\n",
            "Epoch 00022: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_22.h5\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 23/150\n",
            "85/85 [==============================] - 403s 5s/step - loss: 1.7979 - regression_loss: 1.5826 - classification_loss: 0.2152\n",
            "\n",
            "Epoch 00023: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_23.h5\n",
            "Epoch 24/150\n",
            "85/85 [==============================] - 376s 4s/step - loss: 1.7483 - regression_loss: 1.5437 - classification_loss: 0.2046\n",
            "\n",
            "Epoch 00024: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_24.h5\n",
            "Epoch 25/150\n",
            "85/85 [==============================] - 375s 4s/step - loss: 1.7594 - regression_loss: 1.5578 - classification_loss: 0.2016\n",
            "\n",
            "Epoch 00025: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_25.h5\n",
            "Epoch 26/150\n",
            "85/85 [==============================] - 380s 4s/step - loss: 1.7169 - regression_loss: 1.5243 - classification_loss: 0.1926\n",
            "\n",
            "Epoch 00026: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_26.h5\n",
            "Epoch 27/150\n",
            "85/85 [==============================] - 374s 4s/step - loss: 1.7113 - regression_loss: 1.5166 - classification_loss: 0.1947\n",
            "\n",
            "Epoch 00027: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_27.h5\n",
            "Epoch 28/150\n",
            "85/85 [==============================] - 412s 5s/step - loss: 1.6916 - regression_loss: 1.4996 - classification_loss: 0.1920\n",
            "\n",
            "Epoch 00028: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_28.h5\n",
            "Epoch 29/150\n",
            "85/85 [==============================] - 403s 5s/step - loss: 1.6811 - regression_loss: 1.4881 - classification_loss: 0.1931\n",
            "\n",
            "Epoch 00029: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_29.h5\n",
            "Epoch 30/150\n",
            "85/85 [==============================] - 436s 5s/step - loss: 1.7292 - regression_loss: 1.5313 - classification_loss: 0.1978\n",
            "\n",
            "Epoch 00030: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_30.h5\n",
            "Epoch 31/150\n",
            "85/85 [==============================] - 414s 5s/step - loss: 1.6771 - regression_loss: 1.4916 - classification_loss: 0.1855\n",
            "\n",
            "Epoch 00031: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_31.h5\n",
            "Epoch 32/150\n",
            "85/85 [==============================] - 410s 5s/step - loss: 1.7297 - regression_loss: 1.5337 - classification_loss: 0.1960\n",
            "\n",
            "Epoch 00032: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_32.h5\n",
            "Epoch 33/150\n",
            "85/85 [==============================] - 415s 5s/step - loss: 1.6507 - regression_loss: 1.4617 - classification_loss: 0.1891\n",
            "\n",
            "Epoch 00033: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_33.h5\n",
            "Epoch 34/150\n",
            "85/85 [==============================] - 425s 5s/step - loss: 1.6871 - regression_loss: 1.5013 - classification_loss: 0.1857\n",
            "\n",
            "Epoch 00034: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_34.h5\n",
            "Epoch 35/150\n",
            "85/85 [==============================] - 399s 5s/step - loss: 1.7112 - regression_loss: 1.5211 - classification_loss: 0.1901\n",
            "\n",
            "Epoch 00035: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_35.h5\n",
            "Epoch 36/150\n",
            "85/85 [==============================] - 409s 5s/step - loss: 1.7203 - regression_loss: 1.5272 - classification_loss: 0.1930\n",
            "\n",
            "Epoch 00036: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_36.h5\n",
            "Epoch 37/150\n",
            "85/85 [==============================] - 389s 5s/step - loss: 1.6435 - regression_loss: 1.4584 - classification_loss: 0.1851\n",
            "\n",
            "Epoch 00037: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_37.h5\n",
            "Epoch 38/150\n",
            "85/85 [==============================] - 404s 5s/step - loss: 1.6842 - regression_loss: 1.4930 - classification_loss: 0.1913\n",
            "\n",
            "Epoch 00038: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_38.h5\n",
            "Epoch 39/150\n",
            "85/85 [==============================] - 416s 5s/step - loss: 1.7065 - regression_loss: 1.5153 - classification_loss: 0.1913\n",
            "\n",
            "Epoch 00039: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_39.h5\n",
            "Epoch 40/150\n",
            "85/85 [==============================] - 429s 5s/step - loss: 1.6477 - regression_loss: 1.4640 - classification_loss: 0.1837\n",
            "\n",
            "Epoch 00040: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_40.h5\n",
            "Epoch 41/150\n",
            "85/85 [==============================] - 451s 5s/step - loss: 1.6804 - regression_loss: 1.4912 - classification_loss: 0.1891\n",
            "\n",
            "Epoch 00041: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_41.h5\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 42/150\n",
            "85/85 [==============================] - 462s 5s/step - loss: 1.6763 - regression_loss: 1.4865 - classification_loss: 0.1898\n",
            "\n",
            "Epoch 00042: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_42.h5\n",
            "Epoch 43/150\n",
            "85/85 [==============================] - 417s 5s/step - loss: 1.6867 - regression_loss: 1.4980 - classification_loss: 0.1886\n",
            "\n",
            "Epoch 00043: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_43.h5\n",
            "Epoch 44/150\n",
            "85/85 [==============================] - 402s 5s/step - loss: 1.6489 - regression_loss: 1.4665 - classification_loss: 0.1823\n",
            "\n",
            "Epoch 00044: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_44.h5\n",
            "Epoch 45/150\n",
            "85/85 [==============================] - 415s 5s/step - loss: 1.6509 - regression_loss: 1.4690 - classification_loss: 0.1819\n",
            "\n",
            "Epoch 00045: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_45.h5\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 46/150\n",
            "85/85 [==============================] - 425s 5s/step - loss: 1.7370 - regression_loss: 1.5411 - classification_loss: 0.1959\n",
            "\n",
            "Epoch 00046: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_46.h5\n",
            "Epoch 47/150\n",
            "85/85 [==============================] - 398s 5s/step - loss: 1.7043 - regression_loss: 1.5112 - classification_loss: 0.1932\n",
            "\n",
            "Epoch 00047: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_47.h5\n",
            "Epoch 48/150\n",
            "85/85 [==============================] - 461s 5s/step - loss: 1.6297 - regression_loss: 1.4493 - classification_loss: 0.1804\n",
            "\n",
            "Epoch 00048: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_48.h5\n",
            "Epoch 49/150\n",
            "85/85 [==============================] - 412s 5s/step - loss: 1.6469 - regression_loss: 1.4631 - classification_loss: 0.1838\n",
            "\n",
            "Epoch 00049: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_49.h5\n",
            "Epoch 50/150\n",
            "85/85 [==============================] - 410s 5s/step - loss: 1.6973 - regression_loss: 1.5082 - classification_loss: 0.1891\n",
            "\n",
            "Epoch 00050: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_50.h5\n",
            "Epoch 51/150\n",
            "85/85 [==============================] - 380s 4s/step - loss: 1.6834 - regression_loss: 1.4985 - classification_loss: 0.1849\n",
            "\n",
            "Epoch 00051: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_51.h5\n",
            "Epoch 52/150\n",
            "85/85 [==============================] - 413s 5s/step - loss: 1.6819 - regression_loss: 1.4929 - classification_loss: 0.1890\n",
            "\n",
            "Epoch 00052: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_52.h5\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "Epoch 53/150\n",
            "85/85 [==============================] - 451s 5s/step - loss: 1.6397 - regression_loss: 1.4590 - classification_loss: 0.1808\n",
            "\n",
            "Epoch 00053: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_53.h5\n",
            "Epoch 54/150\n",
            "85/85 [==============================] - 399s 5s/step - loss: 1.6558 - regression_loss: 1.4775 - classification_loss: 0.1783\n",
            "\n",
            "Epoch 00054: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_54.h5\n",
            "Epoch 55/150\n",
            "85/85 [==============================] - 408s 5s/step - loss: 1.7195 - regression_loss: 1.5229 - classification_loss: 0.1966\n",
            "\n",
            "Epoch 00055: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_55.h5\n",
            "Epoch 56/150\n",
            "11/85 [==>...........................] - ETA: 5:58 - loss: 1.5936 - regression_loss: 1.4166 - classification_loss: 0.1769"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP-iMKggxOaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwgMflVpxOTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWZiVrkjxOLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzIHcNo-xOEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkZWL8yfxN8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R6QDrKuxNua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3G42f5jjzJl",
        "colab_type": "text"
      },
      "source": [
        "#Logs for training\n",
        "- Changing the Iou threshold = 0.5\n",
        "- NMS =0.5\n",
        "- score threshold = 0.1\n",
        "- Img Max side increased to 1400\n",
        "- Ratios changed to 1 1.5 2 2.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aGf301bkR-L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71a8733b-b783-407a-a401-6f9ccae6995d"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "85/85 [==============================] - 332s 4s/step - loss: 1.9143 - regression_loss: 1.7073 - classification_loss: 0.2070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:25 Time:  0:00:25\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9994\n",
            "57 instances of class fin_1 with average precision: 0.9915\n",
            "57 instances of class fin_2 with average precision: 0.9988\n",
            "57 instances of class fin_3 with average precision: 0.9850\n",
            "57 instances of class fin_4 with average precision: 0.9691\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9906\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_01.h5\n",
            "Epoch 2/150\n",
            "85/85 [==============================] - 302s 4s/step - loss: 1.8569 - regression_loss: 1.6600 - classification_loss: 0.1969\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 1.0000\n",
            "57 instances of class fin_1 with average precision: 0.9994\n",
            "57 instances of class fin_2 with average precision: 1.0000\n",
            "57 instances of class fin_3 with average precision: 0.9953\n",
            "57 instances of class fin_4 with average precision: 0.9934\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9980\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_02.h5\n",
            "Epoch 3/150\n",
            "85/85 [==============================] - 351s 4s/step - loss: 1.9319 - regression_loss: 1.7114 - classification_loss: 0.2204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9985\n",
            "57 instances of class fin_1 with average precision: 0.9941\n",
            "57 instances of class fin_2 with average precision: 1.0000\n",
            "57 instances of class fin_3 with average precision: 0.9962\n",
            "57 instances of class fin_4 with average precision: 0.9925\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9969\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_03.h5\n",
            "Epoch 4/150\n",
            "85/85 [==============================] - 309s 4s/step - loss: 1.8430 - regression_loss: 1.6441 - classification_loss: 0.1989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:08 Time:  0:00:08\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9985\n",
            "57 instances of class fin_1 with average precision: 0.9849\n",
            "57 instances of class fin_2 with average precision: 0.9951\n",
            "57 instances of class fin_3 with average precision: 0.9712\n",
            "57 instances of class fin_4 with average precision: 0.9741\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9873\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_04.h5\n",
            "Epoch 5/150\n",
            "85/85 [==============================] - 366s 4s/step - loss: 1.9322 - regression_loss: 1.7171 - classification_loss: 0.2151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9991\n",
            "57 instances of class fin_1 with average precision: 0.9985\n",
            "57 instances of class fin_2 with average precision: 1.0000\n",
            "57 instances of class fin_3 with average precision: 0.9906\n",
            "57 instances of class fin_4 with average precision: 0.9700\n",
            "57 instances of class wrist with average precision: 0.9977\n",
            "mAP: 0.9927\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_05.h5\n",
            "Epoch 6/150\n",
            "85/85 [==============================] - 326s 4s/step - loss: 1.8808 - regression_loss: 1.6777 - classification_loss: 0.2032\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 1.0000\n",
            "57 instances of class fin_1 with average precision: 1.0000\n",
            "57 instances of class fin_2 with average precision: 0.9964\n",
            "57 instances of class fin_3 with average precision: 0.9965\n",
            "57 instances of class fin_4 with average precision: 0.9929\n",
            "57 instances of class wrist with average precision: 0.9697\n",
            "mAP: 0.9926\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_06.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f8c781da940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVifc6Tyzrma",
        "colab_type": "text"
      },
      "source": [
        "# New logs for training\n",
        "- Chagning back the ratios and scale to normal if class loss doesnt decrease (Changed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4TFciz2zq9S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d43ea915-11d1-48ec-ac24-f3294564117a"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "85/85 [==============================] - 418s 5s/step - loss: 2.7223 - regression_loss: 2.2535 - classification_loss: 0.4688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:57 Time:  0:00:57\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9549\n",
            "57 instances of class fin_1 with average precision: 0.9498\n",
            "57 instances of class fin_2 with average precision: 0.9214\n",
            "57 instances of class fin_3 with average precision: 0.9046\n",
            "57 instances of class fin_4 with average precision: 0.8363\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9278\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_01.h5\n",
            "Epoch 2/150\n",
            "85/85 [==============================] - 329s 4s/step - loss: 2.4997 - regression_loss: 2.1512 - classification_loss: 0.3485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.8417\n",
            "57 instances of class fin_1 with average precision: 0.9633\n",
            "57 instances of class fin_2 with average precision: 0.9486\n",
            "57 instances of class fin_3 with average precision: 0.9077\n",
            "57 instances of class fin_4 with average precision: 0.8074\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9115\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_02.h5\n",
            "Epoch 3/150\n",
            "85/85 [==============================] - 339s 4s/step - loss: 2.5156 - regression_loss: 2.1573 - classification_loss: 0.3584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:21 Time:  0:00:21\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9630\n",
            "57 instances of class fin_1 with average precision: 0.9320\n",
            "57 instances of class fin_2 with average precision: 0.9349\n",
            "57 instances of class fin_3 with average precision: 0.9293\n",
            "57 instances of class fin_4 with average precision: 0.8979\n",
            "57 instances of class wrist with average precision: 0.9997\n",
            "mAP: 0.9428\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_03.h5\n",
            "Epoch 4/150\n",
            "85/85 [==============================] - 324s 4s/step - loss: 2.5228 - regression_loss: 2.1774 - classification_loss: 0.3454\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9605\n",
            "57 instances of class fin_1 with average precision: 0.9535\n",
            "57 instances of class fin_2 with average precision: 0.9653\n",
            "57 instances of class fin_3 with average precision: 0.9079\n",
            "57 instances of class fin_4 with average precision: 0.8811\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9447\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_04.h5\n",
            "Epoch 5/150\n",
            "85/85 [==============================] - 354s 4s/step - loss: 2.4778 - regression_loss: 2.1368 - classification_loss: 0.3409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9538\n",
            "57 instances of class fin_1 with average precision: 0.9700\n",
            "57 instances of class fin_2 with average precision: 0.9841\n",
            "57 instances of class fin_3 with average precision: 0.9443\n",
            "57 instances of class fin_4 with average precision: 0.9102\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9604\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_05.h5\n",
            "Epoch 6/150\n",
            "85/85 [==============================] - 375s 4s/step - loss: 2.4976 - regression_loss: 2.1623 - classification_loss: 0.3353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9481\n",
            "57 instances of class fin_1 with average precision: 0.9156\n",
            "57 instances of class fin_2 with average precision: 0.9160\n",
            "57 instances of class fin_3 with average precision: 0.8847\n",
            "57 instances of class fin_4 with average precision: 0.8948\n",
            "57 instances of class wrist with average precision: 0.9838\n",
            "mAP: 0.9238\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_06.h5\n",
            "Epoch 7/150\n",
            "85/85 [==============================] - 342s 4s/step - loss: 2.5027 - regression_loss: 2.1685 - classification_loss: 0.3342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:21 Time:  0:00:21\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9755\n",
            "57 instances of class fin_1 with average precision: 0.9579\n",
            "57 instances of class fin_2 with average precision: 0.9410\n",
            "57 instances of class fin_3 with average precision: 0.9204\n",
            "57 instances of class fin_4 with average precision: 0.9482\n",
            "57 instances of class wrist with average precision: 0.9903\n",
            "mAP: 0.9555\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_07.h5\n",
            "Epoch 8/150\n",
            "85/85 [==============================] - 360s 4s/step - loss: 2.4092 - regression_loss: 2.0896 - classification_loss: 0.3196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:21 Time:  0:00:21\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9734\n",
            "57 instances of class fin_1 with average precision: 0.9726\n",
            "57 instances of class fin_2 with average precision: 0.9881\n",
            "57 instances of class fin_3 with average precision: 0.9384\n",
            "57 instances of class fin_4 with average precision: 0.9574\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9716\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_08.h5\n",
            "Epoch 9/150\n",
            "85/85 [==============================] - 345s 4s/step - loss: 2.4928 - regression_loss: 2.1568 - classification_loss: 0.3360\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9565\n",
            "57 instances of class fin_1 with average precision: 0.9544\n",
            "57 instances of class fin_2 with average precision: 0.9737\n",
            "57 instances of class fin_3 with average precision: 0.9369\n",
            "57 instances of class fin_4 with average precision: 0.9402\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9603\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_09.h5\n",
            "Epoch 10/150\n",
            "85/85 [==============================] - 355s 4s/step - loss: 2.5467 - regression_loss: 2.2036 - classification_loss: 0.3431\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.7490\n",
            "57 instances of class fin_1 with average precision: 0.9650\n",
            "57 instances of class fin_2 with average precision: 0.9562\n",
            "57 instances of class fin_3 with average precision: 0.9321\n",
            "57 instances of class fin_4 with average precision: 0.8944\n",
            "57 instances of class wrist with average precision: 0.9741\n",
            "mAP: 0.9118\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_10.h5\n",
            "Epoch 11/150\n",
            "85/85 [==============================] - 351s 4s/step - loss: 2.4768 - regression_loss: 2.1538 - classification_loss: 0.3230\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9462\n",
            "57 instances of class fin_1 with average precision: 0.9036\n",
            "57 instances of class fin_2 with average precision: 0.8996\n",
            "57 instances of class fin_3 with average precision: 0.8992\n",
            "57 instances of class fin_4 with average precision: 0.8350\n",
            "57 instances of class wrist with average precision: 0.9906\n",
            "mAP: 0.9124\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_11.h5\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 12/150\n",
            "85/85 [==============================] - 388s 5s/step - loss: 2.4355 - regression_loss: 2.1196 - classification_loss: 0.3159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9859\n",
            "57 instances of class fin_1 with average precision: 0.9794\n",
            "57 instances of class fin_2 with average precision: 0.9832\n",
            "57 instances of class fin_3 with average precision: 0.9714\n",
            "57 instances of class fin_4 with average precision: 0.9766\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9828\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_12.h5\n",
            "Epoch 13/150\n",
            "85/85 [==============================] - 361s 4s/step - loss: 2.4038 - regression_loss: 2.0997 - classification_loss: 0.3041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9922\n",
            "57 instances of class fin_1 with average precision: 0.9787\n",
            "57 instances of class fin_2 with average precision: 0.9845\n",
            "57 instances of class fin_3 with average precision: 0.9700\n",
            "57 instances of class fin_4 with average precision: 0.9507\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9793\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_13.h5\n",
            "Epoch 14/150\n",
            "85/85 [==============================] - 350s 4s/step - loss: 2.3487 - regression_loss: 2.0573 - classification_loss: 0.2914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9925\n",
            "57 instances of class fin_1 with average precision: 0.9857\n",
            "57 instances of class fin_2 with average precision: 0.9892\n",
            "57 instances of class fin_3 with average precision: 0.9727\n",
            "57 instances of class fin_4 with average precision: 0.9455\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9809\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_14.h5\n",
            "Epoch 15/150\n",
            "85/85 [==============================] - 357s 4s/step - loss: 2.3191 - regression_loss: 2.0338 - classification_loss: 0.2852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:21 Time:  0:00:21\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9866\n",
            "57 instances of class fin_1 with average precision: 0.9839\n",
            "57 instances of class fin_2 with average precision: 0.9916\n",
            "57 instances of class fin_3 with average precision: 0.9783\n",
            "57 instances of class fin_4 with average precision: 0.9688\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9849\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_15.h5\n",
            "Epoch 16/150\n",
            "85/85 [==============================] - 352s 4s/step - loss: 2.3192 - regression_loss: 2.0328 - classification_loss: 0.2864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9912\n",
            "57 instances of class fin_1 with average precision: 0.9860\n",
            "57 instances of class fin_2 with average precision: 0.9932\n",
            "57 instances of class fin_3 with average precision: 0.9764\n",
            "57 instances of class fin_4 with average precision: 0.9712\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9863\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_16.h5\n",
            "Epoch 17/150\n",
            "85/85 [==============================] - 368s 4s/step - loss: 2.3050 - regression_loss: 2.0174 - classification_loss: 0.2876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:22 Time:  0:00:22\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9972\n",
            "57 instances of class fin_1 with average precision: 0.9841\n",
            "57 instances of class fin_2 with average precision: 0.9938\n",
            "57 instances of class fin_3 with average precision: 0.9781\n",
            "57 instances of class fin_4 with average precision: 0.9602\n",
            "57 instances of class wrist with average precision: 0.9985\n",
            "mAP: 0.9853\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_17.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb603ada8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrwRMXu7gSfZ",
        "colab_type": "text"
      },
      "source": [
        "## New logs for some changes mentioned below\n",
        "- Trained with best model from Version 3\n",
        "- Changed the Iou threshold = 0.6, NMS thresh = 0.55, score thresh = 0.1\n",
        "- Ratios to 1 - 3 with a gap of 0.2\n",
        "- Freeze backbone=False\n",
        "- Chnages in some augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5g5Qg_NgSKB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9894d0f6-ad7a-4cab-8337-91ee552aec09"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "85/85 [==============================] - 420s 5s/step - loss: 3.2111 - regression_loss: 2.4692 - classification_loss: 0.7419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:24 Time:  0:00:24\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.6118\n",
            "57 instances of class fin_1 with average precision: 0.5126\n",
            "57 instances of class fin_2 with average precision: 0.6828\n",
            "57 instances of class fin_3 with average precision: 0.3896\n",
            "57 instances of class fin_4 with average precision: 0.2241\n",
            "57 instances of class wrist with average precision: 0.9323\n",
            "mAP: 0.5588\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_01.h5\n",
            "Epoch 2/150\n",
            "85/85 [==============================] - 328s 4s/step - loss: 2.8417 - regression_loss: 2.3148 - classification_loss: 0.5268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:10 Time:  0:00:10\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.1175\n",
            "57 instances of class fin_1 with average precision: 0.2088\n",
            "57 instances of class fin_2 with average precision: 0.2564\n",
            "57 instances of class fin_3 with average precision: 0.1408\n",
            "57 instances of class fin_4 with average precision: 0.0930\n",
            "57 instances of class wrist with average precision: 0.5130\n",
            "mAP: 0.2216\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_02.h5\n",
            "Epoch 3/150\n",
            "85/85 [==============================] - 323s 4s/step - loss: 2.9127 - regression_loss: 2.3035 - classification_loss: 0.6092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:08 Time:  0:00:08\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.7249\n",
            "57 instances of class fin_1 with average precision: 0.9296\n",
            "57 instances of class fin_2 with average precision: 0.8601\n",
            "57 instances of class fin_3 with average precision: 0.2980\n",
            "57 instances of class fin_4 with average precision: 0.5406\n",
            "57 instances of class wrist with average precision: 0.9747\n",
            "mAP: 0.7213\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_03.h5\n",
            "Epoch 4/150\n",
            "85/85 [==============================] - 315s 4s/step - loss: 5.3611 - regression_loss: 2.3530 - classification_loss: 3.0082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.0021\n",
            "57 instances of class fin_1 with average precision: 0.0001\n",
            "57 instances of class fin_2 with average precision: 0.0000\n",
            "57 instances of class fin_3 with average precision: 0.0000\n",
            "57 instances of class fin_4 with average precision: 0.0000\n",
            "57 instances of class wrist with average precision: 0.0000\n",
            "mAP: 0.0004\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_04.h5\n",
            "Epoch 5/150\n",
            "85/85 [==============================] - 346s 4s/step - loss: 4.6611 - regression_loss: 2.4714 - classification_loss: 2.1897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.0499\n",
            "57 instances of class fin_1 with average precision: 0.0643\n",
            "57 instances of class fin_2 with average precision: 0.0482\n",
            "57 instances of class fin_3 with average precision: 0.0581\n",
            "57 instances of class fin_4 with average precision: 0.0521\n",
            "57 instances of class wrist with average precision: 0.2502\n",
            "mAP: 0.0871\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_05.h5\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 6/150\n",
            "85/85 [==============================] - 361s 4s/step - loss: 2.6850 - regression_loss: 2.2524 - classification_loss: 0.4327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.2448\n",
            "57 instances of class fin_1 with average precision: 0.7210\n",
            "57 instances of class fin_2 with average precision: 0.8021\n",
            "57 instances of class fin_3 with average precision: 0.4460\n",
            "57 instances of class fin_4 with average precision: 0.4176\n",
            "57 instances of class wrist with average precision: 0.9555\n",
            "mAP: 0.5978\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_06.h5\n",
            "Epoch 7/150\n",
            "85/85 [==============================] - 326s 4s/step - loss: 2.5691 - regression_loss: 2.1686 - classification_loss: 0.4005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.6738\n",
            "57 instances of class fin_1 with average precision: 0.9258\n",
            "57 instances of class fin_2 with average precision: 0.8985\n",
            "57 instances of class fin_3 with average precision: 0.6571\n",
            "57 instances of class fin_4 with average precision: 0.7329\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.8147\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_07.h5\n",
            "Epoch 8/150\n",
            "85/85 [==============================] - 354s 4s/step - loss: 2.4757 - regression_loss: 2.0983 - classification_loss: 0.3774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.7385\n",
            "57 instances of class fin_1 with average precision: 0.9410\n",
            "57 instances of class fin_2 with average precision: 0.9463\n",
            "57 instances of class fin_3 with average precision: 0.8788\n",
            "57 instances of class fin_4 with average precision: 0.8142\n",
            "57 instances of class wrist with average precision: 0.9961\n",
            "mAP: 0.8858\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_08.h5\n",
            "Epoch 9/150\n",
            "85/85 [==============================] - 345s 4s/step - loss: 2.5153 - regression_loss: 2.1309 - classification_loss: 0.3844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.8309\n",
            "57 instances of class fin_1 with average precision: 0.9294\n",
            "57 instances of class fin_2 with average precision: 0.9419\n",
            "57 instances of class fin_3 with average precision: 0.8468\n",
            "57 instances of class fin_4 with average precision: 0.8596\n",
            "57 instances of class wrist with average precision: 0.9930\n",
            "mAP: 0.9003\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_09.h5\n",
            "Epoch 10/150\n",
            "85/85 [==============================] - 357s 4s/step - loss: 2.5319 - regression_loss: 2.1526 - classification_loss: 0.3793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.8257\n",
            "57 instances of class fin_1 with average precision: 0.9118\n",
            "57 instances of class fin_2 with average precision: 0.9608\n",
            "57 instances of class fin_3 with average precision: 0.8879\n",
            "57 instances of class fin_4 with average precision: 0.8960\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9137\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_10.h5\n",
            "Epoch 11/150\n",
            "85/85 [==============================] - 342s 4s/step - loss: 2.4229 - regression_loss: 2.0582 - classification_loss: 0.3647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.8753\n",
            "57 instances of class fin_1 with average precision: 0.8837\n",
            "57 instances of class fin_2 with average precision: 0.9480\n",
            "57 instances of class fin_3 with average precision: 0.9389\n",
            "57 instances of class fin_4 with average precision: 0.9011\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9245\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_11.h5\n",
            "Epoch 12/150\n",
            "85/85 [==============================] - 390s 5s/step - loss: 2.4331 - regression_loss: 2.0662 - classification_loss: 0.3669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.8867\n",
            "57 instances of class fin_1 with average precision: 0.9084\n",
            "57 instances of class fin_2 with average precision: 0.9819\n",
            "57 instances of class fin_3 with average precision: 0.9722\n",
            "57 instances of class fin_4 with average precision: 0.9184\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9446\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_12.h5\n",
            "Epoch 13/150\n",
            "85/85 [==============================] - 372s 4s/step - loss: 2.4501 - regression_loss: 2.0870 - classification_loss: 0.3630\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.8835\n",
            "57 instances of class fin_1 with average precision: 0.9202\n",
            "57 instances of class fin_2 with average precision: 0.9749\n",
            "57 instances of class fin_3 with average precision: 0.9591\n",
            "57 instances of class fin_4 with average precision: 0.9339\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9453\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_13.h5\n",
            "Epoch 14/150\n",
            "85/85 [==============================] - 354s 4s/step - loss: 2.3964 - regression_loss: 2.0477 - classification_loss: 0.3487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9316\n",
            "57 instances of class fin_1 with average precision: 0.9390\n",
            "57 instances of class fin_2 with average precision: 0.9891\n",
            "57 instances of class fin_3 with average precision: 0.9557\n",
            "57 instances of class fin_4 with average precision: 0.9255\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9568\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_14.h5\n",
            "Epoch 15/150\n",
            "85/85 [==============================] - 367s 4s/step - loss: 2.3656 - regression_loss: 2.0205 - classification_loss: 0.3452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9163\n",
            "57 instances of class fin_1 with average precision: 0.9262\n",
            "57 instances of class fin_2 with average precision: 0.9895\n",
            "57 instances of class fin_3 with average precision: 0.9640\n",
            "57 instances of class fin_4 with average precision: 0.9427\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9564\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_15.h5\n",
            "Epoch 16/150\n",
            "85/85 [==============================] - 365s 4s/step - loss: 2.3668 - regression_loss: 2.0211 - classification_loss: 0.3457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:10 Time:  0:00:10\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9267\n",
            "57 instances of class fin_1 with average precision: 0.9271\n",
            "57 instances of class fin_2 with average precision: 0.9861\n",
            "57 instances of class fin_3 with average precision: 0.9801\n",
            "57 instances of class fin_4 with average precision: 0.9599\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9633\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_16.h5\n",
            "Epoch 17/150\n",
            "85/85 [==============================] - 386s 5s/step - loss: 2.3318 - regression_loss: 1.9952 - classification_loss: 0.3366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:10 Time:  0:00:10\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9489\n",
            "57 instances of class fin_1 with average precision: 0.9418\n",
            "57 instances of class fin_2 with average precision: 0.9994\n",
            "57 instances of class fin_3 with average precision: 0.9711\n",
            "57 instances of class fin_4 with average precision: 0.9561\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9695\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_17.h5\n",
            "Epoch 18/150\n",
            "85/85 [==============================] - 366s 4s/step - loss: 2.3905 - regression_loss: 2.0421 - classification_loss: 0.3484\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9455\n",
            "57 instances of class fin_1 with average precision: 0.9404\n",
            "57 instances of class fin_2 with average precision: 0.9933\n",
            "57 instances of class fin_3 with average precision: 0.9764\n",
            "57 instances of class fin_4 with average precision: 0.9589\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9691\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_18.h5\n",
            "Epoch 19/150\n",
            "85/85 [==============================] - 425s 5s/step - loss: 2.3153 - regression_loss: 1.9828 - classification_loss: 0.3325\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9430\n",
            "57 instances of class fin_1 with average precision: 0.9361\n",
            "57 instances of class fin_2 with average precision: 0.9914\n",
            "57 instances of class fin_3 with average precision: 0.9707\n",
            "57 instances of class fin_4 with average precision: 0.9339\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9625\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_19.h5\n",
            "Epoch 20/150\n",
            "85/85 [==============================] - 361s 4s/step - loss: 2.3514 - regression_loss: 2.0135 - classification_loss: 0.3379\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9515\n",
            "57 instances of class fin_1 with average precision: 0.9291\n",
            "57 instances of class fin_2 with average precision: 0.9962\n",
            "57 instances of class fin_3 with average precision: 0.9680\n",
            "57 instances of class fin_4 with average precision: 0.9427\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9646\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_20.h5\n",
            "Epoch 21/150\n",
            "85/85 [==============================] - 366s 4s/step - loss: 2.3311 - regression_loss: 1.9974 - classification_loss: 0.3337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9533\n",
            "57 instances of class fin_1 with average precision: 0.9490\n",
            "57 instances of class fin_2 with average precision: 0.9944\n",
            "57 instances of class fin_3 with average precision: 0.9782\n",
            "57 instances of class fin_4 with average precision: 0.9278\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9671\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_21.h5\n",
            "Epoch 22/150\n",
            "85/85 [==============================] - 355s 4s/step - loss: 2.2940 - regression_loss: 1.9640 - classification_loss: 0.3301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Running network: 100% (57 of 57) |#######| Elapsed Time: 0:00:09 Time:  0:00:09\n",
            "Parsing annotations: 100% (57 of 57) |###| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57 instances of class fin_ip with average precision: 0.9269\n",
            "57 instances of class fin_1 with average precision: 0.9378\n",
            "57 instances of class fin_2 with average precision: 0.9913\n",
            "57 instances of class fin_3 with average precision: 0.9627\n",
            "57 instances of class fin_4 with average precision: 0.9494\n",
            "57 instances of class wrist with average precision: 1.0000\n",
            "mAP: 0.9614\n",
            "\n",
            "Epoch 00022: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_22.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fd956f831d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzY8eZV5gSBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF2ElO2zgR4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpKkYz3m6bIM",
        "colab_type": "text"
      },
      "source": [
        "## Logs for some changes -> \n",
        "- Freeze Backbone =False,  (Loaded previous best model) \n",
        "- Ratio increased from previous to 1, 1.2, 1.5, 2, 2.5, 3 - > 1 1.2 1.5 1.8 2 2.5 2.8 3\n",
        "- Image _min side changed from 1000 -> 1000 and Max side remains the same 1200 - Adding these augmentations as necessary \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4rjur1k6agr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "ad7dee93-1648-426e-fa38-e4884a3c6bb2"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "47/47 [==============================] - 370s 8s/step - loss: 3.8632 - regression_loss: 2.5618 - classification_loss: 1.3014\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_01.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `mAP` which is not available. Available metrics are: loss,regression_loss,classification_loss,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/150\n",
            "16/47 [=========>....................] - ETA: 2:35 - loss: 3.0826 - regression_loss: 2.4846 - classification_loss: 0.5980"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8e106f12b5e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         callbacks=callbacks,) \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45paAKdCgt2l",
        "colab_type": "text"
      },
      "source": [
        "## Logs for -> \n",
        "- Free backbone = False, \n",
        "- Ratio increased from previous to 1, 1.2, 1.5, 2, 2.5, 3 - > 1 1.2 1.5 1.8 2 2.5 2.8 3 \n",
        "- Image _min side changed from 1000 -> 800 and Max side remains the same 1200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6JoRuLcgSpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db4cadc4-1ddc-454e-a25c-19557f8f3b21"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "47/47 [==============================] - 324s 7s/step - loss: 3.2019 - regression_loss: 2.5069 - classification_loss: 0.6950\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_01.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `mAP` which is not available. Available metrics are: loss,regression_loss,classification_loss,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/150\n",
            "47/47 [==============================] - 337s 7s/step - loss: 2.9032 - regression_loss: 2.3767 - classification_loss: 0.5265\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_02.h5\n",
            "Epoch 3/150\n",
            "47/47 [==============================] - 347s 7s/step - loss: 2.8350 - regression_loss: 2.3292 - classification_loss: 0.5058\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_03.h5\n",
            "Epoch 4/150\n",
            "47/47 [==============================] - 318s 7s/step - loss: 2.8103 - regression_loss: 2.2674 - classification_loss: 0.5429\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_04.h5\n",
            "Epoch 5/150\n",
            "47/47 [==============================] - 304s 6s/step - loss: 2.8447 - regression_loss: 2.3410 - classification_loss: 0.5037\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_05.h5\n",
            "Epoch 6/150\n",
            "47/47 [==============================] - 308s 7s/step - loss: 2.7908 - regression_loss: 2.3014 - classification_loss: 0.4894\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_06.h5\n",
            "Epoch 7/150\n",
            "47/47 [==============================] - 314s 7s/step - loss: 2.7358 - regression_loss: 2.2658 - classification_loss: 0.4700\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_07.h5\n",
            "Epoch 8/150\n",
            "47/47 [==============================] - 293s 6s/step - loss: 2.7003 - regression_loss: 2.2313 - classification_loss: 0.4691\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_08.h5\n",
            "Epoch 9/150\n",
            "47/47 [==============================] - 324s 7s/step - loss: 2.7247 - regression_loss: 2.2589 - classification_loss: 0.4657\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_09.h5\n",
            "Epoch 10/150\n",
            "47/47 [==============================] - 342s 7s/step - loss: 2.6635 - regression_loss: 2.2206 - classification_loss: 0.4430\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_10.h5\n",
            "Epoch 11/150\n",
            "47/47 [==============================] - 325s 7s/step - loss: 2.6879 - regression_loss: 2.2473 - classification_loss: 0.4406\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_11.h5\n",
            "Epoch 12/150\n",
            "47/47 [==============================] - 320s 7s/step - loss: 2.7844 - regression_loss: 2.2938 - classification_loss: 0.4905\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_12.h5\n",
            "Epoch 13/150\n",
            "47/47 [==============================] - 312s 7s/step - loss: 2.6608 - regression_loss: 2.2207 - classification_loss: 0.4401\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_13.h5\n",
            "Epoch 14/150\n",
            "47/47 [==============================] - 286s 6s/step - loss: 2.6565 - regression_loss: 2.2209 - classification_loss: 0.4356\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_14.h5\n",
            "Epoch 15/150\n",
            "47/47 [==============================] - 313s 7s/step - loss: 2.6380 - regression_loss: 2.2139 - classification_loss: 0.4241\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_15.h5\n",
            "Epoch 16/150\n",
            "47/47 [==============================] - 319s 7s/step - loss: 2.6699 - regression_loss: 2.2428 - classification_loss: 0.4270\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_16.h5\n",
            "Epoch 17/150\n",
            "47/47 [==============================] - 305s 6s/step - loss: 2.6108 - regression_loss: 2.1802 - classification_loss: 0.4307\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_17.h5\n",
            "Epoch 18/150\n",
            "47/47 [==============================] - 317s 7s/step - loss: 2.6709 - regression_loss: 2.2339 - classification_loss: 0.4370\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_18.h5\n",
            "Epoch 19/150\n",
            " 7/47 [===>..........................] - ETA: 4:14 - loss: 2.7566 - regression_loss: 2.3099 - classification_loss: 0.4468"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-8e106f12b5e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         callbacks=callbacks,) \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQQxkH8FgCtB",
        "colab_type": "text"
      },
      "source": [
        "# Logs for Some changes i.e IOU = 0.6, Freeze backbone = False, N_sizes for anchors = 64, 128, 256, 1024 and scales changed to 1, 1.2, 1.6, 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prd_5aWuHlip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a7ca830-3389-4eaf-d9ce-323755e49cf9"
      },
      "source": [
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "70/70 [==============================] - 382s 5s/step - loss: 3.2361 - regression_loss: 2.5124 - classification_loss: 0.7237\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_01.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `mAP` which is not available. Available metrics are: loss,regression_loss,classification_loss,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/150\n",
            "70/70 [==============================] - 272s 4s/step - loss: 3.0100 - regression_loss: 2.4488 - classification_loss: 0.5612\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_02.h5\n",
            "Epoch 3/150\n",
            "70/70 [==============================] - 304s 4s/step - loss: 2.9926 - regression_loss: 2.4405 - classification_loss: 0.5521\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_03.h5\n",
            "Epoch 4/150\n",
            "70/70 [==============================] - 302s 4s/step - loss: 2.9356 - regression_loss: 2.4174 - classification_loss: 0.5182\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_04.h5\n",
            "Epoch 5/150\n",
            "70/70 [==============================] - 291s 4s/step - loss: 2.8784 - regression_loss: 2.3730 - classification_loss: 0.5054\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_05.h5\n",
            "Epoch 6/150\n",
            "70/70 [==============================] - 283s 4s/step - loss: 2.8729 - regression_loss: 2.3715 - classification_loss: 0.5014\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_06.h5\n",
            "Epoch 7/150\n",
            "70/70 [==============================] - 323s 5s/step - loss: 2.8396 - regression_loss: 2.3515 - classification_loss: 0.4881\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_07.h5\n",
            "Epoch 8/150\n",
            "70/70 [==============================] - 299s 4s/step - loss: 2.8664 - regression_loss: 2.3896 - classification_loss: 0.4768\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_08.h5\n",
            "Epoch 9/150\n",
            "70/70 [==============================] - 288s 4s/step - loss: 2.8753 - regression_loss: 2.3987 - classification_loss: 0.4766\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_09.h5\n",
            "Epoch 10/150\n",
            "70/70 [==============================] - 302s 4s/step - loss: 2.8256 - regression_loss: 2.3676 - classification_loss: 0.4580\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_10.h5\n",
            "Epoch 11/150\n",
            "70/70 [==============================] - 305s 4s/step - loss: 2.8163 - regression_loss: 2.3592 - classification_loss: 0.4571\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_11.h5\n",
            "Epoch 12/150\n",
            "70/70 [==============================] - 295s 4s/step - loss: 2.8360 - regression_loss: 2.3688 - classification_loss: 0.4671\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_12.h5\n",
            "Epoch 13/150\n",
            "70/70 [==============================] - 311s 4s/step - loss: 2.8324 - regression_loss: 2.3664 - classification_loss: 0.4660\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_13.h5\n",
            "Epoch 14/150\n",
            "70/70 [==============================] - 296s 4s/step - loss: 2.7659 - regression_loss: 2.3128 - classification_loss: 0.4531\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_14.h5\n",
            "Epoch 15/150\n",
            "70/70 [==============================] - 286s 4s/step - loss: 2.8681 - regression_loss: 2.3967 - classification_loss: 0.4714\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_15.h5\n",
            "Epoch 16/150\n",
            "70/70 [==============================] - 296s 4s/step - loss: 2.8593 - regression_loss: 2.4005 - classification_loss: 0.4588\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_16.h5\n",
            "Epoch 17/150\n",
            "70/70 [==============================] - 296s 4s/step - loss: 3.2453 - regression_loss: 2.4326 - classification_loss: 0.8127\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_17.h5\n",
            "Epoch 18/150\n",
            "70/70 [==============================] - 319s 5s/step - loss: 3.0440 - regression_loss: 2.4181 - classification_loss: 0.6259\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/Hand models New annotations/resnet50_csv_18.h5\n",
            "Epoch 19/150\n",
            "50/70 [====================>.........] - ETA: 1:21 - loss: 2.8161 - regression_loss: 2.3646 - classification_loss: 0.4514"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8e106f12b5e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         callbacks=callbacks,) \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTY-JBKV9j54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVH41u7rBAXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXkp8zRIzbV1",
        "colab_type": "text"
      },
      "source": [
        "# Right hands joint detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4IVA1GjyzZs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c13e8c1-2fd4-40c0-c141-cb345e71620d"
      },
      "source": [
        "\n",
        "# tf.set_random_seed(31) # SEEDS MAKE RESULTS MORE REPRODUCABLE\n",
        "np.random.seed(17)\n",
        "classes = np.arange(0, 11, 1).tolist()\n",
        "\n",
        "# def convert_annotation(image_id,filename):\n",
        "#     in_file = open('training_data/labels/%s.xml'%(image_id))\n",
        "#     out_file = open(filename, 'a')\n",
        "#     tree=ET.parse(in_file)\n",
        "#     root = tree.getroot()\n",
        "    \n",
        "#     if root.iter('object') is not None:\n",
        "#         for obj in root.iter('object'):\n",
        "#             cls = obj.find('name').text\n",
        "#             if cls not in classes:\n",
        "#                 continue\n",
        "#             cls_id = classes.index(cls)\n",
        "            \n",
        "#             xmlbox = obj.find('bndbox')\n",
        "#             x1 = math.ceil(float(xmlbox.find('xmin').text))\n",
        "#             y1 = math.ceil(float(xmlbox.find('ymin').text))\n",
        "#             x2 = math.ceil(float(xmlbox.find('xmax').text))\n",
        "#             y2 = math.ceil(float(xmlbox.find('ymax').text))\n",
        "#             if x1 == x2 or y1 == y2:\n",
        "#                 continue\n",
        "                \n",
        "#             out_file.write(f'training_data/images/{image_id}.jpg,{x1},{y1},{x2},{y2},{cls}\\n')\n",
        "#     else:\n",
        "#         out_file.write(f'training_data/images/{image_id}.jpg,,,,,\\n')\n",
        "\n",
        "\n",
        "# _,_,image_ids = next(walk('training_data/images'))\n",
        "# image_ids = [i[:-4] for i in image_ids]\n",
        "# open('annotations.csv','w')\n",
        "# open('val_annotations.csv','w')\n",
        "\n",
        "# train_ids,val_ids = train_test_split(image_ids,random_state=31,test_size=0)\n",
        "\n",
        "# for image_id in train_ids:\n",
        "#     convert_annotation(image_id,'annotations.csv')\n",
        "        \n",
        "# for image_id in val_ids:\n",
        "#     convert_annotation(image_id,'val_annotations.csv')\n",
        "    \n",
        "# print(len(train_ids),len(val_ids))\n",
        "\n",
        "import pandas as pd\n",
        "tr_annots = pd.read_csv('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/Right hand train.csv', header = None)\n",
        "tr_annots['ids'] = tr_annots[0].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
        "train_ids = tr_annots['ids'].unique().tolist()\n",
        "\n",
        "import pandas as pd\n",
        "tr_annots = pd.read_csv('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/Right hand validation.csv', header = None)\n",
        "tr_annots['ids'] = tr_annots[0].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
        "val_ids = tr_annots['ids'].unique().tolist()\n",
        "\n",
        "print(len(train_ids), len(val_ids))\n",
        "\n",
        "with open('config.ini','w') as f:\n",
        "    f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 1 1.2 1.5 2 \\nscales  = 2 3 4 \\n')\n",
        "\t\n",
        "b = backbone('resnet50')\n",
        "\n",
        "class args:\n",
        "    batch_size = 2\n",
        "    config = read_config_file('config.ini')\n",
        "    random_transform = True # Image augmentation\n",
        "    annotations = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/Right hand train.csv'\n",
        "    val_annotations = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/training_data/Right hand validation.csv'\n",
        "    classes = '/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/hands_class_names.csv'\n",
        "    image_min_side = 900\n",
        "    image_max_side = 1200\n",
        "    no_resize = False\n",
        "    dataset_type = 'csv'\n",
        "    tensorboard_dir = ''\n",
        "    evaluation = False\n",
        "    snapshots = True\n",
        "    snapshot_path = \"/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/\"\n",
        "    backbone = 'resnet50'\n",
        "    epochs = 100\n",
        "    steps = len(train_ids)//(batch_size)\n",
        "    weighted_average = True\n",
        "\t\n",
        "train_gen, valid_gen = create_generators(args, b.preprocess_image)\n",
        "\n",
        "\n",
        "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "# Define our sequence of augmentation steps that will be applied to every image.\n",
        "seq = iaa.Sequential(\n",
        "    [\n",
        "        #\n",
        "        # Execute 1 to 9 of the following (less important) augmenters per\n",
        "        # image. Don't execute all of them, as that would often be way too\n",
        "        # strong.\n",
        "        #\n",
        "        iaa.SomeOf((1, 9),\n",
        "            [\n",
        "\n",
        "                        # Blur each image with varying strength using\n",
        "                        # gaussian blur (sigma between 0 and .5),\n",
        "                        # average/uniform blur (kernel size 1x1)\n",
        "                        # median blur (kernel size 1x1).\n",
        "                        iaa.OneOf([\n",
        "                            iaa.GaussianBlur((0,0.5)),\n",
        "                            iaa.AverageBlur(k=(1)),\n",
        "                            iaa.MedianBlur(k=(1)),\n",
        "                        ]),\n",
        "\n",
        "                        # Sharpen each image, overlay the result with the original\n",
        "                        # image using an alpha between 0 (no sharpening) and 1\n",
        "                        # (full sharpening effect).\n",
        "                        iaa.Sharpen(alpha=(0, 0.25), lightness=(0.75, 1.5)),\n",
        "\n",
        "                        # Add gaussian noise to some images.\n",
        "                        # In 50% of these cases, the noise is randomly sampled per\n",
        "                        # channel and pixel.\n",
        "                        # In the other 50% of all cases it is sampled once per\n",
        "                        # pixel (i.e. brightness change).\n",
        "                        iaa.AdditiveGaussianNoise(\n",
        "                            loc=0, scale=(0.0, 0.01*255), per_channel=0.5\n",
        "                        ),\n",
        "\n",
        "                        # Either drop randomly 1 to 10% of all pixels (i.e. set\n",
        "                        # them to black) or drop them on an image with 2-5% percent\n",
        "                        # of the original size, leading to large dropped\n",
        "                        # rectangles.\n",
        "                        iaa.OneOf([\n",
        "                            iaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
        "                            iaa.CoarseDropout(\n",
        "                                (0.03, 0.15), size_percent=(0.02, 0.05),\n",
        "                                per_channel=0.2\n",
        "                            ),\n",
        "                        ]),\n",
        "\n",
        "                        # Add a value of -5 to 5 to each pixel.\n",
        "                        iaa.Add((-5, 5), per_channel=0.5),\n",
        "\n",
        "                        # Change brightness of images (85-115% of original value).\n",
        "                        iaa.Multiply((0.85, 1.15), per_channel=0.5),\n",
        "\n",
        "                        # Improve or worsen the contrast of images.\n",
        "                        iaa.ContrastNormalization((0.75, 1.25), per_channel=0.5),\n",
        "\n",
        "                        # Convert each image to grayscale and then overlay the\n",
        "                        # result with the original with random alpha. I.e. remove\n",
        "                        # colors with varying strengths.\n",
        "                        iaa.Grayscale(alpha=(0.0, 0.25)),\n",
        "\n",
        "                        # In some images distort local areas with varying strength.\n",
        "                        sometimes(iaa.PiecewiseAffine(scale=(0.001, 0.01)))\n",
        "                    ],\n",
        "            # do all of the above augmentations in random order\n",
        "            random_order=True\n",
        "        )\n",
        "    ],\n",
        "    # do all of the above augmentations in random order\n",
        "    random_order=True\n",
        ")\n",
        "\n",
        "\n",
        "def augment_train_gen(train_gen, visualize=False):\n",
        "    '''\n",
        "    Creates a generator using another generator with applied image augmentation.\n",
        "    Args\n",
        "        train_gen  : keras-retinanet generator object.\n",
        "        visualize  : Boolean; False will convert bounding boxes to their anchor box targets for the model.\n",
        "    '''\n",
        "    imgs = []\n",
        "    boxes = []\n",
        "    targets = []\n",
        "    size = train_gen.size()\n",
        "    idx = 0\n",
        "    while True:\n",
        "        while len(imgs) < args.batch_size:\n",
        "            image       = train_gen.load_image(idx % size)\n",
        "            annotations = train_gen.load_annotations(idx % size)\n",
        "            image,annotations = train_gen.random_transform_group_entry(image,annotations)\n",
        "            imgs.append(image)            \n",
        "            boxes.append(annotations['bboxes'])\n",
        "            targets.append(annotations)\n",
        "            idx += 1\n",
        "        if visualize:\n",
        "            imgs = seq.augment_images(imgs)\n",
        "            imgs = np.array(imgs)\n",
        "            boxes = np.array(boxes)\n",
        "            yield imgs,boxes\n",
        "        else:\n",
        "            imgs = seq.augment_images(imgs)\n",
        "            imgs,targets = train_gen.preprocess_group(imgs,targets)\n",
        "            imgs = train_gen.compute_inputs(imgs)\n",
        "            targets = train_gen.compute_targets(imgs,targets)\n",
        "            imgs = np.array(imgs)\n",
        "            yield imgs,targets\n",
        "        imgs = []\n",
        "        boxes = []\n",
        "        targets = []\n",
        "\t\t\n",
        "\t\t\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# skip_batches = 5\n",
        "# i = 0\n",
        "# for imgs,boxes in augment_train_gen(train_gen,visualize=True):\n",
        "#     if i > skip_batches:\n",
        "#         fig=plt.figure(figsize=(24,96))\n",
        "#         columns = 2\n",
        "#         rows = 8\n",
        "#         for i in range(1, columns*rows + 1):\n",
        "#             draw_boxes(imgs[i], boxes[i], (0, 255, 0), thickness=1)\n",
        "#             fig.add_subplot(rows, columns, i)\n",
        "#             plt.imshow(cv2.cvtColor(imgs[i],cv2.COLOR_BGR2RGB))\n",
        "#         plt.show()\n",
        "#         break\n",
        "#     else:\n",
        "#         i += 1\n",
        "\n",
        "model, training_model, prediction_model = create_models(\n",
        "            backbone_retinanet=b.retinanet,\n",
        "            num_classes=train_gen.num_classes(),\n",
        "            weights=None,\n",
        "            multi_gpu=False,\n",
        "            freeze_backbone=True,\n",
        "            lr=1e-3,\n",
        "            config=args.config\n",
        "        )\n",
        "\n",
        "\t\t\n",
        "callbacks = create_callbacks(\n",
        "    model,\n",
        "    training_model,\n",
        "    prediction_model,\n",
        "    valid_gen,\n",
        "    args,\n",
        ")\n",
        "training_model.load_weights('/content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/resnet50_coco_best_v2.0.1.h5', skip_mismatch = True, by_name = True)\n",
        "\n",
        "##m Final model for Right hand joints detections\n",
        "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
        "        steps_per_epoch = args.steps,\n",
        "        epochs=args.epochs,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks, ) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66 12\n",
            "tracking <tf.Variable 'Variable:0' shape=(12, 4) dtype=float32, numpy=\n",
            "array([[-32.      , -32.      ,  32.      ,  32.      ],\n",
            "       [-48.      , -48.      ,  48.      ,  48.      ],\n",
            "       [-64.      , -64.      ,  64.      ,  64.      ],\n",
            "       [-29.211868, -35.054245,  29.211868,  35.054245],\n",
            "       [-43.817802, -52.581367,  43.817802,  52.581367],\n",
            "       [-58.423737, -70.10849 ,  58.423737,  70.10849 ],\n",
            "       [-26.127892, -39.191837,  26.127892,  39.191837],\n",
            "       [-39.191837, -58.787754,  39.191837,  58.787754],\n",
            "       [-52.255783, -78.383675,  52.255783,  78.383675],\n",
            "       [-22.627417, -45.254833,  22.627417,  45.254833],\n",
            "       [-33.941124, -67.88225 ,  33.941124,  67.88225 ],\n",
            "       [-45.254833, -90.50967 ,  45.254833,  90.50967 ]], dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(12, 4) dtype=float32, numpy=\n",
            "array([[ -64.      ,  -64.      ,   64.      ,   64.      ],\n",
            "       [ -96.      ,  -96.      ,   96.      ,   96.      ],\n",
            "       [-128.      , -128.      ,  128.      ,  128.      ],\n",
            "       [ -58.423737,  -70.10849 ,   58.423737,   70.10849 ],\n",
            "       [ -87.635605, -105.162735,   87.635605,  105.162735],\n",
            "       [-116.84747 , -140.21698 ,  116.84747 ,  140.21698 ],\n",
            "       [ -52.255783,  -78.383675,   52.255783,   78.383675],\n",
            "       [ -78.383675, -117.57551 ,   78.383675,  117.57551 ],\n",
            "       [-104.511566, -156.76735 ,  104.511566,  156.76735 ],\n",
            "       [ -45.254833,  -90.50967 ,   45.254833,   90.50967 ],\n",
            "       [ -67.88225 , -135.7645  ,   67.88225 ,  135.7645  ],\n",
            "       [ -90.50967 , -181.01933 ,   90.50967 ,  181.01933 ]],\n",
            "      dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(12, 4) dtype=float32, numpy=\n",
            "array([[-128.      , -128.      ,  128.      ,  128.      ],\n",
            "       [-192.      , -192.      ,  192.      ,  192.      ],\n",
            "       [-256.      , -256.      ,  256.      ,  256.      ],\n",
            "       [-116.84747 , -140.21698 ,  116.84747 ,  140.21698 ],\n",
            "       [-175.27121 , -210.32547 ,  175.27121 ,  210.32547 ],\n",
            "       [-233.69495 , -280.43396 ,  233.69495 ,  280.43396 ],\n",
            "       [-104.511566, -156.76735 ,  104.511566,  156.76735 ],\n",
            "       [-156.76735 , -235.15102 ,  156.76735 ,  235.15102 ],\n",
            "       [-209.02313 , -313.5347  ,  209.02313 ,  313.5347  ],\n",
            "       [ -90.50967 , -181.01933 ,   90.50967 ,  181.01933 ],\n",
            "       [-135.7645  , -271.529   ,  135.7645  ,  271.529   ],\n",
            "       [-181.01933 , -362.03867 ,  181.01933 ,  362.03867 ]],\n",
            "      dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(12, 4) dtype=float32, numpy=\n",
            "array([[-256.     , -256.     ,  256.     ,  256.     ],\n",
            "       [-384.     , -384.     ,  384.     ,  384.     ],\n",
            "       [-512.     , -512.     ,  512.     ,  512.     ],\n",
            "       [-233.69495, -280.43396,  233.69495,  280.43396],\n",
            "       [-350.54242, -420.65094,  350.54242,  420.65094],\n",
            "       [-467.3899 , -560.8679 ,  467.3899 ,  560.8679 ],\n",
            "       [-209.02313, -313.5347 ,  209.02313,  313.5347 ],\n",
            "       [-313.5347 , -470.30203,  313.5347 ,  470.30203],\n",
            "       [-418.04626, -627.0694 ,  418.04626,  627.0694 ],\n",
            "       [-181.01933, -362.03867,  181.01933,  362.03867],\n",
            "       [-271.529  , -543.058  ,  271.529  ,  543.058  ],\n",
            "       [-362.03867, -724.07733,  362.03867,  724.07733]], dtype=float32)> anchors\n",
            "tracking <tf.Variable 'Variable:0' shape=(12, 4) dtype=float32, numpy=\n",
            "array([[ -512.     ,  -512.     ,   512.     ,   512.     ],\n",
            "       [ -768.     ,  -768.     ,   768.     ,   768.     ],\n",
            "       [-1024.     , -1024.     ,  1024.     ,  1024.     ],\n",
            "       [ -467.3899 ,  -560.8679 ,   467.3899 ,   560.8679 ],\n",
            "       [ -701.08484,  -841.3019 ,   701.08484,   841.3019 ],\n",
            "       [ -934.7798 , -1121.7358 ,   934.7798 ,  1121.7358 ],\n",
            "       [ -418.04626,  -627.0694 ,   418.04626,   627.0694 ],\n",
            "       [ -627.0694 ,  -940.60406,   627.0694 ,   940.60406],\n",
            "       [ -836.0925 , -1254.1388 ,   836.0925 ,  1254.1388 ],\n",
            "       [ -362.03867,  -724.07733,   362.03867,   724.07733],\n",
            "       [ -543.058  , -1086.116  ,   543.058  ,  1086.116  ],\n",
            "       [ -724.07733, -1448.1547 ,   724.07733,  1448.1547 ]],\n",
            "      dtype=float32)> anchors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer regression_submodel due to mismatch in shape ((3, 3, 256, 48) vs (36, 256, 3, 3)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer regression_submodel due to mismatch in shape ((48,) vs (36,)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((3, 3, 256, 132) vs (720, 256, 3, 3)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((132,) vs (720,)).\n",
            "  weight_values[i].shape))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "33/33 [==============================] - 83s 3s/step - loss: 2.9203 - regression_loss: 2.2455 - classification_loss: 0.6748\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_01.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `mAP` which is not available. Available metrics are: loss,regression_loss,classification_loss,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/100\n",
            "33/33 [==============================] - 57s 2s/step - loss: 2.2100 - regression_loss: 1.7395 - classification_loss: 0.4705\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_02.h5\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 73s 2s/step - loss: 2.1320 - regression_loss: 1.6897 - classification_loss: 0.4423\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_03.h5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 60s 2s/step - loss: 1.9202 - regression_loss: 1.5340 - classification_loss: 0.3862\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_04.h5\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 69s 2s/step - loss: 1.9229 - regression_loss: 1.5448 - classification_loss: 0.3781\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_05.h5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 74s 2s/step - loss: 1.8407 - regression_loss: 1.4852 - classification_loss: 0.3555\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_06.h5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 69s 2s/step - loss: 1.7782 - regression_loss: 1.4310 - classification_loss: 0.3473\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_07.h5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 48s 1s/step - loss: 1.7979 - regression_loss: 1.4582 - classification_loss: 0.3397\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_08.h5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 72s 2s/step - loss: 1.7783 - regression_loss: 1.4263 - classification_loss: 0.3520\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_09.h5\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 71s 2s/step - loss: 1.5936 - regression_loss: 1.3178 - classification_loss: 0.2757\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_10.h5\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 65s 2s/step - loss: 1.5319 - regression_loss: 1.2874 - classification_loss: 0.2445\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_11.h5\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 78s 2s/step - loss: 1.5153 - regression_loss: 1.2726 - classification_loss: 0.2427\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_12.h5\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 67s 2s/step - loss: 1.5076 - regression_loss: 1.2552 - classification_loss: 0.2524\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_13.h5\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 67s 2s/step - loss: 1.5257 - regression_loss: 1.2736 - classification_loss: 0.2522\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_14.h5\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 63s 2s/step - loss: 1.4616 - regression_loss: 1.2374 - classification_loss: 0.2242\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_15.h5\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 61s 2s/step - loss: 1.4526 - regression_loss: 1.2385 - classification_loss: 0.2141\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_16.h5\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 73s 2s/step - loss: 1.4902 - regression_loss: 1.2617 - classification_loss: 0.2285\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_17.h5\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 74s 2s/step - loss: 1.4489 - regression_loss: 1.2266 - classification_loss: 0.2223\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_18.h5\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 68s 2s/step - loss: 1.4519 - regression_loss: 1.2416 - classification_loss: 0.2103\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_19.h5\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 63s 2s/step - loss: 1.4296 - regression_loss: 1.2111 - classification_loss: 0.2185\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_20.h5\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 72s 2s/step - loss: 1.4207 - regression_loss: 1.2114 - classification_loss: 0.2093\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_21.h5\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 70s 2s/step - loss: 1.4301 - regression_loss: 1.2218 - classification_loss: 0.2083\n",
            "\n",
            "Epoch 00022: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_22.h5\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 64s 2s/step - loss: 1.4447 - regression_loss: 1.2233 - classification_loss: 0.2214\n",
            "\n",
            "Epoch 00023: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_23.h5\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 77s 2s/step - loss: 1.3804 - regression_loss: 1.1729 - classification_loss: 0.2075\n",
            "\n",
            "Epoch 00024: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_24.h5\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 68s 2s/step - loss: 1.3600 - regression_loss: 1.1511 - classification_loss: 0.2089\n",
            "\n",
            "Epoch 00025: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_25.h5\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 62s 2s/step - loss: 1.3283 - regression_loss: 1.1293 - classification_loss: 0.1990\n",
            "\n",
            "Epoch 00026: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_26.h5\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 65s 2s/step - loss: 1.2881 - regression_loss: 1.0943 - classification_loss: 0.1938\n",
            "\n",
            "Epoch 00027: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_27.h5\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 59s 2s/step - loss: 1.3377 - regression_loss: 1.1379 - classification_loss: 0.1998\n",
            "\n",
            "Epoch 00028: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_28.h5\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 67s 2s/step - loss: 1.3263 - regression_loss: 1.1305 - classification_loss: 0.1958\n",
            "\n",
            "Epoch 00029: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_29.h5\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 54s 2s/step - loss: 1.3455 - regression_loss: 1.1447 - classification_loss: 0.2008\n",
            "\n",
            "Epoch 00030: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_30.h5\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 63s 2s/step - loss: 1.3051 - regression_loss: 1.1187 - classification_loss: 0.1864\n",
            "\n",
            "Epoch 00031: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_31.h5\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 69s 2s/step - loss: 1.3252 - regression_loss: 1.1323 - classification_loss: 0.1930\n",
            "\n",
            "Epoch 00032: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_32.h5\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 74s 2s/step - loss: 1.3106 - regression_loss: 1.1187 - classification_loss: 0.1918\n",
            "\n",
            "Epoch 00033: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_33.h5\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 69s 2s/step - loss: 1.3170 - regression_loss: 1.1213 - classification_loss: 0.1957\n",
            "\n",
            "Epoch 00034: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_34.h5\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 66s 2s/step - loss: 1.3174 - regression_loss: 1.1191 - classification_loss: 0.1983\n",
            "\n",
            "Epoch 00035: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_35.h5\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 60s 2s/step - loss: 1.3029 - regression_loss: 1.1120 - classification_loss: 0.1909\n",
            "\n",
            "Epoch 00036: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_36.h5\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 66s 2s/step - loss: 1.3125 - regression_loss: 1.1233 - classification_loss: 0.1892\n",
            "\n",
            "Epoch 00037: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_37.h5\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 81s 2s/step - loss: 1.3517 - regression_loss: 1.1467 - classification_loss: 0.2051\n",
            "\n",
            "Epoch 00038: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_38.h5\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 61s 2s/step - loss: 1.3134 - regression_loss: 1.1222 - classification_loss: 0.1912\n",
            "\n",
            "Epoch 00039: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_39.h5\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 75s 2s/step - loss: 1.3043 - regression_loss: 1.1065 - classification_loss: 0.1977\n",
            "\n",
            "Epoch 00040: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_40.h5\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 72s 2s/step - loss: 1.3441 - regression_loss: 1.1374 - classification_loss: 0.2067\n",
            "\n",
            "Epoch 00041: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_41.h5\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 61s 2s/step - loss: 1.2985 - regression_loss: 1.1060 - classification_loss: 0.1925\n",
            "\n",
            "Epoch 00042: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_42.h5\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 71s 2s/step - loss: 1.3447 - regression_loss: 1.1469 - classification_loss: 0.1978\n",
            "\n",
            "Epoch 00043: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_43.h5\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 75s 2s/step - loss: 1.3553 - regression_loss: 1.1480 - classification_loss: 0.2073\n",
            "\n",
            "Epoch 00044: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_44.h5\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 60s 2s/step - loss: 1.3019 - regression_loss: 1.1072 - classification_loss: 0.1947\n",
            "\n",
            "Epoch 00045: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_45.h5\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 69s 2s/step - loss: 1.3145 - regression_loss: 1.1199 - classification_loss: 0.1946\n",
            "\n",
            "Epoch 00046: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_46.h5\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 66s 2s/step - loss: 1.3096 - regression_loss: 1.1096 - classification_loss: 0.2000\n",
            "\n",
            "Epoch 00047: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_47.h5\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 59s 2s/step - loss: 1.3190 - regression_loss: 1.1255 - classification_loss: 0.1935\n",
            "\n",
            "Epoch 00048: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_48.h5\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 54s 2s/step - loss: 1.3120 - regression_loss: 1.1166 - classification_loss: 0.1953\n",
            "\n",
            "Epoch 00049: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_49.h5\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 68s 2s/step - loss: 1.3098 - regression_loss: 1.1204 - classification_loss: 0.1895\n",
            "\n",
            "Epoch 00050: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_50.h5\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 71s 2s/step - loss: 1.3340 - regression_loss: 1.1269 - classification_loss: 0.2071\n",
            "\n",
            "Epoch 00051: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_51.h5\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 81s 2s/step - loss: 1.3273 - regression_loss: 1.1346 - classification_loss: 0.1928\n",
            "\n",
            "Epoch 00052: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_52.h5\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 61s 2s/step - loss: 1.3221 - regression_loss: 1.1315 - classification_loss: 0.1905\n",
            "\n",
            "Epoch 00053: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_53.h5\n",
            "\n",
            "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 57s 2s/step - loss: 1.3058 - regression_loss: 1.1170 - classification_loss: 0.1888\n",
            "\n",
            "Epoch 00054: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_54.h5\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 59s 2s/step - loss: 1.3368 - regression_loss: 1.1399 - classification_loss: 0.1969\n",
            "\n",
            "Epoch 00055: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_55.h5\n",
            "\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 71s 2s/step - loss: 1.3160 - regression_loss: 1.1128 - classification_loss: 0.2032\n",
            "\n",
            "Epoch 00056: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_56.h5\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 82s 2s/step - loss: 1.3222 - regression_loss: 1.1184 - classification_loss: 0.2038\n",
            "\n",
            "Epoch 00057: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_57.h5\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 67s 2s/step - loss: 1.3606 - regression_loss: 1.1565 - classification_loss: 0.2041\n",
            "\n",
            "Epoch 00058: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_58.h5\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 64s 2s/step - loss: 1.3039 - regression_loss: 1.1094 - classification_loss: 0.1945\n",
            "\n",
            "Epoch 00059: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_59.h5\n",
            "\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 64s 2s/step - loss: 1.3245 - regression_loss: 1.1287 - classification_loss: 0.1958\n",
            "\n",
            "Epoch 00060: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_60.h5\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 65s 2s/step - loss: 1.3277 - regression_loss: 1.1326 - classification_loss: 0.1951\n",
            "\n",
            "Epoch 00061: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_61.h5\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 62s 2s/step - loss: 1.3026 - regression_loss: 1.1150 - classification_loss: 0.1876\n",
            "\n",
            "Epoch 00062: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_62.h5\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 67s 2s/step - loss: 1.3237 - regression_loss: 1.1264 - classification_loss: 0.1973\n",
            "\n",
            "Epoch 00063: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_63.h5\n",
            "\n",
            "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.0000000944832675e-23.\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 56s 2s/step - loss: 1.3369 - regression_loss: 1.1363 - classification_loss: 0.2006\n",
            "\n",
            "Epoch 00064: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_64.h5\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 53s 2s/step - loss: 1.3002 - regression_loss: 1.1155 - classification_loss: 0.1846\n",
            "\n",
            "Epoch 00065: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_65.h5\n",
            "\n",
            "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.0000000787060494e-24.\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 80s 2s/step - loss: 1.3197 - regression_loss: 1.1218 - classification_loss: 0.1979\n",
            "\n",
            "Epoch 00066: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_66.h5\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 69s 2s/step - loss: 1.3296 - regression_loss: 1.1275 - classification_loss: 0.2020\n",
            "\n",
            "Epoch 00067: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_67.h5\n",
            "\n",
            "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.0000001181490946e-25.\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 62s 2s/step - loss: 1.3265 - regression_loss: 1.1360 - classification_loss: 0.1905\n",
            "\n",
            "Epoch 00068: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_68.h5\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 69s 2s/step - loss: 1.3125 - regression_loss: 1.1191 - classification_loss: 0.1934\n",
            "\n",
            "Epoch 00069: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_69.h5\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.0000001428009978e-26.\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 68s 2s/step - loss: 1.2988 - regression_loss: 1.1105 - classification_loss: 0.1883\n",
            "\n",
            "Epoch 00070: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_70.h5\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 65s 2s/step - loss: 1.3194 - regression_loss: 1.1333 - classification_loss: 0.1861\n",
            "\n",
            "Epoch 00071: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_71.h5\n",
            "\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.000000142800998e-27.\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 64s 2s/step - loss: 1.3142 - regression_loss: 1.1253 - classification_loss: 0.1889\n",
            "\n",
            "Epoch 00072: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_72.h5\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 83s 3s/step - loss: 1.3376 - regression_loss: 1.1360 - classification_loss: 0.2016\n",
            "\n",
            "Epoch 00073: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_73.h5\n",
            "\n",
            "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.0000001235416984e-28.\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 67s 2s/step - loss: 1.3156 - regression_loss: 1.1267 - classification_loss: 0.1889\n",
            "\n",
            "Epoch 00074: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_74.h5\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 79s 2s/step - loss: 1.3177 - regression_loss: 1.1270 - classification_loss: 0.1907\n",
            "\n",
            "Epoch 00075: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_75.h5\n",
            "\n",
            "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.0000001235416985e-29.\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 65s 2s/step - loss: 1.3040 - regression_loss: 1.1149 - classification_loss: 0.1891\n",
            "\n",
            "Epoch 00076: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_76.h5\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 67s 2s/step - loss: 1.3018 - regression_loss: 1.1084 - classification_loss: 0.1934\n",
            "\n",
            "Epoch 00077: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_77.h5\n",
            "\n",
            "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1.0000001536343539e-30.\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 68s 2s/step - loss: 1.3136 - regression_loss: 1.1196 - classification_loss: 0.1940\n",
            "\n",
            "Epoch 00078: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_78.h5\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 65s 2s/step - loss: 1.3849 - regression_loss: 1.1788 - classification_loss: 0.2061\n",
            "\n",
            "Epoch 00079: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_79.h5\n",
            "\n",
            "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.000000191250173e-31.\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 63s 2s/step - loss: 1.3330 - regression_loss: 1.1322 - classification_loss: 0.2009\n",
            "\n",
            "Epoch 00080: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_80.h5\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 75s 2s/step - loss: 1.3360 - regression_loss: 1.1321 - classification_loss: 0.2039\n",
            "\n",
            "Epoch 00081: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_81.h5\n",
            "\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1.0000002147600601e-32.\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 66s 2s/step - loss: 1.3101 - regression_loss: 1.1159 - classification_loss: 0.1942\n",
            "\n",
            "Epoch 00082: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_82.h5\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 73s 2s/step - loss: 1.3116 - regression_loss: 1.1174 - classification_loss: 0.1942\n",
            "\n",
            "Epoch 00083: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_83.h5\n",
            "\n",
            "Epoch 00083: ReduceLROnPlateau reducing learning rate to 1.0000002441474188e-33.\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 66s 2s/step - loss: 1.2749 - regression_loss: 1.0897 - classification_loss: 0.1852\n",
            "\n",
            "Epoch 00084: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_84.h5\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 70s 2s/step - loss: 1.3283 - regression_loss: 1.1294 - classification_loss: 0.1990\n",
            "\n",
            "Epoch 00085: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_85.h5\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 59s 2s/step - loss: 1.3176 - regression_loss: 1.1238 - classification_loss: 0.1938\n",
            "\n",
            "Epoch 00086: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_86.h5\n",
            "\n",
            "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.0000002074132203e-34.\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 74s 2s/step - loss: 1.2876 - regression_loss: 1.0950 - classification_loss: 0.1926\n",
            "\n",
            "Epoch 00087: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_87.h5\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 69s 2s/step - loss: 1.3059 - regression_loss: 1.1141 - classification_loss: 0.1918\n",
            "\n",
            "Epoch 00088: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_88.h5\n",
            "\n",
            "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.0000001614954722e-35.\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 76s 2s/step - loss: 1.3353 - regression_loss: 1.1261 - classification_loss: 0.2092\n",
            "\n",
            "Epoch 00089: saving model to /content/drive/My Drive/RA2/Retinanet/Upload for Kaggle/saved_right_hand_models/resnet50_csv_89.h5\n",
            "Epoch 90/100\n",
            "19/33 [================>.............] - ETA: 40s - loss: 1.3037 - regression_loss: 1.1153 - classification_loss: 0.1884"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a24ef7e36494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         callbacks=callbacks, ) \n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQyVmQXLyzG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWhDsP2Pyydk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df2KYOJUyyS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEjfgQQ0yyIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bPuxen7yx2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}